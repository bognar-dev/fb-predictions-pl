{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niklas\\AppData\\Local\\Temp\\ipykernel_14304\\3496188416.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pandas as pd\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import keras.utils\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import textwrap\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def split_match_data(match_data, list_of_features: list[str], target_variable: str, test_size=0.3, random_state=42):\n",
    "    X = match_data[list_of_features]\n",
    "    y = match_data[target_variable]\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def get_model_metrics(models, X_train, X_test, y_train, y_test, epochs=300):\n",
    "    batch_size = 64\n",
    "    model_metrics = {}\n",
    "    for model in models:\n",
    "        model_name = model.__class__.__name__\n",
    "        if model_name == 'KerasClassifier':\n",
    "            log_dir = \"./logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "            model_name = model.model.name\n",
    "            print(f\"Training {model_name}\")\n",
    "            early_stopping = EarlyStopping(monitor=\"val_loss\",\n",
    "                                           mode=\"min\", patience=5,\n",
    "                                           restore_best_weights=True)\n",
    "            model.fit(\n",
    "                X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=epochs\n",
    "                ,\n",
    "                callbacks=[early_stopping, tensorboard_callback]\n",
    "            )\n",
    "            keras.utils.plot_model(model.model, show_shapes=True, show_layer_names=True,\n",
    "                                   to_file=f\"stat/{model_name}_flowchart.png\")\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        model_score = model.score(X_test, y_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        conf_matrix = confusion_matrix(y_test, predictions)\n",
    "        precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, predictions, average='weighted')\n",
    "        f1 = f1_score(y_test, predictions, average='weighted')\n",
    "        y_true = y_test\n",
    "        y_pred = predictions\n",
    "        # plot_confusionMatrix(y_true, y_pred)\n",
    "\n",
    "        model_metrics[model_name] = {\n",
    "            'Score': model_score,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1\n",
    "        }\n",
    "\n",
    "    return model_metrics\n",
    "\n",
    "\n",
    "def get_scores(modelMetrics):\n",
    "    for model_name, metrics in modelMetrics.items():\n",
    "        print(f\"{model_name} Score is {str(metrics['Score'])[:4]}\")\n",
    "\n",
    "\n",
    "def wrap_labels(ax, width, break_long_words=True):\n",
    "    labels = []\n",
    "    for label in ax.get_xticklabels():\n",
    "        text = label.get_text()\n",
    "        labels.append(textwrap.fill(text, width=width,\n",
    "                                    break_long_words=break_long_words))\n",
    "    ax.set_xticklabels(labels, rotation=0)\n",
    "\n",
    "\n",
    "def plot_scores(model_metrics):\n",
    "    # Define the label locations and the width of the bars\n",
    "    x = np.arange(len(model_metrics))\n",
    "    width = 0.15\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "    # Create a color palette\n",
    "    colours = sns.color_palette('pastel')\n",
    "\n",
    "    # Specify the order of metrics\n",
    "    metrics_order = ['Score', 'Recall', 'Accuracy', 'F1-score', 'Precision']\n",
    "\n",
    "    # Create a bar for each metric in the specified order\n",
    "    for metric_id, metric in enumerate(metrics_order):\n",
    "        values = [metrics[metric] for metrics in model_metrics.values()]\n",
    "        rects = ax.bar(x + width * metric_id, values, width, label=metric, color=colours[metric_id])\n",
    "\n",
    "    # find min and max values for y axis and limit the y axis\n",
    "    min_y = min([min([metrics[metric] for metrics in model_metrics.values()]) for metric in metrics_order])\n",
    "    max_y = max([max([metrics[metric] for metrics in model_metrics.values()]) for metric in metrics_order])\n",
    "    ax.set_ylim([min_y - 0.05, max_y + 0.05])\n",
    "\n",
    "    # Add labels, title, legend, etc.\n",
    "    ax.set_ylabel('Metrics')\n",
    "    ax.set_title('Model Metrics by Model')\n",
    "    ax.set_xticks(x + width * 2)\n",
    "    ax.set_xticklabels(model_metrics.keys())\n",
    "    ax.legend()\n",
    "\n",
    "    wrap_labels(ax, 10)\n",
    "\n",
    "    plt.show()\n",
    "    plt.savefig('stat/model_metrics.png')\n",
    "\n",
    "\n",
    "def plot_confusionMatrix(y_true, y_pred):\n",
    "    cn = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    sns.heatmap(cn, annot=True, linewidths=1.5)\n",
    "    plt.show()\n",
    "    return cn\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def get_LSTM(input_dim=29, units=64, output_size=3, allow_cudnn_kernel=True):\n",
    "    model = keras.models.Sequential(name=\"lstm_model\")\n",
    "    if allow_cudnn_kernel:\n",
    "        model.add(keras.layers.LSTM(units, input_shape=(input_dim, 1), return_sequences=False))\n",
    "    else:\n",
    "        model.add(keras.layers.RNN(keras.layers.LSTMCell(units), input_shape=(input_dim, 1)))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(output_size, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_Dense(input_dim=29, output_size=3):\n",
    "    model = keras.models.Sequential(name=\"dense_model\")\n",
    "    model.add(keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)))\n",
    "    model.add(keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(keras.layers.Dense(output_size, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_CNN(input_dim=29, output_size=3):\n",
    "    model = keras.models.Sequential(name=\"cnn_model\")\n",
    "    model.add(keras.layers.Conv1D(32, 3, activation='relu', input_shape=(input_dim, 1)))\n",
    "    model.add(keras.layers.MaxPooling1D(2))\n",
    "    model.add(keras.layers.Conv1D(64, 3, activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling1D(2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(output_size, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print(model.summary())\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cnn_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 27, 32)            128       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 13, 32)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 11, 64)            6208      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 5, 64)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 320)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                20544     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,075\n",
      "Trainable params: 27,075\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"dense_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 64)                1920      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,099\n",
      "Trainable params: 4,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"lstm_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 64)                16896     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,251\n",
      "Trainable params: 21,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators=100),\n",
    "    GradientBoostingClassifier(),\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    SVC(),\n",
    "    LinearSVC(),\n",
    "    NuSVC(),\n",
    "    KNeighborsClassifier(n_neighbors=22),\n",
    "    KerasClassifier(\n",
    "        get_CNN(),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "    ),\n",
    "    KerasClassifier(\n",
    "        get_Dense(),\n",
    "        loss=\"sparse_categorical_crossentropy\"\n",
    "    ),\n",
    "    KerasClassifier(\n",
    "        get_LSTM(),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "    ),\n",
    "\n",
    "    GaussianNB(),\n",
    "]\n",
    "\n",
    "# list_of_features = [\n",
    "#    'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'HTGS', 'ATGS', 'HTGC', 'ATGC', 'HTP', 'ATP',\n",
    "#    'HM1', 'AM1', 'HM2', 'AM2', 'HM3', 'AM3', 'HM4', 'AM4', 'HM5', 'AM5', 'MW', 'HomeTeamLP',\n",
    "#    'AwayTeamLP', 'HTFormPtsStr', 'ATFormPtsStr', 'HTFormPts', 'ATFormPts', 'HTWinStreak3',\n",
    "#    'HTWinStreak5', 'HTLossStreak3', 'HTLossStreak5', 'ATWinStreak3', 'ATWinStreak5',\n",
    "#    'ATLossStreak3', 'ATLossStreak5', 'HTGD', 'ATGD', 'DiffPts', 'DiffFormPts', 'DiffLP'\n",
    "# ]\n",
    "\n",
    "list_of_features = [\n",
    "    'HTP', 'ATP',\n",
    "    'HM1', 'AM1', 'HM2', 'AM2', 'HM3', 'AM3', 'HM4', 'AM4', 'HM5', 'AM5', 'HomeTeamLP',\n",
    "    'AwayTeamLP', 'HTFormPts', 'ATFormPts', 'HTWinStreak3',\n",
    "    'HTWinStreak5', 'HTLossStreak3', 'HTLossStreak5', 'ATWinStreak3', 'ATWinStreak5',\n",
    "    'ATLossStreak3', 'ATLossStreak5', 'HTGD', 'ATGD', 'DiffPts', 'DiffFormPts', 'DiffLP'\n",
    "]\n",
    "\n",
    "target_variable = 'FTR'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-f4cc26be46270696\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-f4cc26be46270696\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niklas\\PycharmProjects\\fb-predictions-pl\\venv\\lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Niklas\\PycharmProjects\\fb-predictions-pl\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cnn_model\n",
      "Epoch 1/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3723 - accuracy: 0.8447 - val_loss: 1.9446 - val_accuracy: 0.5699\n",
      "Epoch 2/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3601 - accuracy: 0.8518 - val_loss: 1.9441 - val_accuracy: 0.5567\n",
      "Epoch 3/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3664 - accuracy: 0.8489 - val_loss: 1.9331 - val_accuracy: 0.5597\n",
      "Epoch 4/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3570 - accuracy: 0.8504 - val_loss: 1.9294 - val_accuracy: 0.5533\n",
      "Epoch 5/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3506 - accuracy: 0.8555 - val_loss: 1.9860 - val_accuracy: 0.5567\n",
      "Epoch 6/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.3619 - accuracy: 0.8482 - val_loss: 1.9490 - val_accuracy: 0.5580\n",
      "Epoch 7/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3564 - accuracy: 0.8531 - val_loss: 1.9539 - val_accuracy: 0.5490\n",
      "Epoch 8/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3605 - accuracy: 0.8467 - val_loss: 1.9857 - val_accuracy: 0.5384\n",
      "Epoch 9/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3545 - accuracy: 0.8487 - val_loss: 2.0280 - val_accuracy: 0.5673\n",
      "Epoch 10/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.3480 - accuracy: 0.8576 - val_loss: 1.9981 - val_accuracy: 0.5588\n",
      "Epoch 11/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3538 - accuracy: 0.8511 - val_loss: 1.9771 - val_accuracy: 0.5529\n",
      "Epoch 12/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3643 - accuracy: 0.8462 - val_loss: 2.0288 - val_accuracy: 0.5516\n",
      "Epoch 13/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3563 - accuracy: 0.8511 - val_loss: 1.9781 - val_accuracy: 0.5482\n",
      "Epoch 14/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3599 - accuracy: 0.8522 - val_loss: 1.9871 - val_accuracy: 0.5533\n",
      "Epoch 15/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.3679 - accuracy: 0.8496 - val_loss: 2.0205 - val_accuracy: 0.5380\n",
      "Epoch 16/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3639 - accuracy: 0.8504 - val_loss: 2.0730 - val_accuracy: 0.5567\n",
      "Epoch 17/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3581 - accuracy: 0.8522 - val_loss: 1.9870 - val_accuracy: 0.5469\n",
      "Epoch 18/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3550 - accuracy: 0.8476 - val_loss: 2.0582 - val_accuracy: 0.5240\n",
      "Epoch 19/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3625 - accuracy: 0.8442 - val_loss: 2.0138 - val_accuracy: 0.5707\n",
      "Epoch 20/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3525 - accuracy: 0.8518 - val_loss: 2.0801 - val_accuracy: 0.5537\n",
      "Epoch 21/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3422 - accuracy: 0.8587 - val_loss: 2.0207 - val_accuracy: 0.5588\n",
      "Epoch 22/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3456 - accuracy: 0.8604 - val_loss: 2.0314 - val_accuracy: 0.5529\n",
      "Epoch 23/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3528 - accuracy: 0.8544 - val_loss: 2.1193 - val_accuracy: 0.5673\n",
      "Epoch 24/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3605 - accuracy: 0.8438 - val_loss: 2.0515 - val_accuracy: 0.5563\n",
      "Epoch 25/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3478 - accuracy: 0.8556 - val_loss: 2.0810 - val_accuracy: 0.5397\n",
      "Epoch 26/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3580 - accuracy: 0.8494 - val_loss: 2.0309 - val_accuracy: 0.5711\n",
      "Epoch 27/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3652 - accuracy: 0.8471 - val_loss: 2.0616 - val_accuracy: 0.5588\n",
      "Epoch 28/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3526 - accuracy: 0.8520 - val_loss: 2.0924 - val_accuracy: 0.5575\n",
      "Epoch 29/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3513 - accuracy: 0.8527 - val_loss: 2.0677 - val_accuracy: 0.5567\n",
      "Epoch 30/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3568 - accuracy: 0.8500 - val_loss: 2.0873 - val_accuracy: 0.5656\n",
      "Epoch 31/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3513 - accuracy: 0.8500 - val_loss: 2.0436 - val_accuracy: 0.5592\n",
      "Epoch 32/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3435 - accuracy: 0.8585 - val_loss: 2.0720 - val_accuracy: 0.5384\n",
      "Epoch 33/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3616 - accuracy: 0.8507 - val_loss: 2.0719 - val_accuracy: 0.5648\n",
      "Epoch 34/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3353 - accuracy: 0.8605 - val_loss: 2.0680 - val_accuracy: 0.5520\n",
      "Epoch 35/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3454 - accuracy: 0.8615 - val_loss: 2.0682 - val_accuracy: 0.5478\n",
      "Epoch 36/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3366 - accuracy: 0.8620 - val_loss: 2.0781 - val_accuracy: 0.5401\n",
      "Epoch 37/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3538 - accuracy: 0.8491 - val_loss: 2.1298 - val_accuracy: 0.5592\n",
      "Epoch 38/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3436 - accuracy: 0.8576 - val_loss: 2.1392 - val_accuracy: 0.5482\n",
      "Epoch 39/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3460 - accuracy: 0.8507 - val_loss: 2.0827 - val_accuracy: 0.5520\n",
      "Epoch 40/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3365 - accuracy: 0.8587 - val_loss: 2.1840 - val_accuracy: 0.5643\n",
      "Epoch 41/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3448 - accuracy: 0.8504 - val_loss: 2.0715 - val_accuracy: 0.5673\n",
      "Epoch 42/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3417 - accuracy: 0.8565 - val_loss: 2.0888 - val_accuracy: 0.5601\n",
      "Epoch 43/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3426 - accuracy: 0.8569 - val_loss: 2.1504 - val_accuracy: 0.5626\n",
      "Epoch 44/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3421 - accuracy: 0.8564 - val_loss: 2.1253 - val_accuracy: 0.5541\n",
      "Epoch 45/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3313 - accuracy: 0.8605 - val_loss: 2.1218 - val_accuracy: 0.5554\n",
      "Epoch 46/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3336 - accuracy: 0.8589 - val_loss: 2.1471 - val_accuracy: 0.5673\n",
      "Epoch 47/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3383 - accuracy: 0.8584 - val_loss: 2.1842 - val_accuracy: 0.5601\n",
      "Epoch 48/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3307 - accuracy: 0.8642 - val_loss: 2.1132 - val_accuracy: 0.5605\n",
      "Epoch 49/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3390 - accuracy: 0.8536 - val_loss: 2.1474 - val_accuracy: 0.5533\n",
      "Epoch 50/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3389 - accuracy: 0.8587 - val_loss: 2.1288 - val_accuracy: 0.5444\n",
      "Epoch 51/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3255 - accuracy: 0.8646 - val_loss: 2.1637 - val_accuracy: 0.5656\n",
      "Epoch 52/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3417 - accuracy: 0.8542 - val_loss: 2.1458 - val_accuracy: 0.5541\n",
      "Epoch 53/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3324 - accuracy: 0.8611 - val_loss: 2.2077 - val_accuracy: 0.5622\n",
      "Epoch 54/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3519 - accuracy: 0.8489 - val_loss: 2.2188 - val_accuracy: 0.5469\n",
      "Epoch 55/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3468 - accuracy: 0.8556 - val_loss: 2.2002 - val_accuracy: 0.5728\n",
      "Epoch 56/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3459 - accuracy: 0.8576 - val_loss: 2.2415 - val_accuracy: 0.5512\n",
      "Epoch 57/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3328 - accuracy: 0.8591 - val_loss: 2.1696 - val_accuracy: 0.5622\n",
      "Epoch 58/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3299 - accuracy: 0.8604 - val_loss: 2.1918 - val_accuracy: 0.5605\n",
      "Epoch 59/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3411 - accuracy: 0.8585 - val_loss: 2.1654 - val_accuracy: 0.5431\n",
      "Epoch 60/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3310 - accuracy: 0.8620 - val_loss: 2.1907 - val_accuracy: 0.5380\n",
      "Epoch 61/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3230 - accuracy: 0.8622 - val_loss: 2.2069 - val_accuracy: 0.5665\n",
      "Epoch 62/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3255 - accuracy: 0.8653 - val_loss: 2.2262 - val_accuracy: 0.5350\n",
      "Epoch 63/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3362 - accuracy: 0.8591 - val_loss: 2.1622 - val_accuracy: 0.5533\n",
      "Epoch 64/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3354 - accuracy: 0.8589 - val_loss: 2.2555 - val_accuracy: 0.5516\n",
      "Epoch 65/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3253 - accuracy: 0.8580 - val_loss: 2.2106 - val_accuracy: 0.5478\n",
      "Epoch 66/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3214 - accuracy: 0.8675 - val_loss: 2.2042 - val_accuracy: 0.5529\n",
      "Epoch 67/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3288 - accuracy: 0.8565 - val_loss: 2.2279 - val_accuracy: 0.5482\n",
      "Epoch 68/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3270 - accuracy: 0.8611 - val_loss: 2.2424 - val_accuracy: 0.5614\n",
      "Epoch 69/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3255 - accuracy: 0.8607 - val_loss: 2.2306 - val_accuracy: 0.5355\n",
      "Epoch 70/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3192 - accuracy: 0.8640 - val_loss: 2.2336 - val_accuracy: 0.5465\n",
      "Epoch 71/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3204 - accuracy: 0.8666 - val_loss: 2.2102 - val_accuracy: 0.5550\n",
      "Epoch 72/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3348 - accuracy: 0.8618 - val_loss: 2.2843 - val_accuracy: 0.5724\n",
      "Epoch 73/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3361 - accuracy: 0.8585 - val_loss: 2.2837 - val_accuracy: 0.5516\n",
      "Epoch 74/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3267 - accuracy: 0.8618 - val_loss: 2.2721 - val_accuracy: 0.5512\n",
      "Epoch 75/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3326 - accuracy: 0.8609 - val_loss: 2.2851 - val_accuracy: 0.5507\n",
      "Epoch 76/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3293 - accuracy: 0.8587 - val_loss: 2.3194 - val_accuracy: 0.5567\n",
      "Epoch 77/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3320 - accuracy: 0.8569 - val_loss: 2.2070 - val_accuracy: 0.5473\n",
      "Epoch 78/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3210 - accuracy: 0.8646 - val_loss: 2.2347 - val_accuracy: 0.5605\n",
      "Epoch 79/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3250 - accuracy: 0.8653 - val_loss: 2.2610 - val_accuracy: 0.5618\n",
      "Epoch 80/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3355 - accuracy: 0.8565 - val_loss: 2.3078 - val_accuracy: 0.5567\n",
      "Epoch 81/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3277 - accuracy: 0.8565 - val_loss: 2.3084 - val_accuracy: 0.5512\n",
      "Epoch 82/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3104 - accuracy: 0.8706 - val_loss: 2.3166 - val_accuracy: 0.5541\n",
      "Epoch 83/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3233 - accuracy: 0.8644 - val_loss: 2.2916 - val_accuracy: 0.5660\n",
      "Epoch 84/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3176 - accuracy: 0.8686 - val_loss: 2.2942 - val_accuracy: 0.5520\n",
      "Epoch 85/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3157 - accuracy: 0.8649 - val_loss: 2.3628 - val_accuracy: 0.5558\n",
      "Epoch 86/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3229 - accuracy: 0.8624 - val_loss: 2.3383 - val_accuracy: 0.5665\n",
      "Epoch 87/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3197 - accuracy: 0.8635 - val_loss: 2.3469 - val_accuracy: 0.5597\n",
      "Epoch 88/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3247 - accuracy: 0.8616 - val_loss: 2.3588 - val_accuracy: 0.5444\n",
      "Epoch 89/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3147 - accuracy: 0.8678 - val_loss: 2.3102 - val_accuracy: 0.5614\n",
      "Epoch 90/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3193 - accuracy: 0.8660 - val_loss: 2.3560 - val_accuracy: 0.5537\n",
      "Epoch 91/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3232 - accuracy: 0.8631 - val_loss: 2.3030 - val_accuracy: 0.5499\n",
      "Epoch 92/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3215 - accuracy: 0.8613 - val_loss: 2.2919 - val_accuracy: 0.5541\n",
      "Epoch 93/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3065 - accuracy: 0.8702 - val_loss: 2.3449 - val_accuracy: 0.5482\n",
      "Epoch 94/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3094 - accuracy: 0.8680 - val_loss: 2.3541 - val_accuracy: 0.5516\n",
      "Epoch 95/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3182 - accuracy: 0.8646 - val_loss: 2.3752 - val_accuracy: 0.5503\n",
      "Epoch 96/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3147 - accuracy: 0.8656 - val_loss: 2.3929 - val_accuracy: 0.5605\n",
      "Epoch 97/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3067 - accuracy: 0.8700 - val_loss: 2.3659 - val_accuracy: 0.5711\n",
      "Epoch 98/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3114 - accuracy: 0.8660 - val_loss: 2.3445 - val_accuracy: 0.5588\n",
      "Epoch 99/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3280 - accuracy: 0.8562 - val_loss: 2.4625 - val_accuracy: 0.5490\n",
      "Epoch 100/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3341 - accuracy: 0.8591 - val_loss: 2.3443 - val_accuracy: 0.5563\n",
      "Epoch 101/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3129 - accuracy: 0.8684 - val_loss: 2.4302 - val_accuracy: 0.5452\n",
      "Epoch 102/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3232 - accuracy: 0.8640 - val_loss: 2.3952 - val_accuracy: 0.5456\n",
      "Epoch 103/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3084 - accuracy: 0.8715 - val_loss: 2.4345 - val_accuracy: 0.5639\n",
      "Epoch 104/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3043 - accuracy: 0.8757 - val_loss: 2.4047 - val_accuracy: 0.5520\n",
      "Epoch 105/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3226 - accuracy: 0.8642 - val_loss: 2.4628 - val_accuracy: 0.5631\n",
      "Epoch 106/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3114 - accuracy: 0.8707 - val_loss: 2.4084 - val_accuracy: 0.5414\n",
      "Epoch 107/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3094 - accuracy: 0.8691 - val_loss: 2.4309 - val_accuracy: 0.5652\n",
      "Epoch 108/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3216 - accuracy: 0.8616 - val_loss: 2.4224 - val_accuracy: 0.5520\n",
      "Epoch 109/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3152 - accuracy: 0.8656 - val_loss: 2.4680 - val_accuracy: 0.5321\n",
      "Epoch 110/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3033 - accuracy: 0.8729 - val_loss: 2.4302 - val_accuracy: 0.5524\n",
      "Epoch 111/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3094 - accuracy: 0.8702 - val_loss: 2.4458 - val_accuracy: 0.5482\n",
      "Epoch 112/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3141 - accuracy: 0.8669 - val_loss: 2.4830 - val_accuracy: 0.5478\n",
      "Epoch 113/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3289 - accuracy: 0.8587 - val_loss: 2.4935 - val_accuracy: 0.5665\n",
      "Epoch 114/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.3230 - accuracy: 0.8602 - val_loss: 2.4518 - val_accuracy: 0.5656\n",
      "Epoch 115/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3017 - accuracy: 0.8707 - val_loss: 2.4314 - val_accuracy: 0.5414\n",
      "Epoch 116/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3023 - accuracy: 0.8717 - val_loss: 2.4635 - val_accuracy: 0.5524\n",
      "Epoch 117/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3076 - accuracy: 0.8700 - val_loss: 2.5299 - val_accuracy: 0.5529\n",
      "Epoch 118/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3064 - accuracy: 0.8698 - val_loss: 2.4560 - val_accuracy: 0.5435\n",
      "Epoch 119/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2998 - accuracy: 0.8727 - val_loss: 2.4648 - val_accuracy: 0.5265\n",
      "Epoch 120/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3110 - accuracy: 0.8676 - val_loss: 2.4912 - val_accuracy: 0.5571\n",
      "Epoch 121/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3232 - accuracy: 0.8633 - val_loss: 2.4661 - val_accuracy: 0.5456\n",
      "Epoch 122/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3101 - accuracy: 0.8684 - val_loss: 2.4987 - val_accuracy: 0.5601\n",
      "Epoch 123/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.2964 - accuracy: 0.8764 - val_loss: 2.5262 - val_accuracy: 0.5677\n",
      "Epoch 124/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3064 - accuracy: 0.8722 - val_loss: 2.4361 - val_accuracy: 0.5631\n",
      "Epoch 125/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2990 - accuracy: 0.8740 - val_loss: 2.4845 - val_accuracy: 0.5575\n",
      "Epoch 126/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3051 - accuracy: 0.8678 - val_loss: 2.4342 - val_accuracy: 0.5507\n",
      "Epoch 127/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.2967 - accuracy: 0.8747 - val_loss: 2.5304 - val_accuracy: 0.5541\n",
      "Epoch 128/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3110 - accuracy: 0.8676 - val_loss: 2.5327 - val_accuracy: 0.5490\n",
      "Epoch 129/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.2991 - accuracy: 0.8718 - val_loss: 2.5907 - val_accuracy: 0.5524\n",
      "Epoch 130/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3141 - accuracy: 0.8667 - val_loss: 2.6000 - val_accuracy: 0.5503\n",
      "Epoch 131/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3210 - accuracy: 0.8638 - val_loss: 2.4634 - val_accuracy: 0.5516\n",
      "Epoch 132/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.2974 - accuracy: 0.8740 - val_loss: 2.5204 - val_accuracy: 0.5605\n",
      "Epoch 133/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2936 - accuracy: 0.8691 - val_loss: 2.5627 - val_accuracy: 0.5626\n",
      "Epoch 134/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2923 - accuracy: 0.8737 - val_loss: 2.5714 - val_accuracy: 0.5669\n",
      "Epoch 135/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2956 - accuracy: 0.8729 - val_loss: 2.5078 - val_accuracy: 0.5499\n",
      "Epoch 136/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3029 - accuracy: 0.8709 - val_loss: 2.5097 - val_accuracy: 0.5656\n",
      "Epoch 137/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3009 - accuracy: 0.8680 - val_loss: 2.4839 - val_accuracy: 0.5520\n",
      "Epoch 138/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.2928 - accuracy: 0.8771 - val_loss: 2.5064 - val_accuracy: 0.5563\n",
      "Epoch 139/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2905 - accuracy: 0.8762 - val_loss: 2.5635 - val_accuracy: 0.5550\n",
      "Epoch 140/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.2986 - accuracy: 0.8744 - val_loss: 2.5400 - val_accuracy: 0.5529\n",
      "Epoch 141/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.3037 - accuracy: 0.8680 - val_loss: 2.5822 - val_accuracy: 0.5456\n",
      "Epoch 142/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2984 - accuracy: 0.8695 - val_loss: 2.5766 - val_accuracy: 0.5550\n",
      "Epoch 143/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3029 - accuracy: 0.8727 - val_loss: 2.5630 - val_accuracy: 0.5533\n",
      "Epoch 144/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2987 - accuracy: 0.8764 - val_loss: 2.5386 - val_accuracy: 0.5571\n",
      "Epoch 145/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2913 - accuracy: 0.8738 - val_loss: 2.6105 - val_accuracy: 0.5673\n",
      "Epoch 146/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2997 - accuracy: 0.8680 - val_loss: 2.5674 - val_accuracy: 0.5558\n",
      "Epoch 147/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3012 - accuracy: 0.8695 - val_loss: 2.5677 - val_accuracy: 0.5486\n",
      "Epoch 148/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2965 - accuracy: 0.8697 - val_loss: 2.6273 - val_accuracy: 0.5503\n",
      "Epoch 149/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3012 - accuracy: 0.8695 - val_loss: 2.6949 - val_accuracy: 0.5605\n",
      "Epoch 150/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3089 - accuracy: 0.8651 - val_loss: 2.5931 - val_accuracy: 0.5418\n",
      "Epoch 151/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2924 - accuracy: 0.8764 - val_loss: 2.6231 - val_accuracy: 0.5503\n",
      "Epoch 152/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2877 - accuracy: 0.8771 - val_loss: 2.6272 - val_accuracy: 0.5151\n",
      "Epoch 153/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2949 - accuracy: 0.8797 - val_loss: 2.6669 - val_accuracy: 0.5673\n",
      "Epoch 154/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2990 - accuracy: 0.8717 - val_loss: 2.6340 - val_accuracy: 0.5512\n",
      "Epoch 155/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2841 - accuracy: 0.8755 - val_loss: 2.6279 - val_accuracy: 0.5567\n",
      "Epoch 156/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2734 - accuracy: 0.8849 - val_loss: 2.6468 - val_accuracy: 0.5618\n",
      "Epoch 157/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2841 - accuracy: 0.8822 - val_loss: 2.5777 - val_accuracy: 0.5503\n",
      "Epoch 158/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2950 - accuracy: 0.8733 - val_loss: 2.6825 - val_accuracy: 0.5554\n",
      "Epoch 159/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2893 - accuracy: 0.8804 - val_loss: 2.6722 - val_accuracy: 0.5609\n",
      "Epoch 160/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2918 - accuracy: 0.8778 - val_loss: 2.6623 - val_accuracy: 0.5541\n",
      "Epoch 161/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2957 - accuracy: 0.8733 - val_loss: 2.7036 - val_accuracy: 0.5660\n",
      "Epoch 162/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2868 - accuracy: 0.8746 - val_loss: 2.6919 - val_accuracy: 0.5601\n",
      "Epoch 163/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2950 - accuracy: 0.8666 - val_loss: 2.7274 - val_accuracy: 0.5618\n",
      "Epoch 164/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2912 - accuracy: 0.8798 - val_loss: 2.6939 - val_accuracy: 0.5444\n",
      "Epoch 165/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2869 - accuracy: 0.8744 - val_loss: 2.6831 - val_accuracy: 0.5473\n",
      "Epoch 166/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3051 - accuracy: 0.8709 - val_loss: 2.7114 - val_accuracy: 0.5677\n",
      "Epoch 167/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2893 - accuracy: 0.8798 - val_loss: 2.7744 - val_accuracy: 0.5499\n",
      "Epoch 168/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2860 - accuracy: 0.8762 - val_loss: 2.6976 - val_accuracy: 0.5427\n",
      "Epoch 169/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2791 - accuracy: 0.8778 - val_loss: 2.6882 - val_accuracy: 0.5490\n",
      "Epoch 170/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2808 - accuracy: 0.8775 - val_loss: 2.7899 - val_accuracy: 0.5550\n",
      "Epoch 171/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2923 - accuracy: 0.8749 - val_loss: 2.6973 - val_accuracy: 0.5558\n",
      "Epoch 172/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2944 - accuracy: 0.8771 - val_loss: 2.7353 - val_accuracy: 0.5588\n",
      "Epoch 173/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.2965 - accuracy: 0.8715 - val_loss: 2.7624 - val_accuracy: 0.5635\n",
      "Epoch 174/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3029 - accuracy: 0.8727 - val_loss: 2.7110 - val_accuracy: 0.5495\n",
      "Epoch 175/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3107 - accuracy: 0.8747 - val_loss: 2.8482 - val_accuracy: 0.5486\n",
      "Epoch 176/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2818 - accuracy: 0.8800 - val_loss: 2.7218 - val_accuracy: 0.5448\n",
      "Epoch 177/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2724 - accuracy: 0.8837 - val_loss: 2.7185 - val_accuracy: 0.5495\n",
      "Epoch 178/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2805 - accuracy: 0.8849 - val_loss: 2.7588 - val_accuracy: 0.5325\n",
      "Epoch 179/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2870 - accuracy: 0.8747 - val_loss: 2.7814 - val_accuracy: 0.5524\n",
      "Epoch 180/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2803 - accuracy: 0.8800 - val_loss: 2.8060 - val_accuracy: 0.5393\n",
      "Epoch 181/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2783 - accuracy: 0.8811 - val_loss: 2.8152 - val_accuracy: 0.5469\n",
      "Epoch 182/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2766 - accuracy: 0.8857 - val_loss: 2.7819 - val_accuracy: 0.5338\n",
      "Epoch 183/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2851 - accuracy: 0.8806 - val_loss: 2.8119 - val_accuracy: 0.5490\n",
      "Epoch 184/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2829 - accuracy: 0.8831 - val_loss: 2.7778 - val_accuracy: 0.5427\n",
      "Epoch 185/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2763 - accuracy: 0.8798 - val_loss: 2.7552 - val_accuracy: 0.5584\n",
      "Epoch 186/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2806 - accuracy: 0.8833 - val_loss: 2.8317 - val_accuracy: 0.5529\n",
      "Epoch 187/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.3034 - accuracy: 0.8697 - val_loss: 2.7575 - val_accuracy: 0.5444\n",
      "Epoch 188/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2837 - accuracy: 0.8786 - val_loss: 2.7045 - val_accuracy: 0.5516\n",
      "Epoch 189/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2877 - accuracy: 0.8766 - val_loss: 2.7930 - val_accuracy: 0.5465\n",
      "Epoch 190/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2853 - accuracy: 0.8769 - val_loss: 2.8224 - val_accuracy: 0.5520\n",
      "Epoch 191/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2974 - accuracy: 0.8698 - val_loss: 2.7783 - val_accuracy: 0.5397\n",
      "Epoch 192/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2763 - accuracy: 0.8855 - val_loss: 2.8287 - val_accuracy: 0.5389\n",
      "Epoch 193/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2804 - accuracy: 0.8833 - val_loss: 2.7762 - val_accuracy: 0.5660\n",
      "Epoch 194/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2750 - accuracy: 0.8820 - val_loss: 2.8106 - val_accuracy: 0.5469\n",
      "Epoch 195/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2835 - accuracy: 0.8851 - val_loss: 2.7840 - val_accuracy: 0.5469\n",
      "Epoch 196/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2780 - accuracy: 0.8818 - val_loss: 2.8265 - val_accuracy: 0.5618\n",
      "Epoch 197/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2767 - accuracy: 0.8839 - val_loss: 2.8209 - val_accuracy: 0.5367\n",
      "Epoch 198/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2739 - accuracy: 0.8848 - val_loss: 2.8108 - val_accuracy: 0.5355\n",
      "Epoch 199/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2649 - accuracy: 0.8862 - val_loss: 2.8414 - val_accuracy: 0.5575\n",
      "Epoch 200/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2756 - accuracy: 0.8828 - val_loss: 2.8443 - val_accuracy: 0.5558\n",
      "Epoch 201/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2717 - accuracy: 0.8842 - val_loss: 2.8033 - val_accuracy: 0.5588\n",
      "Epoch 202/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2779 - accuracy: 0.8829 - val_loss: 2.8992 - val_accuracy: 0.5499\n",
      "Epoch 203/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2769 - accuracy: 0.8818 - val_loss: 2.8599 - val_accuracy: 0.5520\n",
      "Epoch 204/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2750 - accuracy: 0.8848 - val_loss: 2.9375 - val_accuracy: 0.5631\n",
      "Epoch 205/500\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.3047 - accuracy: 0.8698 - val_loss: 2.8246 - val_accuracy: 0.5393\n",
      "Epoch 206/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2916 - accuracy: 0.8747 - val_loss: 2.9028 - val_accuracy: 0.5444\n",
      "Epoch 207/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2760 - accuracy: 0.8829 - val_loss: 2.8014 - val_accuracy: 0.5537\n",
      "Epoch 208/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2675 - accuracy: 0.8868 - val_loss: 2.9414 - val_accuracy: 0.5486\n",
      "Epoch 209/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2912 - accuracy: 0.8791 - val_loss: 2.9347 - val_accuracy: 0.5397\n",
      "Epoch 210/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2820 - accuracy: 0.8800 - val_loss: 2.8380 - val_accuracy: 0.5482\n",
      "Epoch 211/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2756 - accuracy: 0.8813 - val_loss: 2.8481 - val_accuracy: 0.5418\n",
      "Epoch 212/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2654 - accuracy: 0.8895 - val_loss: 2.8447 - val_accuracy: 0.5427\n",
      "Epoch 213/500\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.2737 - accuracy: 0.8813 - val_loss: 2.8616 - val_accuracy: 0.5316\n",
      "Epoch 214/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2755 - accuracy: 0.8818 - val_loss: 2.8701 - val_accuracy: 0.5359\n",
      "Epoch 215/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2810 - accuracy: 0.8804 - val_loss: 2.8214 - val_accuracy: 0.5363\n",
      "Epoch 216/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2723 - accuracy: 0.8817 - val_loss: 2.8240 - val_accuracy: 0.5456\n",
      "Epoch 217/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2665 - accuracy: 0.8840 - val_loss: 2.9276 - val_accuracy: 0.5541\n",
      "Epoch 218/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2732 - accuracy: 0.8822 - val_loss: 2.9032 - val_accuracy: 0.5507\n",
      "Epoch 219/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2623 - accuracy: 0.8944 - val_loss: 2.8421 - val_accuracy: 0.5524\n",
      "Epoch 220/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2612 - accuracy: 0.8857 - val_loss: 2.8860 - val_accuracy: 0.5410\n",
      "Epoch 221/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2647 - accuracy: 0.8880 - val_loss: 2.8966 - val_accuracy: 0.5380\n",
      "Epoch 222/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2835 - accuracy: 0.8778 - val_loss: 2.9007 - val_accuracy: 0.5299\n",
      "Epoch 223/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2629 - accuracy: 0.8877 - val_loss: 2.9377 - val_accuracy: 0.5435\n",
      "Epoch 224/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2682 - accuracy: 0.8871 - val_loss: 2.9499 - val_accuracy: 0.5338\n",
      "Epoch 225/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2675 - accuracy: 0.8842 - val_loss: 3.0152 - val_accuracy: 0.5414\n",
      "Epoch 226/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2837 - accuracy: 0.8822 - val_loss: 2.9343 - val_accuracy: 0.5575\n",
      "Epoch 227/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2853 - accuracy: 0.8839 - val_loss: 2.9454 - val_accuracy: 0.5431\n",
      "Epoch 228/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2782 - accuracy: 0.8813 - val_loss: 2.9721 - val_accuracy: 0.5456\n",
      "Epoch 229/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3146 - accuracy: 0.8626 - val_loss: 3.1559 - val_accuracy: 0.5401\n",
      "Epoch 230/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2913 - accuracy: 0.8749 - val_loss: 3.0491 - val_accuracy: 0.5257\n",
      "Epoch 231/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2690 - accuracy: 0.8835 - val_loss: 3.0075 - val_accuracy: 0.5465\n",
      "Epoch 232/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2530 - accuracy: 0.8926 - val_loss: 2.9967 - val_accuracy: 0.5507\n",
      "Epoch 233/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2616 - accuracy: 0.8895 - val_loss: 2.9791 - val_accuracy: 0.5499\n",
      "Epoch 234/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2625 - accuracy: 0.8873 - val_loss: 2.9620 - val_accuracy: 0.5469\n",
      "Epoch 235/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2552 - accuracy: 0.8933 - val_loss: 3.0029 - val_accuracy: 0.5431\n",
      "Epoch 236/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2546 - accuracy: 0.8939 - val_loss: 3.0091 - val_accuracy: 0.5410\n",
      "Epoch 237/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2588 - accuracy: 0.8873 - val_loss: 2.9702 - val_accuracy: 0.5444\n",
      "Epoch 238/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2617 - accuracy: 0.8897 - val_loss: 3.0803 - val_accuracy: 0.5503\n",
      "Epoch 239/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2602 - accuracy: 0.8899 - val_loss: 3.0035 - val_accuracy: 0.5452\n",
      "Epoch 240/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2509 - accuracy: 0.8928 - val_loss: 2.9796 - val_accuracy: 0.5236\n",
      "Epoch 241/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2608 - accuracy: 0.8911 - val_loss: 2.9780 - val_accuracy: 0.5507\n",
      "Epoch 242/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2652 - accuracy: 0.8866 - val_loss: 3.1281 - val_accuracy: 0.5452\n",
      "Epoch 243/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2652 - accuracy: 0.8846 - val_loss: 3.0986 - val_accuracy: 0.5452\n",
      "Epoch 244/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2517 - accuracy: 0.8879 - val_loss: 3.0200 - val_accuracy: 0.5342\n",
      "Epoch 245/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2513 - accuracy: 0.8900 - val_loss: 3.0608 - val_accuracy: 0.5448\n",
      "Epoch 246/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2603 - accuracy: 0.8873 - val_loss: 3.0823 - val_accuracy: 0.5359\n",
      "Epoch 247/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2664 - accuracy: 0.8817 - val_loss: 2.9651 - val_accuracy: 0.5520\n",
      "Epoch 248/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2564 - accuracy: 0.8899 - val_loss: 3.0745 - val_accuracy: 0.5499\n",
      "Epoch 249/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2693 - accuracy: 0.8828 - val_loss: 3.0894 - val_accuracy: 0.5384\n",
      "Epoch 250/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2565 - accuracy: 0.8957 - val_loss: 3.0363 - val_accuracy: 0.5444\n",
      "Epoch 251/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2611 - accuracy: 0.8851 - val_loss: 3.1680 - val_accuracy: 0.5478\n",
      "Epoch 252/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2837 - accuracy: 0.8813 - val_loss: 3.1131 - val_accuracy: 0.5512\n",
      "Epoch 253/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2630 - accuracy: 0.8920 - val_loss: 3.0551 - val_accuracy: 0.5427\n",
      "Epoch 254/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2510 - accuracy: 0.8897 - val_loss: 3.0768 - val_accuracy: 0.5478\n",
      "Epoch 255/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2695 - accuracy: 0.8875 - val_loss: 3.1093 - val_accuracy: 0.5308\n",
      "Epoch 256/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2784 - accuracy: 0.8813 - val_loss: 3.1121 - val_accuracy: 0.5465\n",
      "Epoch 257/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2695 - accuracy: 0.8860 - val_loss: 3.1243 - val_accuracy: 0.5397\n",
      "Epoch 258/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2625 - accuracy: 0.8846 - val_loss: 3.1828 - val_accuracy: 0.5414\n",
      "Epoch 259/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2471 - accuracy: 0.8937 - val_loss: 3.1848 - val_accuracy: 0.5558\n",
      "Epoch 260/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2538 - accuracy: 0.8931 - val_loss: 3.0838 - val_accuracy: 0.5427\n",
      "Epoch 261/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2482 - accuracy: 0.8944 - val_loss: 3.1514 - val_accuracy: 0.5410\n",
      "Epoch 262/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2520 - accuracy: 0.8913 - val_loss: 3.1671 - val_accuracy: 0.5512\n",
      "Epoch 263/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2514 - accuracy: 0.8979 - val_loss: 3.1067 - val_accuracy: 0.5465\n",
      "Epoch 264/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2461 - accuracy: 0.8981 - val_loss: 3.1482 - val_accuracy: 0.5495\n",
      "Epoch 265/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2562 - accuracy: 0.8902 - val_loss: 3.1478 - val_accuracy: 0.5465\n",
      "Epoch 266/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2576 - accuracy: 0.8913 - val_loss: 3.1801 - val_accuracy: 0.5533\n",
      "Epoch 267/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2763 - accuracy: 0.8857 - val_loss: 3.1403 - val_accuracy: 0.5372\n",
      "Epoch 268/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2802 - accuracy: 0.8837 - val_loss: 3.2090 - val_accuracy: 0.5473\n",
      "Epoch 269/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2643 - accuracy: 0.8864 - val_loss: 3.2107 - val_accuracy: 0.5473\n",
      "Epoch 270/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2634 - accuracy: 0.8882 - val_loss: 3.1074 - val_accuracy: 0.5435\n",
      "Epoch 271/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2506 - accuracy: 0.8939 - val_loss: 3.1789 - val_accuracy: 0.5448\n",
      "Epoch 272/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2447 - accuracy: 0.8986 - val_loss: 3.1103 - val_accuracy: 0.5537\n",
      "Epoch 273/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2597 - accuracy: 0.8868 - val_loss: 3.1638 - val_accuracy: 0.5486\n",
      "Epoch 274/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2492 - accuracy: 0.8960 - val_loss: 3.2356 - val_accuracy: 0.5473\n",
      "Epoch 275/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2580 - accuracy: 0.8910 - val_loss: 3.1413 - val_accuracy: 0.5414\n",
      "Epoch 276/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2476 - accuracy: 0.8955 - val_loss: 3.2077 - val_accuracy: 0.5567\n",
      "Epoch 277/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2424 - accuracy: 0.8999 - val_loss: 3.1449 - val_accuracy: 0.5367\n",
      "Epoch 278/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2857 - accuracy: 0.8758 - val_loss: 3.2414 - val_accuracy: 0.5465\n",
      "Epoch 279/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2653 - accuracy: 0.8888 - val_loss: 3.2606 - val_accuracy: 0.5482\n",
      "Epoch 280/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2570 - accuracy: 0.8869 - val_loss: 3.2155 - val_accuracy: 0.5558\n",
      "Epoch 281/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2530 - accuracy: 0.8939 - val_loss: 3.4086 - val_accuracy: 0.5546\n",
      "Epoch 282/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2672 - accuracy: 0.8839 - val_loss: 3.2695 - val_accuracy: 0.5507\n",
      "Epoch 283/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2380 - accuracy: 0.9010 - val_loss: 3.1969 - val_accuracy: 0.5431\n",
      "Epoch 284/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2451 - accuracy: 0.8962 - val_loss: 3.2747 - val_accuracy: 0.5418\n",
      "Epoch 285/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2648 - accuracy: 0.8886 - val_loss: 3.3259 - val_accuracy: 0.5486\n",
      "Epoch 286/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2544 - accuracy: 0.8939 - val_loss: 3.1689 - val_accuracy: 0.5333\n",
      "Epoch 287/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2375 - accuracy: 0.8991 - val_loss: 3.1365 - val_accuracy: 0.5431\n",
      "Epoch 288/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2439 - accuracy: 0.8990 - val_loss: 3.2522 - val_accuracy: 0.5512\n",
      "Epoch 289/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2629 - accuracy: 0.8846 - val_loss: 3.2450 - val_accuracy: 0.5614\n",
      "Epoch 290/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2491 - accuracy: 0.8944 - val_loss: 3.2903 - val_accuracy: 0.5465\n",
      "Epoch 291/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2490 - accuracy: 0.8942 - val_loss: 3.2712 - val_accuracy: 0.5503\n",
      "Epoch 292/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2527 - accuracy: 0.8908 - val_loss: 3.2458 - val_accuracy: 0.5537\n",
      "Epoch 293/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2597 - accuracy: 0.8899 - val_loss: 3.2776 - val_accuracy: 0.5401\n",
      "Epoch 294/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2589 - accuracy: 0.8893 - val_loss: 3.3176 - val_accuracy: 0.5376\n",
      "Epoch 295/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2435 - accuracy: 0.8986 - val_loss: 3.2917 - val_accuracy: 0.5410\n",
      "Epoch 296/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2357 - accuracy: 0.9004 - val_loss: 3.3034 - val_accuracy: 0.5439\n",
      "Epoch 297/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2385 - accuracy: 0.9013 - val_loss: 3.3293 - val_accuracy: 0.5524\n",
      "Epoch 298/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2418 - accuracy: 0.8966 - val_loss: 3.2176 - val_accuracy: 0.5342\n",
      "Epoch 299/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2895 - accuracy: 0.8729 - val_loss: 3.3144 - val_accuracy: 0.5214\n",
      "Epoch 300/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2645 - accuracy: 0.8848 - val_loss: 3.3891 - val_accuracy: 0.5427\n",
      "Epoch 301/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2620 - accuracy: 0.8875 - val_loss: 3.4480 - val_accuracy: 0.5138\n",
      "Epoch 302/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2657 - accuracy: 0.8868 - val_loss: 3.2937 - val_accuracy: 0.5287\n",
      "Epoch 303/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2366 - accuracy: 0.9022 - val_loss: 3.2778 - val_accuracy: 0.5257\n",
      "Epoch 304/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2319 - accuracy: 0.8991 - val_loss: 3.2638 - val_accuracy: 0.5448\n",
      "Epoch 305/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2308 - accuracy: 0.9052 - val_loss: 3.2517 - val_accuracy: 0.5393\n",
      "Epoch 306/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2400 - accuracy: 0.8975 - val_loss: 3.3217 - val_accuracy: 0.5465\n",
      "Epoch 307/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.2390 - accuracy: 0.9004 - val_loss: 3.3426 - val_accuracy: 0.5516\n",
      "Epoch 308/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2395 - accuracy: 0.8984 - val_loss: 3.3766 - val_accuracy: 0.5312\n",
      "Epoch 309/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2531 - accuracy: 0.8931 - val_loss: 3.2741 - val_accuracy: 0.5448\n",
      "Epoch 310/500\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.2426 - accuracy: 0.9001 - val_loss: 3.3468 - val_accuracy: 0.5219\n",
      "Epoch 311/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2574 - accuracy: 0.8891 - val_loss: 3.3477 - val_accuracy: 0.5418\n",
      "Epoch 312/500\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2626 - accuracy: 0.8882 - val_loss: 3.4212 - val_accuracy: 0.5363\n",
      "Epoch 313/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2455 - accuracy: 0.8928 - val_loss: 3.5799 - val_accuracy: 0.5558\n",
      "Epoch 314/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2339 - accuracy: 0.9044 - val_loss: 3.4552 - val_accuracy: 0.5257\n",
      "Epoch 315/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2393 - accuracy: 0.8990 - val_loss: 3.3572 - val_accuracy: 0.5431\n",
      "Epoch 316/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2441 - accuracy: 0.8971 - val_loss: 3.4159 - val_accuracy: 0.5567\n",
      "Epoch 317/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2422 - accuracy: 0.8968 - val_loss: 3.4246 - val_accuracy: 0.5333\n",
      "Epoch 318/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2366 - accuracy: 0.8993 - val_loss: 3.3687 - val_accuracy: 0.5342\n",
      "Epoch 319/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2291 - accuracy: 0.9053 - val_loss: 3.4005 - val_accuracy: 0.5452\n",
      "Epoch 320/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2418 - accuracy: 0.8970 - val_loss: 3.4282 - val_accuracy: 0.5448\n",
      "Epoch 321/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2401 - accuracy: 0.8964 - val_loss: 3.3896 - val_accuracy: 0.5350\n",
      "Epoch 322/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2531 - accuracy: 0.8946 - val_loss: 3.3268 - val_accuracy: 0.5418\n",
      "Epoch 323/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2381 - accuracy: 0.8993 - val_loss: 3.4658 - val_accuracy: 0.5414\n",
      "Epoch 324/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2627 - accuracy: 0.8880 - val_loss: 3.5003 - val_accuracy: 0.5130\n",
      "Epoch 325/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2654 - accuracy: 0.8880 - val_loss: 3.4445 - val_accuracy: 0.5389\n",
      "Epoch 326/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2363 - accuracy: 0.9011 - val_loss: 3.5535 - val_accuracy: 0.5558\n",
      "Epoch 327/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2439 - accuracy: 0.8966 - val_loss: 3.4656 - val_accuracy: 0.5384\n",
      "Epoch 328/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2267 - accuracy: 0.9052 - val_loss: 3.4541 - val_accuracy: 0.5265\n",
      "Epoch 329/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2250 - accuracy: 0.9061 - val_loss: 3.4431 - val_accuracy: 0.5495\n",
      "Epoch 330/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2353 - accuracy: 0.9017 - val_loss: 3.4908 - val_accuracy: 0.5444\n",
      "Epoch 331/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.2240 - accuracy: 0.9050 - val_loss: 3.4835 - val_accuracy: 0.5342\n",
      "Epoch 332/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2242 - accuracy: 0.9011 - val_loss: 3.5686 - val_accuracy: 0.5414\n",
      "Epoch 333/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2239 - accuracy: 0.9073 - val_loss: 3.5161 - val_accuracy: 0.5397\n",
      "Epoch 334/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2371 - accuracy: 0.8995 - val_loss: 3.5018 - val_accuracy: 0.5495\n",
      "Epoch 335/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2244 - accuracy: 0.9041 - val_loss: 3.5543 - val_accuracy: 0.5516\n",
      "Epoch 336/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2331 - accuracy: 0.9026 - val_loss: 3.5009 - val_accuracy: 0.5389\n",
      "Epoch 337/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2415 - accuracy: 0.8959 - val_loss: 3.5890 - val_accuracy: 0.5567\n",
      "Epoch 338/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2300 - accuracy: 0.9035 - val_loss: 3.5408 - val_accuracy: 0.5524\n",
      "Epoch 339/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2353 - accuracy: 0.9022 - val_loss: 3.4504 - val_accuracy: 0.5359\n",
      "Epoch 340/500\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2333 - accuracy: 0.8995 - val_loss: 3.5105 - val_accuracy: 0.5486\n",
      "Epoch 341/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2323 - accuracy: 0.9030 - val_loss: 3.4292 - val_accuracy: 0.5410\n",
      "Epoch 342/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.2257 - accuracy: 0.9028 - val_loss: 3.5230 - val_accuracy: 0.5397\n",
      "Epoch 343/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2226 - accuracy: 0.9050 - val_loss: 3.5947 - val_accuracy: 0.5350\n",
      "Epoch 344/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2248 - accuracy: 0.9059 - val_loss: 3.5552 - val_accuracy: 0.5363\n",
      "Epoch 345/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2420 - accuracy: 0.8982 - val_loss: 3.4807 - val_accuracy: 0.5418\n",
      "Epoch 346/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.2329 - accuracy: 0.9046 - val_loss: 3.5226 - val_accuracy: 0.5355\n",
      "Epoch 347/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2398 - accuracy: 0.8953 - val_loss: 3.5657 - val_accuracy: 0.5439\n",
      "Epoch 348/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.2470 - accuracy: 0.8999 - val_loss: 3.5977 - val_accuracy: 0.5444\n",
      "Epoch 349/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.2561 - accuracy: 0.8939 - val_loss: 3.6385 - val_accuracy: 0.5546\n",
      "Epoch 350/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.2380 - accuracy: 0.9017 - val_loss: 3.5065 - val_accuracy: 0.5427\n",
      "Epoch 351/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.2453 - accuracy: 0.8982 - val_loss: 3.5422 - val_accuracy: 0.5448\n",
      "Epoch 352/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2455 - accuracy: 0.8971 - val_loss: 3.6183 - val_accuracy: 0.5214\n",
      "Epoch 353/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2333 - accuracy: 0.8948 - val_loss: 3.5365 - val_accuracy: 0.5321\n",
      "Epoch 354/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2245 - accuracy: 0.9039 - val_loss: 3.5220 - val_accuracy: 0.5533\n",
      "Epoch 355/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2184 - accuracy: 0.9081 - val_loss: 3.6322 - val_accuracy: 0.5507\n",
      "Epoch 356/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.2384 - accuracy: 0.8984 - val_loss: 3.5300 - val_accuracy: 0.5321\n",
      "Epoch 357/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2458 - accuracy: 0.8973 - val_loss: 3.9407 - val_accuracy: 0.5291\n",
      "Epoch 358/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.3043 - accuracy: 0.8755 - val_loss: 3.6040 - val_accuracy: 0.5282\n",
      "Epoch 359/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2445 - accuracy: 0.8971 - val_loss: 3.5644 - val_accuracy: 0.5439\n",
      "Epoch 360/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2413 - accuracy: 0.8979 - val_loss: 3.6959 - val_accuracy: 0.5414\n",
      "Epoch 361/500\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.2214 - accuracy: 0.9061 - val_loss: 3.5737 - val_accuracy: 0.5282\n",
      "Epoch 362/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.2264 - accuracy: 0.9026 - val_loss: 3.6253 - val_accuracy: 0.5456\n",
      "Epoch 363/500\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.2426 - accuracy: 0.8951 - val_loss: 3.6384 - val_accuracy: 0.5193\n",
      "Epoch 364/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2327 - accuracy: 0.9015 - val_loss: 3.5320 - val_accuracy: 0.5397\n",
      "Epoch 365/500\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.2225 - accuracy: 0.9079 - val_loss: 3.6027 - val_accuracy: 0.5376\n",
      "Epoch 366/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2159 - accuracy: 0.9108 - val_loss: 3.5569 - val_accuracy: 0.5482\n",
      "Epoch 367/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2269 - accuracy: 0.9031 - val_loss: 3.7771 - val_accuracy: 0.5592\n",
      "Epoch 368/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2205 - accuracy: 0.9079 - val_loss: 3.6385 - val_accuracy: 0.5401\n",
      "Epoch 369/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2183 - accuracy: 0.9101 - val_loss: 3.6454 - val_accuracy: 0.5461\n",
      "Epoch 370/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2339 - accuracy: 0.9031 - val_loss: 3.6799 - val_accuracy: 0.5499\n",
      "Epoch 371/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2209 - accuracy: 0.9053 - val_loss: 3.5814 - val_accuracy: 0.5384\n",
      "Epoch 372/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2127 - accuracy: 0.9102 - val_loss: 3.5581 - val_accuracy: 0.5308\n",
      "Epoch 373/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2223 - accuracy: 0.9090 - val_loss: 3.7465 - val_accuracy: 0.5253\n",
      "Epoch 374/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.2209 - accuracy: 0.9073 - val_loss: 3.6429 - val_accuracy: 0.5274\n",
      "Epoch 375/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.2374 - accuracy: 0.8964 - val_loss: 3.7042 - val_accuracy: 0.5333\n",
      "Epoch 376/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2337 - accuracy: 0.9042 - val_loss: 3.6717 - val_accuracy: 0.5274\n",
      "Epoch 377/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.2256 - accuracy: 0.9044 - val_loss: 3.6985 - val_accuracy: 0.5465\n",
      "Epoch 378/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2391 - accuracy: 0.9030 - val_loss: 3.5806 - val_accuracy: 0.5444\n",
      "Epoch 379/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2224 - accuracy: 0.9077 - val_loss: 3.5951 - val_accuracy: 0.5478\n",
      "Epoch 380/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2261 - accuracy: 0.9066 - val_loss: 3.6844 - val_accuracy: 0.5282\n",
      "Epoch 381/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2221 - accuracy: 0.9062 - val_loss: 3.6126 - val_accuracy: 0.5448\n",
      "Epoch 382/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2230 - accuracy: 0.9059 - val_loss: 3.6168 - val_accuracy: 0.5550\n",
      "Epoch 383/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2247 - accuracy: 0.9064 - val_loss: 3.7050 - val_accuracy: 0.5439\n",
      "Epoch 384/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2238 - accuracy: 0.9075 - val_loss: 3.7201 - val_accuracy: 0.5363\n",
      "Epoch 385/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2221 - accuracy: 0.9030 - val_loss: 3.6560 - val_accuracy: 0.5346\n",
      "Epoch 386/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2233 - accuracy: 0.9090 - val_loss: 3.6809 - val_accuracy: 0.5499\n",
      "Epoch 387/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2143 - accuracy: 0.9073 - val_loss: 3.6822 - val_accuracy: 0.5338\n",
      "Epoch 388/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2115 - accuracy: 0.9153 - val_loss: 3.7407 - val_accuracy: 0.5176\n",
      "Epoch 389/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2138 - accuracy: 0.9124 - val_loss: 3.6868 - val_accuracy: 0.5473\n",
      "Epoch 390/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2026 - accuracy: 0.9133 - val_loss: 3.7158 - val_accuracy: 0.5376\n",
      "Epoch 391/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2189 - accuracy: 0.9077 - val_loss: 3.7266 - val_accuracy: 0.5291\n",
      "Epoch 392/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2236 - accuracy: 0.9048 - val_loss: 3.7830 - val_accuracy: 0.5465\n",
      "Epoch 393/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2171 - accuracy: 0.9064 - val_loss: 3.7899 - val_accuracy: 0.5329\n",
      "Epoch 394/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2195 - accuracy: 0.9066 - val_loss: 3.8179 - val_accuracy: 0.5333\n",
      "Epoch 395/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2227 - accuracy: 0.9044 - val_loss: 3.7576 - val_accuracy: 0.5456\n",
      "Epoch 396/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2087 - accuracy: 0.9106 - val_loss: 3.7893 - val_accuracy: 0.5401\n",
      "Epoch 397/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2185 - accuracy: 0.9090 - val_loss: 3.7047 - val_accuracy: 0.5338\n",
      "Epoch 398/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2447 - accuracy: 0.8959 - val_loss: 3.7621 - val_accuracy: 0.5125\n",
      "Epoch 399/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2256 - accuracy: 0.9092 - val_loss: 3.9012 - val_accuracy: 0.5325\n",
      "Epoch 400/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2594 - accuracy: 0.8917 - val_loss: 3.8714 - val_accuracy: 0.5507\n",
      "Epoch 401/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2438 - accuracy: 0.8939 - val_loss: 3.8582 - val_accuracy: 0.5265\n",
      "Epoch 402/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2227 - accuracy: 0.9031 - val_loss: 3.8441 - val_accuracy: 0.5439\n",
      "Epoch 403/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2156 - accuracy: 0.9102 - val_loss: 3.8296 - val_accuracy: 0.5469\n",
      "Epoch 404/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2166 - accuracy: 0.9124 - val_loss: 3.8414 - val_accuracy: 0.5461\n",
      "Epoch 405/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2138 - accuracy: 0.9079 - val_loss: 3.8596 - val_accuracy: 0.5512\n",
      "Epoch 406/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2186 - accuracy: 0.9079 - val_loss: 3.8512 - val_accuracy: 0.5410\n",
      "Epoch 407/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2004 - accuracy: 0.9172 - val_loss: 3.7941 - val_accuracy: 0.5410\n",
      "Epoch 408/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2135 - accuracy: 0.9115 - val_loss: 3.8037 - val_accuracy: 0.5380\n",
      "Epoch 409/500\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.2089 - accuracy: 0.9159 - val_loss: 3.8693 - val_accuracy: 0.5490\n",
      "Epoch 410/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2032 - accuracy: 0.9148 - val_loss: 3.8353 - val_accuracy: 0.5372\n",
      "Epoch 411/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2079 - accuracy: 0.9132 - val_loss: 3.9020 - val_accuracy: 0.5524\n",
      "Epoch 412/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2047 - accuracy: 0.9144 - val_loss: 3.8359 - val_accuracy: 0.5376\n",
      "Epoch 413/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2003 - accuracy: 0.9201 - val_loss: 3.7890 - val_accuracy: 0.5439\n",
      "Epoch 414/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2049 - accuracy: 0.9128 - val_loss: 3.8599 - val_accuracy: 0.5376\n",
      "Epoch 415/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2092 - accuracy: 0.9161 - val_loss: 3.8981 - val_accuracy: 0.5465\n",
      "Epoch 416/500\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.2069 - accuracy: 0.9141 - val_loss: 3.8298 - val_accuracy: 0.5312\n",
      "Epoch 417/500\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2085 - accuracy: 0.9121 - val_loss: 3.7892 - val_accuracy: 0.5321\n",
      "Epoch 418/500\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.2125 - accuracy: 0.9112 - val_loss: 3.8278 - val_accuracy: 0.5329\n",
      "Epoch 419/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2364 - accuracy: 0.9011 - val_loss: 3.9712 - val_accuracy: 0.5248\n",
      "Epoch 420/500\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.2946 - accuracy: 0.8786 - val_loss: 3.9728 - val_accuracy: 0.5414\n",
      "Epoch 421/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2497 - accuracy: 0.8973 - val_loss: 3.8611 - val_accuracy: 0.5414\n",
      "Epoch 422/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2360 - accuracy: 0.8999 - val_loss: 3.8686 - val_accuracy: 0.5325\n",
      "Epoch 423/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2065 - accuracy: 0.9183 - val_loss: 3.9507 - val_accuracy: 0.5478\n",
      "Epoch 424/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2001 - accuracy: 0.9139 - val_loss: 3.8635 - val_accuracy: 0.5393\n",
      "Epoch 425/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2058 - accuracy: 0.9157 - val_loss: 3.9513 - val_accuracy: 0.5363\n",
      "Epoch 426/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2060 - accuracy: 0.9148 - val_loss: 3.8578 - val_accuracy: 0.5401\n",
      "Epoch 427/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2102 - accuracy: 0.9123 - val_loss: 3.8389 - val_accuracy: 0.5312\n",
      "Epoch 428/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2045 - accuracy: 0.9179 - val_loss: 4.0362 - val_accuracy: 0.5529\n",
      "Epoch 429/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2111 - accuracy: 0.9101 - val_loss: 3.8885 - val_accuracy: 0.5308\n",
      "Epoch 430/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2071 - accuracy: 0.9139 - val_loss: 3.9544 - val_accuracy: 0.5367\n",
      "Epoch 431/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2121 - accuracy: 0.9084 - val_loss: 4.0026 - val_accuracy: 0.5414\n",
      "Epoch 432/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2109 - accuracy: 0.9115 - val_loss: 3.9652 - val_accuracy: 0.5342\n",
      "Epoch 433/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.1905 - accuracy: 0.9179 - val_loss: 3.9554 - val_accuracy: 0.5414\n",
      "Epoch 434/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2002 - accuracy: 0.9184 - val_loss: 4.0124 - val_accuracy: 0.5444\n",
      "Epoch 435/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2104 - accuracy: 0.9088 - val_loss: 3.9058 - val_accuracy: 0.5350\n",
      "Epoch 436/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2176 - accuracy: 0.9110 - val_loss: 3.9580 - val_accuracy: 0.5427\n",
      "Epoch 437/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2330 - accuracy: 0.9002 - val_loss: 4.1632 - val_accuracy: 0.5410\n",
      "Epoch 438/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2764 - accuracy: 0.8879 - val_loss: 4.1211 - val_accuracy: 0.5231\n",
      "Epoch 439/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2278 - accuracy: 0.9068 - val_loss: 4.0536 - val_accuracy: 0.5435\n",
      "Epoch 440/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2081 - accuracy: 0.9124 - val_loss: 3.9618 - val_accuracy: 0.5333\n",
      "Epoch 441/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2130 - accuracy: 0.9133 - val_loss: 4.0649 - val_accuracy: 0.5512\n",
      "Epoch 442/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2107 - accuracy: 0.9104 - val_loss: 4.1804 - val_accuracy: 0.5435\n",
      "Epoch 443/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2091 - accuracy: 0.9121 - val_loss: 4.0319 - val_accuracy: 0.5091\n",
      "Epoch 444/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2079 - accuracy: 0.9132 - val_loss: 4.0401 - val_accuracy: 0.5414\n",
      "Epoch 445/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.1937 - accuracy: 0.9224 - val_loss: 4.0911 - val_accuracy: 0.5537\n",
      "Epoch 446/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2012 - accuracy: 0.9139 - val_loss: 3.9471 - val_accuracy: 0.5270\n",
      "Epoch 447/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2023 - accuracy: 0.9181 - val_loss: 4.0187 - val_accuracy: 0.5461\n",
      "Epoch 448/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1944 - accuracy: 0.9148 - val_loss: 4.0216 - val_accuracy: 0.5346\n",
      "Epoch 449/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2060 - accuracy: 0.9110 - val_loss: 4.0180 - val_accuracy: 0.5397\n",
      "Epoch 450/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.2156 - accuracy: 0.9102 - val_loss: 4.0837 - val_accuracy: 0.5342\n",
      "Epoch 451/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.2180 - accuracy: 0.9061 - val_loss: 4.0681 - val_accuracy: 0.5367\n",
      "Epoch 452/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2610 - accuracy: 0.8926 - val_loss: 4.1166 - val_accuracy: 0.5333\n",
      "Epoch 453/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2420 - accuracy: 0.9002 - val_loss: 4.0607 - val_accuracy: 0.5406\n",
      "Epoch 454/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2110 - accuracy: 0.9150 - val_loss: 4.1105 - val_accuracy: 0.5507\n",
      "Epoch 455/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.1924 - accuracy: 0.9203 - val_loss: 4.0867 - val_accuracy: 0.5456\n",
      "Epoch 456/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.1931 - accuracy: 0.9203 - val_loss: 4.0376 - val_accuracy: 0.5316\n",
      "Epoch 457/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1981 - accuracy: 0.9152 - val_loss: 4.1089 - val_accuracy: 0.5401\n",
      "Epoch 458/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1979 - accuracy: 0.9179 - val_loss: 4.1811 - val_accuracy: 0.5469\n",
      "Epoch 459/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1955 - accuracy: 0.9204 - val_loss: 4.2053 - val_accuracy: 0.5359\n",
      "Epoch 460/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1976 - accuracy: 0.9186 - val_loss: 4.1103 - val_accuracy: 0.5363\n",
      "Epoch 461/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1907 - accuracy: 0.9197 - val_loss: 4.2048 - val_accuracy: 0.5227\n",
      "Epoch 462/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2170 - accuracy: 0.9021 - val_loss: 4.0621 - val_accuracy: 0.5397\n",
      "Epoch 463/500\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.1939 - accuracy: 0.9181 - val_loss: 4.1393 - val_accuracy: 0.5355\n",
      "Epoch 464/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2029 - accuracy: 0.9153 - val_loss: 4.1111 - val_accuracy: 0.5418\n",
      "Epoch 465/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2005 - accuracy: 0.9175 - val_loss: 4.0871 - val_accuracy: 0.5270\n",
      "Epoch 466/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2062 - accuracy: 0.9143 - val_loss: 4.1599 - val_accuracy: 0.5435\n",
      "Epoch 467/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2120 - accuracy: 0.9115 - val_loss: 4.2563 - val_accuracy: 0.5359\n",
      "Epoch 468/500\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.2005 - accuracy: 0.9173 - val_loss: 4.1242 - val_accuracy: 0.5299\n",
      "Epoch 469/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2123 - accuracy: 0.9081 - val_loss: 4.1948 - val_accuracy: 0.5350\n",
      "Epoch 470/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2254 - accuracy: 0.9053 - val_loss: 4.0583 - val_accuracy: 0.5367\n",
      "Epoch 471/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2161 - accuracy: 0.9106 - val_loss: 4.3029 - val_accuracy: 0.5482\n",
      "Epoch 472/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2057 - accuracy: 0.9121 - val_loss: 4.1584 - val_accuracy: 0.5495\n",
      "Epoch 473/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2036 - accuracy: 0.9170 - val_loss: 4.2803 - val_accuracy: 0.5448\n",
      "Epoch 474/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2147 - accuracy: 0.9115 - val_loss: 4.4375 - val_accuracy: 0.5418\n",
      "Epoch 475/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2502 - accuracy: 0.9021 - val_loss: 4.2641 - val_accuracy: 0.5355\n",
      "Epoch 476/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2542 - accuracy: 0.8951 - val_loss: 4.2972 - val_accuracy: 0.5282\n",
      "Epoch 477/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2124 - accuracy: 0.9132 - val_loss: 4.2048 - val_accuracy: 0.5304\n",
      "Epoch 478/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.1973 - accuracy: 0.9194 - val_loss: 4.1704 - val_accuracy: 0.5248\n",
      "Epoch 479/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.1885 - accuracy: 0.9194 - val_loss: 4.2305 - val_accuracy: 0.5295\n",
      "Epoch 480/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.1830 - accuracy: 0.9261 - val_loss: 4.2328 - val_accuracy: 0.5333\n",
      "Epoch 481/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1793 - accuracy: 0.9263 - val_loss: 4.2658 - val_accuracy: 0.5261\n",
      "Epoch 482/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1859 - accuracy: 0.9212 - val_loss: 4.2445 - val_accuracy: 0.5461\n",
      "Epoch 483/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2094 - accuracy: 0.9110 - val_loss: 4.2603 - val_accuracy: 0.5389\n",
      "Epoch 484/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2037 - accuracy: 0.9132 - val_loss: 4.1690 - val_accuracy: 0.5299\n",
      "Epoch 485/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1947 - accuracy: 0.9173 - val_loss: 4.2420 - val_accuracy: 0.5393\n",
      "Epoch 486/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2074 - accuracy: 0.9150 - val_loss: 4.2764 - val_accuracy: 0.5444\n",
      "Epoch 487/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1909 - accuracy: 0.9186 - val_loss: 4.3733 - val_accuracy: 0.5299\n",
      "Epoch 488/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.1927 - accuracy: 0.9181 - val_loss: 4.2779 - val_accuracy: 0.5427\n",
      "Epoch 489/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1868 - accuracy: 0.9224 - val_loss: 4.3268 - val_accuracy: 0.5316\n",
      "Epoch 490/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.1961 - accuracy: 0.9208 - val_loss: 4.2523 - val_accuracy: 0.5295\n",
      "Epoch 491/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.1839 - accuracy: 0.9257 - val_loss: 4.3913 - val_accuracy: 0.5389\n",
      "Epoch 492/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1942 - accuracy: 0.9215 - val_loss: 4.2694 - val_accuracy: 0.5435\n",
      "Epoch 493/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.1917 - accuracy: 0.9199 - val_loss: 4.3522 - val_accuracy: 0.5346\n",
      "Epoch 494/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.1902 - accuracy: 0.9212 - val_loss: 4.3616 - val_accuracy: 0.5486\n",
      "Epoch 495/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.1857 - accuracy: 0.9257 - val_loss: 4.2083 - val_accuracy: 0.5291\n",
      "Epoch 496/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.1952 - accuracy: 0.9148 - val_loss: 4.3554 - val_accuracy: 0.5338\n",
      "Epoch 497/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2484 - accuracy: 0.9017 - val_loss: 4.2620 - val_accuracy: 0.5346\n",
      "Epoch 498/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2205 - accuracy: 0.9113 - val_loss: 4.3243 - val_accuracy: 0.5316\n",
      "Epoch 499/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.2853 - accuracy: 0.8862 - val_loss: 4.2767 - val_accuracy: 0.5291\n",
      "Epoch 500/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.2184 - accuracy: 0.9108 - val_loss: 4.2385 - val_accuracy: 0.5202\n",
      "74/74 [==============================] - 0s 2ms/step\n",
      "74/74 [==============================] - 0s 2ms/step\n",
      "Training dense_model\n",
      "Epoch 1/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5506 - accuracy: 0.7641 - val_loss: 1.1295 - val_accuracy: 0.5958\n",
      "Epoch 2/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5543 - accuracy: 0.7586 - val_loss: 1.1336 - val_accuracy: 0.5911\n",
      "Epoch 3/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5519 - accuracy: 0.7622 - val_loss: 1.1393 - val_accuracy: 0.5975\n",
      "Epoch 4/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5585 - accuracy: 0.7566 - val_loss: 1.1499 - val_accuracy: 0.6004\n",
      "Epoch 5/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5539 - accuracy: 0.7612 - val_loss: 1.1621 - val_accuracy: 0.5949\n",
      "Epoch 6/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5508 - accuracy: 0.7633 - val_loss: 1.1953 - val_accuracy: 0.6042\n",
      "Epoch 7/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5539 - accuracy: 0.7610 - val_loss: 1.1616 - val_accuracy: 0.5992\n",
      "Epoch 8/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5535 - accuracy: 0.7632 - val_loss: 1.1578 - val_accuracy: 0.6013\n",
      "Epoch 9/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5463 - accuracy: 0.7650 - val_loss: 1.1605 - val_accuracy: 0.6017\n",
      "Epoch 10/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5563 - accuracy: 0.7571 - val_loss: 1.1649 - val_accuracy: 0.5983\n",
      "Epoch 11/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5460 - accuracy: 0.7650 - val_loss: 1.1522 - val_accuracy: 0.6081\n",
      "Epoch 12/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5473 - accuracy: 0.7581 - val_loss: 1.1434 - val_accuracy: 0.6072\n",
      "Epoch 13/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5484 - accuracy: 0.7633 - val_loss: 1.1664 - val_accuracy: 0.6000\n",
      "Epoch 14/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5523 - accuracy: 0.7619 - val_loss: 1.2003 - val_accuracy: 0.5868\n",
      "Epoch 15/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5479 - accuracy: 0.7630 - val_loss: 1.1552 - val_accuracy: 0.6000\n",
      "Epoch 16/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5500 - accuracy: 0.7613 - val_loss: 1.2186 - val_accuracy: 0.5941\n",
      "Epoch 17/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5451 - accuracy: 0.7648 - val_loss: 1.1707 - val_accuracy: 0.5962\n",
      "Epoch 18/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5399 - accuracy: 0.7655 - val_loss: 1.1683 - val_accuracy: 0.5915\n",
      "Epoch 19/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5409 - accuracy: 0.7661 - val_loss: 1.1885 - val_accuracy: 0.5945\n",
      "Epoch 20/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5445 - accuracy: 0.7599 - val_loss: 1.1881 - val_accuracy: 0.5885\n",
      "Epoch 21/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5479 - accuracy: 0.7633 - val_loss: 1.1929 - val_accuracy: 0.6042\n",
      "Epoch 22/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5489 - accuracy: 0.7641 - val_loss: 1.2142 - val_accuracy: 0.5932\n",
      "Epoch 23/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5538 - accuracy: 0.7571 - val_loss: 1.1865 - val_accuracy: 0.5805\n",
      "Epoch 24/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5432 - accuracy: 0.7621 - val_loss: 1.1923 - val_accuracy: 0.6051\n",
      "Epoch 25/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5411 - accuracy: 0.7675 - val_loss: 1.1716 - val_accuracy: 0.5915\n",
      "Epoch 26/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5386 - accuracy: 0.7673 - val_loss: 1.2400 - val_accuracy: 0.5953\n",
      "Epoch 27/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5364 - accuracy: 0.7693 - val_loss: 1.2023 - val_accuracy: 0.5877\n",
      "Epoch 28/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5401 - accuracy: 0.7655 - val_loss: 1.2070 - val_accuracy: 0.6051\n",
      "Epoch 29/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5376 - accuracy: 0.7704 - val_loss: 1.2085 - val_accuracy: 0.6004\n",
      "Epoch 30/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5408 - accuracy: 0.7683 - val_loss: 1.2084 - val_accuracy: 0.5868\n",
      "Epoch 31/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5461 - accuracy: 0.7635 - val_loss: 1.2067 - val_accuracy: 0.5864\n",
      "Epoch 32/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7719 - val_loss: 1.2141 - val_accuracy: 0.5919\n",
      "Epoch 33/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5391 - accuracy: 0.7628 - val_loss: 1.2241 - val_accuracy: 0.5970\n",
      "Epoch 34/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5406 - accuracy: 0.7710 - val_loss: 1.1944 - val_accuracy: 0.5983\n",
      "Epoch 35/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5345 - accuracy: 0.7684 - val_loss: 1.1845 - val_accuracy: 0.5915\n",
      "Epoch 36/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5378 - accuracy: 0.7633 - val_loss: 1.2045 - val_accuracy: 0.6051\n",
      "Epoch 37/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5356 - accuracy: 0.7712 - val_loss: 1.1875 - val_accuracy: 0.6034\n",
      "Epoch 38/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5354 - accuracy: 0.7681 - val_loss: 1.2190 - val_accuracy: 0.5962\n",
      "Epoch 39/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5306 - accuracy: 0.7735 - val_loss: 1.2004 - val_accuracy: 0.5928\n",
      "Epoch 40/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7723 - val_loss: 1.1952 - val_accuracy: 0.6017\n",
      "Epoch 41/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5277 - accuracy: 0.7717 - val_loss: 1.1984 - val_accuracy: 0.6042\n",
      "Epoch 42/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5317 - accuracy: 0.7730 - val_loss: 1.2133 - val_accuracy: 0.6085\n",
      "Epoch 43/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7748 - val_loss: 1.2262 - val_accuracy: 0.5987\n",
      "Epoch 44/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.7704 - val_loss: 1.2457 - val_accuracy: 0.5975\n",
      "Epoch 45/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5354 - accuracy: 0.7715 - val_loss: 1.2184 - val_accuracy: 0.5945\n",
      "Epoch 46/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5266 - accuracy: 0.7788 - val_loss: 1.2473 - val_accuracy: 0.5924\n",
      "Epoch 47/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5378 - accuracy: 0.7697 - val_loss: 1.2422 - val_accuracy: 0.5856\n",
      "Epoch 48/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5289 - accuracy: 0.7748 - val_loss: 1.2759 - val_accuracy: 0.5962\n",
      "Epoch 49/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5368 - accuracy: 0.7639 - val_loss: 1.2313 - val_accuracy: 0.5941\n",
      "Epoch 50/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5311 - accuracy: 0.7683 - val_loss: 1.2618 - val_accuracy: 0.5996\n",
      "Epoch 51/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5360 - accuracy: 0.7661 - val_loss: 1.2360 - val_accuracy: 0.5953\n",
      "Epoch 52/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5329 - accuracy: 0.7699 - val_loss: 1.2353 - val_accuracy: 0.5911\n",
      "Epoch 53/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5300 - accuracy: 0.7732 - val_loss: 1.2252 - val_accuracy: 0.5843\n",
      "Epoch 54/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5313 - accuracy: 0.7710 - val_loss: 1.2442 - val_accuracy: 0.5885\n",
      "Epoch 55/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5269 - accuracy: 0.7741 - val_loss: 1.2796 - val_accuracy: 0.5830\n",
      "Epoch 56/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5302 - accuracy: 0.7726 - val_loss: 1.2639 - val_accuracy: 0.5915\n",
      "Epoch 57/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5291 - accuracy: 0.7739 - val_loss: 1.2441 - val_accuracy: 0.5817\n",
      "Epoch 58/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5276 - accuracy: 0.7757 - val_loss: 1.2592 - val_accuracy: 0.5970\n",
      "Epoch 59/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5316 - accuracy: 0.7664 - val_loss: 1.2664 - val_accuracy: 0.5953\n",
      "Epoch 60/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5266 - accuracy: 0.7717 - val_loss: 1.2550 - val_accuracy: 0.5915\n",
      "Epoch 61/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5259 - accuracy: 0.7728 - val_loss: 1.2507 - val_accuracy: 0.5932\n",
      "Epoch 62/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5169 - accuracy: 0.7763 - val_loss: 1.2618 - val_accuracy: 0.5881\n",
      "Epoch 63/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5191 - accuracy: 0.7766 - val_loss: 1.2871 - val_accuracy: 0.5932\n",
      "Epoch 64/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5277 - accuracy: 0.7757 - val_loss: 1.2541 - val_accuracy: 0.5945\n",
      "Epoch 65/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5255 - accuracy: 0.7706 - val_loss: 1.2985 - val_accuracy: 0.5987\n",
      "Epoch 66/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5283 - accuracy: 0.7752 - val_loss: 1.2687 - val_accuracy: 0.6013\n",
      "Epoch 67/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5256 - accuracy: 0.7739 - val_loss: 1.2861 - val_accuracy: 0.5987\n",
      "Epoch 68/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7712 - val_loss: 1.2849 - val_accuracy: 0.5945\n",
      "Epoch 69/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5230 - accuracy: 0.7719 - val_loss: 1.2534 - val_accuracy: 0.5830\n",
      "Epoch 70/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5229 - accuracy: 0.7770 - val_loss: 1.2642 - val_accuracy: 0.5907\n",
      "Epoch 71/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5222 - accuracy: 0.7759 - val_loss: 1.2437 - val_accuracy: 0.5834\n",
      "Epoch 72/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5164 - accuracy: 0.7759 - val_loss: 1.2743 - val_accuracy: 0.6008\n",
      "Epoch 73/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5193 - accuracy: 0.7739 - val_loss: 1.2806 - val_accuracy: 0.5851\n",
      "Epoch 74/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5265 - accuracy: 0.7746 - val_loss: 1.2970 - val_accuracy: 0.5877\n",
      "Epoch 75/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5141 - accuracy: 0.7797 - val_loss: 1.2753 - val_accuracy: 0.5953\n",
      "Epoch 76/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5222 - accuracy: 0.7723 - val_loss: 1.2763 - val_accuracy: 0.5847\n",
      "Epoch 77/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5216 - accuracy: 0.7715 - val_loss: 1.2770 - val_accuracy: 0.5945\n",
      "Epoch 78/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5174 - accuracy: 0.7763 - val_loss: 1.2841 - val_accuracy: 0.5928\n",
      "Epoch 79/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5285 - accuracy: 0.7706 - val_loss: 1.2800 - val_accuracy: 0.5962\n",
      "Epoch 80/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5251 - accuracy: 0.7804 - val_loss: 1.2810 - val_accuracy: 0.5932\n",
      "Epoch 81/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5132 - accuracy: 0.7812 - val_loss: 1.2884 - val_accuracy: 0.5851\n",
      "Epoch 82/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5152 - accuracy: 0.7815 - val_loss: 1.2898 - val_accuracy: 0.5983\n",
      "Epoch 83/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5193 - accuracy: 0.7763 - val_loss: 1.2941 - val_accuracy: 0.5953\n",
      "Epoch 84/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5168 - accuracy: 0.7743 - val_loss: 1.3380 - val_accuracy: 0.5966\n",
      "Epoch 85/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5153 - accuracy: 0.7770 - val_loss: 1.3135 - val_accuracy: 0.5966\n",
      "Epoch 86/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5139 - accuracy: 0.7774 - val_loss: 1.3042 - val_accuracy: 0.5975\n",
      "Epoch 87/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5172 - accuracy: 0.7744 - val_loss: 1.2999 - val_accuracy: 0.5843\n",
      "Epoch 88/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5198 - accuracy: 0.7772 - val_loss: 1.3039 - val_accuracy: 0.5885\n",
      "Epoch 89/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5075 - accuracy: 0.7839 - val_loss: 1.3157 - val_accuracy: 0.5958\n",
      "Epoch 90/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5202 - accuracy: 0.7752 - val_loss: 1.2819 - val_accuracy: 0.5928\n",
      "Epoch 91/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5234 - accuracy: 0.7764 - val_loss: 1.3111 - val_accuracy: 0.5898\n",
      "Epoch 92/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5135 - accuracy: 0.7774 - val_loss: 1.3064 - val_accuracy: 0.5851\n",
      "Epoch 93/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5125 - accuracy: 0.7792 - val_loss: 1.3268 - val_accuracy: 0.5907\n",
      "Epoch 94/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5198 - accuracy: 0.7764 - val_loss: 1.3016 - val_accuracy: 0.5843\n",
      "Epoch 95/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5157 - accuracy: 0.7837 - val_loss: 1.3332 - val_accuracy: 0.6021\n",
      "Epoch 96/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5083 - accuracy: 0.7841 - val_loss: 1.3583 - val_accuracy: 0.5962\n",
      "Epoch 97/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5111 - accuracy: 0.7797 - val_loss: 1.3189 - val_accuracy: 0.5911\n",
      "Epoch 98/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5114 - accuracy: 0.7845 - val_loss: 1.3137 - val_accuracy: 0.5873\n",
      "Epoch 99/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5089 - accuracy: 0.7810 - val_loss: 1.3355 - val_accuracy: 0.5898\n",
      "Epoch 100/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5069 - accuracy: 0.7819 - val_loss: 1.3266 - val_accuracy: 0.5890\n",
      "Epoch 101/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5133 - accuracy: 0.7788 - val_loss: 1.3208 - val_accuracy: 0.5966\n",
      "Epoch 102/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5091 - accuracy: 0.7786 - val_loss: 1.3405 - val_accuracy: 0.5983\n",
      "Epoch 103/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5095 - accuracy: 0.7768 - val_loss: 1.3508 - val_accuracy: 0.5919\n",
      "Epoch 104/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5085 - accuracy: 0.7848 - val_loss: 1.3081 - val_accuracy: 0.5890\n",
      "Epoch 105/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5037 - accuracy: 0.7801 - val_loss: 1.3280 - val_accuracy: 0.5805\n",
      "Epoch 106/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5055 - accuracy: 0.7823 - val_loss: 1.3526 - val_accuracy: 0.5758\n",
      "Epoch 107/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5051 - accuracy: 0.7852 - val_loss: 1.3414 - val_accuracy: 0.5792\n",
      "Epoch 108/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5054 - accuracy: 0.7865 - val_loss: 1.3653 - val_accuracy: 0.5949\n",
      "Epoch 109/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5099 - accuracy: 0.7819 - val_loss: 1.3440 - val_accuracy: 0.5915\n",
      "Epoch 110/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5039 - accuracy: 0.7837 - val_loss: 1.3263 - val_accuracy: 0.5873\n",
      "Epoch 111/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5150 - accuracy: 0.7717 - val_loss: 1.3147 - val_accuracy: 0.5898\n",
      "Epoch 112/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5083 - accuracy: 0.7783 - val_loss: 1.3728 - val_accuracy: 0.5975\n",
      "Epoch 113/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7808 - val_loss: 1.3825 - val_accuracy: 0.5953\n",
      "Epoch 114/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5032 - accuracy: 0.7788 - val_loss: 1.3786 - val_accuracy: 0.5932\n",
      "Epoch 115/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5070 - accuracy: 0.7812 - val_loss: 1.3437 - val_accuracy: 0.5834\n",
      "Epoch 116/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5032 - accuracy: 0.7843 - val_loss: 1.4031 - val_accuracy: 0.5924\n",
      "Epoch 117/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5064 - accuracy: 0.7852 - val_loss: 1.3610 - val_accuracy: 0.5864\n",
      "Epoch 118/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5129 - accuracy: 0.7761 - val_loss: 1.3712 - val_accuracy: 0.5788\n",
      "Epoch 119/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5050 - accuracy: 0.7852 - val_loss: 1.3497 - val_accuracy: 0.5877\n",
      "Epoch 120/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5088 - accuracy: 0.7819 - val_loss: 1.3840 - val_accuracy: 0.5915\n",
      "Epoch 121/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5057 - accuracy: 0.7863 - val_loss: 1.3408 - val_accuracy: 0.5749\n",
      "Epoch 122/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7854 - val_loss: 1.3704 - val_accuracy: 0.5945\n",
      "Epoch 123/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7872 - val_loss: 1.3583 - val_accuracy: 0.5851\n",
      "Epoch 124/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4983 - accuracy: 0.7841 - val_loss: 1.3840 - val_accuracy: 0.6017\n",
      "Epoch 125/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4962 - accuracy: 0.7875 - val_loss: 1.4047 - val_accuracy: 0.5813\n",
      "Epoch 126/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4937 - accuracy: 0.7899 - val_loss: 1.4043 - val_accuracy: 0.5983\n",
      "Epoch 127/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5063 - accuracy: 0.7790 - val_loss: 1.3604 - val_accuracy: 0.5953\n",
      "Epoch 128/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7859 - val_loss: 1.3956 - val_accuracy: 0.5873\n",
      "Epoch 129/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5053 - accuracy: 0.7835 - val_loss: 1.3943 - val_accuracy: 0.5992\n",
      "Epoch 130/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4989 - accuracy: 0.7826 - val_loss: 1.4056 - val_accuracy: 0.5992\n",
      "Epoch 131/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4961 - accuracy: 0.7901 - val_loss: 1.4130 - val_accuracy: 0.5864\n",
      "Epoch 132/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4949 - accuracy: 0.7850 - val_loss: 1.3847 - val_accuracy: 0.5928\n",
      "Epoch 133/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5005 - accuracy: 0.7855 - val_loss: 1.4082 - val_accuracy: 0.5979\n",
      "Epoch 134/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4974 - accuracy: 0.7888 - val_loss: 1.4142 - val_accuracy: 0.5894\n",
      "Epoch 135/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4945 - accuracy: 0.7897 - val_loss: 1.3779 - val_accuracy: 0.5890\n",
      "Epoch 136/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4978 - accuracy: 0.7841 - val_loss: 1.4130 - val_accuracy: 0.5860\n",
      "Epoch 137/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7810 - val_loss: 1.4247 - val_accuracy: 0.5894\n",
      "Epoch 138/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4986 - accuracy: 0.7861 - val_loss: 1.4322 - val_accuracy: 0.5783\n",
      "Epoch 139/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4982 - accuracy: 0.7852 - val_loss: 1.3886 - val_accuracy: 0.5907\n",
      "Epoch 140/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4930 - accuracy: 0.7868 - val_loss: 1.3978 - val_accuracy: 0.5890\n",
      "Epoch 141/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4934 - accuracy: 0.7855 - val_loss: 1.3911 - val_accuracy: 0.5873\n",
      "Epoch 142/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4917 - accuracy: 0.7914 - val_loss: 1.4019 - val_accuracy: 0.5970\n",
      "Epoch 143/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7848 - val_loss: 1.4085 - val_accuracy: 0.5885\n",
      "Epoch 144/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5043 - accuracy: 0.7877 - val_loss: 1.3996 - val_accuracy: 0.5796\n",
      "Epoch 145/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4913 - accuracy: 0.7883 - val_loss: 1.4079 - val_accuracy: 0.5919\n",
      "Epoch 146/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4924 - accuracy: 0.7901 - val_loss: 1.4132 - val_accuracy: 0.5949\n",
      "Epoch 147/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7815 - val_loss: 1.4043 - val_accuracy: 0.5924\n",
      "Epoch 148/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4952 - accuracy: 0.7841 - val_loss: 1.4054 - val_accuracy: 0.5975\n",
      "Epoch 149/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4957 - accuracy: 0.7903 - val_loss: 1.4222 - val_accuracy: 0.5924\n",
      "Epoch 150/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5046 - accuracy: 0.7772 - val_loss: 1.4393 - val_accuracy: 0.5936\n",
      "Epoch 151/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4931 - accuracy: 0.7899 - val_loss: 1.3804 - val_accuracy: 0.5979\n",
      "Epoch 152/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4988 - accuracy: 0.7804 - val_loss: 1.4109 - val_accuracy: 0.5856\n",
      "Epoch 153/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4960 - accuracy: 0.7892 - val_loss: 1.4505 - val_accuracy: 0.5834\n",
      "Epoch 154/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4919 - accuracy: 0.7903 - val_loss: 1.4469 - val_accuracy: 0.5868\n",
      "Epoch 155/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4936 - accuracy: 0.7826 - val_loss: 1.4418 - val_accuracy: 0.5873\n",
      "Epoch 156/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4985 - accuracy: 0.7865 - val_loss: 1.4298 - val_accuracy: 0.5885\n",
      "Epoch 157/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4889 - accuracy: 0.7932 - val_loss: 1.4366 - val_accuracy: 0.5877\n",
      "Epoch 158/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4880 - accuracy: 0.7914 - val_loss: 1.4762 - val_accuracy: 0.5868\n",
      "Epoch 159/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4925 - accuracy: 0.7854 - val_loss: 1.4371 - val_accuracy: 0.5885\n",
      "Epoch 160/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4862 - accuracy: 0.7937 - val_loss: 1.4180 - val_accuracy: 0.5839\n",
      "Epoch 161/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4825 - accuracy: 0.7921 - val_loss: 1.4712 - val_accuracy: 0.5856\n",
      "Epoch 162/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4883 - accuracy: 0.7921 - val_loss: 1.4330 - val_accuracy: 0.5894\n",
      "Epoch 163/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4868 - accuracy: 0.7886 - val_loss: 1.4275 - val_accuracy: 0.5962\n",
      "Epoch 164/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4845 - accuracy: 0.7908 - val_loss: 1.4401 - val_accuracy: 0.5843\n",
      "Epoch 165/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4898 - accuracy: 0.7917 - val_loss: 1.4471 - val_accuracy: 0.5915\n",
      "Epoch 166/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4788 - accuracy: 0.7963 - val_loss: 1.4481 - val_accuracy: 0.5868\n",
      "Epoch 167/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4863 - accuracy: 0.7868 - val_loss: 1.4775 - val_accuracy: 0.5915\n",
      "Epoch 168/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4900 - accuracy: 0.7890 - val_loss: 1.4393 - val_accuracy: 0.5864\n",
      "Epoch 169/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4882 - accuracy: 0.7914 - val_loss: 1.4512 - val_accuracy: 0.5902\n",
      "Epoch 170/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4906 - accuracy: 0.7919 - val_loss: 1.4651 - val_accuracy: 0.5928\n",
      "Epoch 171/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4921 - accuracy: 0.7894 - val_loss: 1.4142 - val_accuracy: 0.5966\n",
      "Epoch 172/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4917 - accuracy: 0.7885 - val_loss: 1.4608 - val_accuracy: 0.5970\n",
      "Epoch 173/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4905 - accuracy: 0.7841 - val_loss: 1.5091 - val_accuracy: 0.5945\n",
      "Epoch 174/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4882 - accuracy: 0.7905 - val_loss: 1.4691 - val_accuracy: 0.5851\n",
      "Epoch 175/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4860 - accuracy: 0.7906 - val_loss: 1.4900 - val_accuracy: 0.5847\n",
      "Epoch 176/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4936 - accuracy: 0.7894 - val_loss: 1.4689 - val_accuracy: 0.5834\n",
      "Epoch 177/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4887 - accuracy: 0.7885 - val_loss: 1.4342 - val_accuracy: 0.5949\n",
      "Epoch 178/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4846 - accuracy: 0.7930 - val_loss: 1.5243 - val_accuracy: 0.5915\n",
      "Epoch 179/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4850 - accuracy: 0.7930 - val_loss: 1.4810 - val_accuracy: 0.5915\n",
      "Epoch 180/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4907 - accuracy: 0.7875 - val_loss: 1.4505 - val_accuracy: 0.5898\n",
      "Epoch 181/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4893 - accuracy: 0.7870 - val_loss: 1.4962 - val_accuracy: 0.5877\n",
      "Epoch 182/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4823 - accuracy: 0.7906 - val_loss: 1.4822 - val_accuracy: 0.5919\n",
      "Epoch 183/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4775 - accuracy: 0.7959 - val_loss: 1.4730 - val_accuracy: 0.5792\n",
      "Epoch 184/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4891 - accuracy: 0.7881 - val_loss: 1.4557 - val_accuracy: 0.5847\n",
      "Epoch 185/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4808 - accuracy: 0.7943 - val_loss: 1.4766 - val_accuracy: 0.5830\n",
      "Epoch 186/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4883 - accuracy: 0.7930 - val_loss: 1.5013 - val_accuracy: 0.5771\n",
      "Epoch 187/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4748 - accuracy: 0.8008 - val_loss: 1.5071 - val_accuracy: 0.5860\n",
      "Epoch 188/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4840 - accuracy: 0.7868 - val_loss: 1.5293 - val_accuracy: 0.5919\n",
      "Epoch 189/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4817 - accuracy: 0.7934 - val_loss: 1.4781 - val_accuracy: 0.5749\n",
      "Epoch 190/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4748 - accuracy: 0.7961 - val_loss: 1.4520 - val_accuracy: 0.5864\n",
      "Epoch 191/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4807 - accuracy: 0.7956 - val_loss: 1.5105 - val_accuracy: 0.5881\n",
      "Epoch 192/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4777 - accuracy: 0.7999 - val_loss: 1.5212 - val_accuracy: 0.5911\n",
      "Epoch 193/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4785 - accuracy: 0.7946 - val_loss: 1.5180 - val_accuracy: 0.5779\n",
      "Epoch 194/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4821 - accuracy: 0.7952 - val_loss: 1.5106 - val_accuracy: 0.5851\n",
      "Epoch 195/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4880 - accuracy: 0.7925 - val_loss: 1.4886 - val_accuracy: 0.5792\n",
      "Epoch 196/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4779 - accuracy: 0.7963 - val_loss: 1.4978 - val_accuracy: 0.5885\n",
      "Epoch 197/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4723 - accuracy: 0.7983 - val_loss: 1.5092 - val_accuracy: 0.5856\n",
      "Epoch 198/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4813 - accuracy: 0.7934 - val_loss: 1.4961 - val_accuracy: 0.5694\n",
      "Epoch 199/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4867 - accuracy: 0.7897 - val_loss: 1.4855 - val_accuracy: 0.5524\n",
      "Epoch 200/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4846 - accuracy: 0.7910 - val_loss: 1.5170 - val_accuracy: 0.5788\n",
      "Epoch 201/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4728 - accuracy: 0.8045 - val_loss: 1.5045 - val_accuracy: 0.5911\n",
      "Epoch 202/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4757 - accuracy: 0.7954 - val_loss: 1.5297 - val_accuracy: 0.5822\n",
      "Epoch 203/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4731 - accuracy: 0.8025 - val_loss: 1.5003 - val_accuracy: 0.5911\n",
      "Epoch 204/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4815 - accuracy: 0.7928 - val_loss: 1.5316 - val_accuracy: 0.5648\n",
      "Epoch 205/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4764 - accuracy: 0.7992 - val_loss: 1.5382 - val_accuracy: 0.5830\n",
      "Epoch 206/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4772 - accuracy: 0.7934 - val_loss: 1.5530 - val_accuracy: 0.5890\n",
      "Epoch 207/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4784 - accuracy: 0.7945 - val_loss: 1.5365 - val_accuracy: 0.5788\n",
      "Epoch 208/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4852 - accuracy: 0.7885 - val_loss: 1.5021 - val_accuracy: 0.5732\n",
      "Epoch 209/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4841 - accuracy: 0.7939 - val_loss: 1.4943 - val_accuracy: 0.5830\n",
      "Epoch 210/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4783 - accuracy: 0.7934 - val_loss: 1.5692 - val_accuracy: 0.5924\n",
      "Epoch 211/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4675 - accuracy: 0.8034 - val_loss: 1.5111 - val_accuracy: 0.5775\n",
      "Epoch 212/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4695 - accuracy: 0.8001 - val_loss: 1.5825 - val_accuracy: 0.5826\n",
      "Epoch 213/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4755 - accuracy: 0.7934 - val_loss: 1.5610 - val_accuracy: 0.5728\n",
      "Epoch 214/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4707 - accuracy: 0.7985 - val_loss: 1.5440 - val_accuracy: 0.5830\n",
      "Epoch 215/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4689 - accuracy: 0.7952 - val_loss: 1.5469 - val_accuracy: 0.5864\n",
      "Epoch 216/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4663 - accuracy: 0.8019 - val_loss: 1.5535 - val_accuracy: 0.5915\n",
      "Epoch 217/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4742 - accuracy: 0.7930 - val_loss: 1.5508 - val_accuracy: 0.5911\n",
      "Epoch 218/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4673 - accuracy: 0.8032 - val_loss: 1.5518 - val_accuracy: 0.5732\n",
      "Epoch 219/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4753 - accuracy: 0.7977 - val_loss: 1.5866 - val_accuracy: 0.5902\n",
      "Epoch 220/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4717 - accuracy: 0.8007 - val_loss: 1.5711 - val_accuracy: 0.5843\n",
      "Epoch 221/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4770 - accuracy: 0.7934 - val_loss: 1.5838 - val_accuracy: 0.5856\n",
      "Epoch 222/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4667 - accuracy: 0.7994 - val_loss: 1.5530 - val_accuracy: 0.5843\n",
      "Epoch 223/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4737 - accuracy: 0.8010 - val_loss: 1.5529 - val_accuracy: 0.5796\n",
      "Epoch 224/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4772 - accuracy: 0.7919 - val_loss: 1.5644 - val_accuracy: 0.5898\n",
      "Epoch 225/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4637 - accuracy: 0.8036 - val_loss: 1.5712 - val_accuracy: 0.5745\n",
      "Epoch 226/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4705 - accuracy: 0.7943 - val_loss: 1.5616 - val_accuracy: 0.5851\n",
      "Epoch 227/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4769 - accuracy: 0.7946 - val_loss: 1.5816 - val_accuracy: 0.5826\n",
      "Epoch 228/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4750 - accuracy: 0.7959 - val_loss: 1.5906 - val_accuracy: 0.5656\n",
      "Epoch 229/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4717 - accuracy: 0.7977 - val_loss: 1.5457 - val_accuracy: 0.5834\n",
      "Epoch 230/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4696 - accuracy: 0.7950 - val_loss: 1.5414 - val_accuracy: 0.5673\n",
      "Epoch 231/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4641 - accuracy: 0.8039 - val_loss: 1.5741 - val_accuracy: 0.5800\n",
      "Epoch 232/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4631 - accuracy: 0.8014 - val_loss: 1.5805 - val_accuracy: 0.5792\n",
      "Epoch 233/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4661 - accuracy: 0.8016 - val_loss: 1.5834 - val_accuracy: 0.5711\n",
      "Epoch 234/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4705 - accuracy: 0.7941 - val_loss: 1.5692 - val_accuracy: 0.5783\n",
      "Epoch 235/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4711 - accuracy: 0.7968 - val_loss: 1.5677 - val_accuracy: 0.5877\n",
      "Epoch 236/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4661 - accuracy: 0.8019 - val_loss: 1.5746 - val_accuracy: 0.5958\n",
      "Epoch 237/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4755 - accuracy: 0.7957 - val_loss: 1.5550 - val_accuracy: 0.5843\n",
      "Epoch 238/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4710 - accuracy: 0.8007 - val_loss: 1.5756 - val_accuracy: 0.5707\n",
      "Epoch 239/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4601 - accuracy: 0.7985 - val_loss: 1.6312 - val_accuracy: 0.5843\n",
      "Epoch 240/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4741 - accuracy: 0.7979 - val_loss: 1.6195 - val_accuracy: 0.5856\n",
      "Epoch 241/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4641 - accuracy: 0.8039 - val_loss: 1.5814 - val_accuracy: 0.5805\n",
      "Epoch 242/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4571 - accuracy: 0.8081 - val_loss: 1.6012 - val_accuracy: 0.5703\n",
      "Epoch 243/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4748 - accuracy: 0.7968 - val_loss: 1.5909 - val_accuracy: 0.5860\n",
      "Epoch 244/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4677 - accuracy: 0.8012 - val_loss: 1.5994 - val_accuracy: 0.5749\n",
      "Epoch 245/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4604 - accuracy: 0.8032 - val_loss: 1.5908 - val_accuracy: 0.5745\n",
      "Epoch 246/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4669 - accuracy: 0.7977 - val_loss: 1.5872 - val_accuracy: 0.5834\n",
      "Epoch 247/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4713 - accuracy: 0.8001 - val_loss: 1.6113 - val_accuracy: 0.5902\n",
      "Epoch 248/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4651 - accuracy: 0.7983 - val_loss: 1.5775 - val_accuracy: 0.5745\n",
      "Epoch 249/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4600 - accuracy: 0.8027 - val_loss: 1.6034 - val_accuracy: 0.5826\n",
      "Epoch 250/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4633 - accuracy: 0.8032 - val_loss: 1.6115 - val_accuracy: 0.5813\n",
      "Epoch 251/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4628 - accuracy: 0.8005 - val_loss: 1.6141 - val_accuracy: 0.5745\n",
      "Epoch 252/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4612 - accuracy: 0.8039 - val_loss: 1.6001 - val_accuracy: 0.5839\n",
      "Epoch 253/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4624 - accuracy: 0.8012 - val_loss: 1.6089 - val_accuracy: 0.5792\n",
      "Epoch 254/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4619 - accuracy: 0.8001 - val_loss: 1.6258 - val_accuracy: 0.5720\n",
      "Epoch 255/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4747 - accuracy: 0.7930 - val_loss: 1.6067 - val_accuracy: 0.5563\n",
      "Epoch 256/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4649 - accuracy: 0.7976 - val_loss: 1.6168 - val_accuracy: 0.5754\n",
      "Epoch 257/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4579 - accuracy: 0.8096 - val_loss: 1.5808 - val_accuracy: 0.5830\n",
      "Epoch 258/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4565 - accuracy: 0.8047 - val_loss: 1.6237 - val_accuracy: 0.5822\n",
      "Epoch 259/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4620 - accuracy: 0.8050 - val_loss: 1.6339 - val_accuracy: 0.5894\n",
      "Epoch 260/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4596 - accuracy: 0.8038 - val_loss: 1.6254 - val_accuracy: 0.5775\n",
      "Epoch 261/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4548 - accuracy: 0.8085 - val_loss: 1.6307 - val_accuracy: 0.5754\n",
      "Epoch 262/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4584 - accuracy: 0.8038 - val_loss: 1.5972 - val_accuracy: 0.5775\n",
      "Epoch 263/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4686 - accuracy: 0.8001 - val_loss: 1.6159 - val_accuracy: 0.5860\n",
      "Epoch 264/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4570 - accuracy: 0.8065 - val_loss: 1.6595 - val_accuracy: 0.5699\n",
      "Epoch 265/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4630 - accuracy: 0.8034 - val_loss: 1.6593 - val_accuracy: 0.5873\n",
      "Epoch 266/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4578 - accuracy: 0.8059 - val_loss: 1.6293 - val_accuracy: 0.5639\n",
      "Epoch 267/500\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.4600 - accuracy: 0.8028 - val_loss: 1.6299 - val_accuracy: 0.5877\n",
      "Epoch 268/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4567 - accuracy: 0.8045 - val_loss: 1.6360 - val_accuracy: 0.5809\n",
      "Epoch 269/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4649 - accuracy: 0.8021 - val_loss: 1.6112 - val_accuracy: 0.5737\n",
      "Epoch 270/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4565 - accuracy: 0.8032 - val_loss: 1.6225 - val_accuracy: 0.5745\n",
      "Epoch 271/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4608 - accuracy: 0.8010 - val_loss: 1.6493 - val_accuracy: 0.5762\n",
      "Epoch 272/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4601 - accuracy: 0.7997 - val_loss: 1.6554 - val_accuracy: 0.5911\n",
      "Epoch 273/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4580 - accuracy: 0.8056 - val_loss: 1.6757 - val_accuracy: 0.5766\n",
      "Epoch 274/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4612 - accuracy: 0.8005 - val_loss: 1.6561 - val_accuracy: 0.5690\n",
      "Epoch 275/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4520 - accuracy: 0.8079 - val_loss: 1.6654 - val_accuracy: 0.5813\n",
      "Epoch 276/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4578 - accuracy: 0.8058 - val_loss: 1.6458 - val_accuracy: 0.5826\n",
      "Epoch 277/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4530 - accuracy: 0.8094 - val_loss: 1.6707 - val_accuracy: 0.5783\n",
      "Epoch 278/500\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.4659 - accuracy: 0.7996 - val_loss: 1.6822 - val_accuracy: 0.5783\n",
      "Epoch 279/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4578 - accuracy: 0.8092 - val_loss: 1.6939 - val_accuracy: 0.5728\n",
      "Epoch 280/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4601 - accuracy: 0.8001 - val_loss: 1.6610 - val_accuracy: 0.5682\n",
      "Epoch 281/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4515 - accuracy: 0.8045 - val_loss: 1.6856 - val_accuracy: 0.5550\n",
      "Epoch 282/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4547 - accuracy: 0.8012 - val_loss: 1.6849 - val_accuracy: 0.5817\n",
      "Epoch 283/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4563 - accuracy: 0.8023 - val_loss: 1.6932 - val_accuracy: 0.5847\n",
      "Epoch 284/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4525 - accuracy: 0.8067 - val_loss: 1.6732 - val_accuracy: 0.5677\n",
      "Epoch 285/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4632 - accuracy: 0.8008 - val_loss: 1.6711 - val_accuracy: 0.5618\n",
      "Epoch 286/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4543 - accuracy: 0.8105 - val_loss: 1.6796 - val_accuracy: 0.5762\n",
      "Epoch 287/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4556 - accuracy: 0.8030 - val_loss: 1.6829 - val_accuracy: 0.5758\n",
      "Epoch 288/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4607 - accuracy: 0.7994 - val_loss: 1.6880 - val_accuracy: 0.5558\n",
      "Epoch 289/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4562 - accuracy: 0.8038 - val_loss: 1.6699 - val_accuracy: 0.5822\n",
      "Epoch 290/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4536 - accuracy: 0.8041 - val_loss: 1.6891 - val_accuracy: 0.5839\n",
      "Epoch 291/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4510 - accuracy: 0.8085 - val_loss: 1.6770 - val_accuracy: 0.5796\n",
      "Epoch 292/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4474 - accuracy: 0.8099 - val_loss: 1.6602 - val_accuracy: 0.5813\n",
      "Epoch 293/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4468 - accuracy: 0.8114 - val_loss: 1.6696 - val_accuracy: 0.5839\n",
      "Epoch 294/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4612 - accuracy: 0.8012 - val_loss: 1.7137 - val_accuracy: 0.5554\n",
      "Epoch 295/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4645 - accuracy: 0.8010 - val_loss: 1.6846 - val_accuracy: 0.5635\n",
      "Epoch 296/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4539 - accuracy: 0.8032 - val_loss: 1.6877 - val_accuracy: 0.5783\n",
      "Epoch 297/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4525 - accuracy: 0.8067 - val_loss: 1.6765 - val_accuracy: 0.5754\n",
      "Epoch 298/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4520 - accuracy: 0.8059 - val_loss: 1.6755 - val_accuracy: 0.5732\n",
      "Epoch 299/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4545 - accuracy: 0.8045 - val_loss: 1.6818 - val_accuracy: 0.5902\n",
      "Epoch 300/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4495 - accuracy: 0.8038 - val_loss: 1.7138 - val_accuracy: 0.5660\n",
      "Epoch 301/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4523 - accuracy: 0.8088 - val_loss: 1.7364 - val_accuracy: 0.5881\n",
      "Epoch 302/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4472 - accuracy: 0.8101 - val_loss: 1.7140 - val_accuracy: 0.5779\n",
      "Epoch 303/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4562 - accuracy: 0.8008 - val_loss: 1.7644 - val_accuracy: 0.5860\n",
      "Epoch 304/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4473 - accuracy: 0.8101 - val_loss: 1.7023 - val_accuracy: 0.5847\n",
      "Epoch 305/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4530 - accuracy: 0.8067 - val_loss: 1.6936 - val_accuracy: 0.5779\n",
      "Epoch 306/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4430 - accuracy: 0.8129 - val_loss: 1.7405 - val_accuracy: 0.5741\n",
      "Epoch 307/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4491 - accuracy: 0.8023 - val_loss: 1.6952 - val_accuracy: 0.5792\n",
      "Epoch 308/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4528 - accuracy: 0.8030 - val_loss: 1.7251 - val_accuracy: 0.5741\n",
      "Epoch 309/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4468 - accuracy: 0.8099 - val_loss: 1.7189 - val_accuracy: 0.5783\n",
      "Epoch 310/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4430 - accuracy: 0.8138 - val_loss: 1.7106 - val_accuracy: 0.5571\n",
      "Epoch 311/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4617 - accuracy: 0.8023 - val_loss: 1.6952 - val_accuracy: 0.5724\n",
      "Epoch 312/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4517 - accuracy: 0.8070 - val_loss: 1.7192 - val_accuracy: 0.5711\n",
      "Epoch 313/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4400 - accuracy: 0.8079 - val_loss: 1.7505 - val_accuracy: 0.5758\n",
      "Epoch 314/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4484 - accuracy: 0.8118 - val_loss: 1.7152 - val_accuracy: 0.5779\n",
      "Epoch 315/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4479 - accuracy: 0.8061 - val_loss: 1.7147 - val_accuracy: 0.5737\n",
      "Epoch 316/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4615 - accuracy: 0.8043 - val_loss: 1.7172 - val_accuracy: 0.5822\n",
      "Epoch 317/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4454 - accuracy: 0.8092 - val_loss: 1.7546 - val_accuracy: 0.5762\n",
      "Epoch 318/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4442 - accuracy: 0.8121 - val_loss: 1.7039 - val_accuracy: 0.5677\n",
      "Epoch 319/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4498 - accuracy: 0.8072 - val_loss: 1.6895 - val_accuracy: 0.5660\n",
      "Epoch 320/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4516 - accuracy: 0.8088 - val_loss: 1.7432 - val_accuracy: 0.5822\n",
      "Epoch 321/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4456 - accuracy: 0.8138 - val_loss: 1.7307 - val_accuracy: 0.5652\n",
      "Epoch 322/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4479 - accuracy: 0.8114 - val_loss: 1.7666 - val_accuracy: 0.5809\n",
      "Epoch 323/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4439 - accuracy: 0.8094 - val_loss: 1.7514 - val_accuracy: 0.5690\n",
      "Epoch 324/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4498 - accuracy: 0.8096 - val_loss: 1.7829 - val_accuracy: 0.5749\n",
      "Epoch 325/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4546 - accuracy: 0.8052 - val_loss: 1.7632 - val_accuracy: 0.5822\n",
      "Epoch 326/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4441 - accuracy: 0.8129 - val_loss: 1.7552 - val_accuracy: 0.5864\n",
      "Epoch 327/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4475 - accuracy: 0.8074 - val_loss: 1.7813 - val_accuracy: 0.5605\n",
      "Epoch 328/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4487 - accuracy: 0.8081 - val_loss: 1.7149 - val_accuracy: 0.5771\n",
      "Epoch 329/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4437 - accuracy: 0.8129 - val_loss: 1.7562 - val_accuracy: 0.5639\n",
      "Epoch 330/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4414 - accuracy: 0.8109 - val_loss: 1.7656 - val_accuracy: 0.5796\n",
      "Epoch 331/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4482 - accuracy: 0.8036 - val_loss: 1.7896 - val_accuracy: 0.5639\n",
      "Epoch 332/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4492 - accuracy: 0.8070 - val_loss: 1.7660 - val_accuracy: 0.5826\n",
      "Epoch 333/500\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4422 - accuracy: 0.8109 - val_loss: 1.7567 - val_accuracy: 0.5715\n",
      "Epoch 334/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4391 - accuracy: 0.8176 - val_loss: 1.7883 - val_accuracy: 0.5868\n",
      "Epoch 335/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4422 - accuracy: 0.8109 - val_loss: 1.7498 - val_accuracy: 0.5877\n",
      "Epoch 336/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4488 - accuracy: 0.8132 - val_loss: 1.7860 - val_accuracy: 0.5694\n",
      "Epoch 337/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4499 - accuracy: 0.8052 - val_loss: 1.8001 - val_accuracy: 0.5766\n",
      "Epoch 338/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4402 - accuracy: 0.8114 - val_loss: 1.8302 - val_accuracy: 0.5762\n",
      "Epoch 339/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4445 - accuracy: 0.8099 - val_loss: 1.7751 - val_accuracy: 0.5673\n",
      "Epoch 340/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4320 - accuracy: 0.8154 - val_loss: 1.8083 - val_accuracy: 0.5495\n",
      "Epoch 341/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4388 - accuracy: 0.8167 - val_loss: 1.7601 - val_accuracy: 0.5694\n",
      "Epoch 342/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4392 - accuracy: 0.8101 - val_loss: 1.7700 - val_accuracy: 0.5715\n",
      "Epoch 343/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4351 - accuracy: 0.8167 - val_loss: 1.7959 - val_accuracy: 0.5673\n",
      "Epoch 344/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4445 - accuracy: 0.8092 - val_loss: 1.8305 - val_accuracy: 0.5749\n",
      "Epoch 345/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4417 - accuracy: 0.8116 - val_loss: 1.8509 - val_accuracy: 0.5775\n",
      "Epoch 346/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4409 - accuracy: 0.8125 - val_loss: 1.8184 - val_accuracy: 0.5805\n",
      "Epoch 347/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4335 - accuracy: 0.8165 - val_loss: 1.8017 - val_accuracy: 0.5805\n",
      "Epoch 348/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4464 - accuracy: 0.8094 - val_loss: 1.7951 - val_accuracy: 0.5813\n",
      "Epoch 349/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4411 - accuracy: 0.8139 - val_loss: 1.8316 - val_accuracy: 0.5503\n",
      "Epoch 350/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4582 - accuracy: 0.8019 - val_loss: 1.8124 - val_accuracy: 0.5762\n",
      "Epoch 351/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4374 - accuracy: 0.8149 - val_loss: 1.7982 - val_accuracy: 0.5732\n",
      "Epoch 352/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4349 - accuracy: 0.8154 - val_loss: 1.8191 - val_accuracy: 0.5898\n",
      "Epoch 353/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4350 - accuracy: 0.8169 - val_loss: 1.7870 - val_accuracy: 0.5771\n",
      "Epoch 354/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4408 - accuracy: 0.8138 - val_loss: 1.8041 - val_accuracy: 0.5699\n",
      "Epoch 355/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4421 - accuracy: 0.8076 - val_loss: 1.7775 - val_accuracy: 0.5745\n",
      "Epoch 356/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4457 - accuracy: 0.8068 - val_loss: 1.8290 - val_accuracy: 0.5677\n",
      "Epoch 357/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4436 - accuracy: 0.8096 - val_loss: 1.7942 - val_accuracy: 0.5724\n",
      "Epoch 358/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4383 - accuracy: 0.8130 - val_loss: 1.8007 - val_accuracy: 0.5584\n",
      "Epoch 359/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4341 - accuracy: 0.8154 - val_loss: 1.8069 - val_accuracy: 0.5754\n",
      "Epoch 360/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4385 - accuracy: 0.8130 - val_loss: 1.8245 - val_accuracy: 0.5673\n",
      "Epoch 361/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4398 - accuracy: 0.8085 - val_loss: 1.8142 - val_accuracy: 0.5724\n",
      "Epoch 362/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4405 - accuracy: 0.8092 - val_loss: 1.7777 - val_accuracy: 0.5732\n",
      "Epoch 363/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.8170 - val_loss: 1.8573 - val_accuracy: 0.5575\n",
      "Epoch 364/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4475 - accuracy: 0.8098 - val_loss: 1.8340 - val_accuracy: 0.5715\n",
      "Epoch 365/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4411 - accuracy: 0.8139 - val_loss: 1.8530 - val_accuracy: 0.5703\n",
      "Epoch 366/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4347 - accuracy: 0.8189 - val_loss: 1.8696 - val_accuracy: 0.5762\n",
      "Epoch 367/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4367 - accuracy: 0.8107 - val_loss: 1.8660 - val_accuracy: 0.5588\n",
      "Epoch 368/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4396 - accuracy: 0.8127 - val_loss: 1.8282 - val_accuracy: 0.5771\n",
      "Epoch 369/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4425 - accuracy: 0.8129 - val_loss: 1.8433 - val_accuracy: 0.5792\n",
      "Epoch 370/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.8185 - val_loss: 1.7915 - val_accuracy: 0.5677\n",
      "Epoch 371/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4277 - accuracy: 0.8212 - val_loss: 1.8210 - val_accuracy: 0.5839\n",
      "Epoch 372/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4334 - accuracy: 0.8170 - val_loss: 1.8824 - val_accuracy: 0.5779\n",
      "Epoch 373/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4385 - accuracy: 0.8132 - val_loss: 1.8561 - val_accuracy: 0.5775\n",
      "Epoch 374/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4336 - accuracy: 0.8170 - val_loss: 1.8342 - val_accuracy: 0.5673\n",
      "Epoch 375/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4398 - accuracy: 0.8136 - val_loss: 1.7964 - val_accuracy: 0.5707\n",
      "Epoch 376/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4347 - accuracy: 0.8201 - val_loss: 1.8643 - val_accuracy: 0.5741\n",
      "Epoch 377/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4292 - accuracy: 0.8149 - val_loss: 1.8258 - val_accuracy: 0.5800\n",
      "Epoch 378/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4326 - accuracy: 0.8150 - val_loss: 1.7975 - val_accuracy: 0.5690\n",
      "Epoch 379/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4289 - accuracy: 0.8194 - val_loss: 1.8516 - val_accuracy: 0.5686\n",
      "Epoch 380/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4359 - accuracy: 0.8145 - val_loss: 1.8146 - val_accuracy: 0.5728\n",
      "Epoch 381/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4339 - accuracy: 0.8158 - val_loss: 1.8508 - val_accuracy: 0.5754\n",
      "Epoch 382/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4311 - accuracy: 0.8145 - val_loss: 1.8597 - val_accuracy: 0.5741\n",
      "Epoch 383/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4389 - accuracy: 0.8129 - val_loss: 1.8686 - val_accuracy: 0.5754\n",
      "Epoch 384/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4356 - accuracy: 0.8116 - val_loss: 1.8242 - val_accuracy: 0.5809\n",
      "Epoch 385/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4272 - accuracy: 0.8220 - val_loss: 1.8618 - val_accuracy: 0.5512\n",
      "Epoch 386/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4263 - accuracy: 0.8214 - val_loss: 1.8704 - val_accuracy: 0.5775\n",
      "Epoch 387/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4307 - accuracy: 0.8156 - val_loss: 1.8621 - val_accuracy: 0.5813\n",
      "Epoch 388/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4325 - accuracy: 0.8190 - val_loss: 1.8375 - val_accuracy: 0.5584\n",
      "Epoch 389/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4338 - accuracy: 0.8159 - val_loss: 1.8622 - val_accuracy: 0.5720\n",
      "Epoch 390/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4251 - accuracy: 0.8185 - val_loss: 1.8704 - val_accuracy: 0.5715\n",
      "Epoch 391/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4300 - accuracy: 0.8205 - val_loss: 1.8657 - val_accuracy: 0.5592\n",
      "Epoch 392/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4396 - accuracy: 0.8090 - val_loss: 1.9170 - val_accuracy: 0.5788\n",
      "Epoch 393/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4220 - accuracy: 0.8190 - val_loss: 1.8661 - val_accuracy: 0.5699\n",
      "Epoch 394/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4395 - accuracy: 0.8116 - val_loss: 1.9041 - val_accuracy: 0.5732\n",
      "Epoch 395/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4371 - accuracy: 0.8145 - val_loss: 1.8812 - val_accuracy: 0.5665\n",
      "Epoch 396/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4315 - accuracy: 0.8159 - val_loss: 1.8541 - val_accuracy: 0.5690\n",
      "Epoch 397/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4326 - accuracy: 0.8165 - val_loss: 1.8765 - val_accuracy: 0.5546\n",
      "Epoch 398/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4448 - accuracy: 0.8092 - val_loss: 1.8669 - val_accuracy: 0.5737\n",
      "Epoch 399/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4248 - accuracy: 0.8170 - val_loss: 1.8554 - val_accuracy: 0.5822\n",
      "Epoch 400/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4305 - accuracy: 0.8229 - val_loss: 1.8786 - val_accuracy: 0.5686\n",
      "Epoch 401/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4213 - accuracy: 0.8192 - val_loss: 1.8932 - val_accuracy: 0.5677\n",
      "Epoch 402/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4290 - accuracy: 0.8163 - val_loss: 1.8606 - val_accuracy: 0.5847\n",
      "Epoch 403/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4347 - accuracy: 0.8127 - val_loss: 1.8884 - val_accuracy: 0.5643\n",
      "Epoch 404/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4221 - accuracy: 0.8209 - val_loss: 1.8757 - val_accuracy: 0.5715\n",
      "Epoch 405/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4283 - accuracy: 0.8203 - val_loss: 1.8665 - val_accuracy: 0.5686\n",
      "Epoch 406/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4343 - accuracy: 0.8150 - val_loss: 1.8875 - val_accuracy: 0.5686\n",
      "Epoch 407/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4258 - accuracy: 0.8210 - val_loss: 1.8837 - val_accuracy: 0.5766\n",
      "Epoch 408/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4235 - accuracy: 0.8216 - val_loss: 1.8915 - val_accuracy: 0.5554\n",
      "Epoch 409/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4165 - accuracy: 0.8256 - val_loss: 1.8942 - val_accuracy: 0.5588\n",
      "Epoch 410/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4226 - accuracy: 0.8218 - val_loss: 1.8660 - val_accuracy: 0.5694\n",
      "Epoch 411/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4192 - accuracy: 0.8265 - val_loss: 1.9164 - val_accuracy: 0.5830\n",
      "Epoch 412/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4302 - accuracy: 0.8181 - val_loss: 1.9185 - val_accuracy: 0.5745\n",
      "Epoch 413/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4212 - accuracy: 0.8223 - val_loss: 1.8924 - val_accuracy: 0.5643\n",
      "Epoch 414/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4239 - accuracy: 0.8214 - val_loss: 1.9305 - val_accuracy: 0.5363\n",
      "Epoch 415/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4379 - accuracy: 0.8156 - val_loss: 1.9538 - val_accuracy: 0.5584\n",
      "Epoch 416/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4278 - accuracy: 0.8201 - val_loss: 1.8990 - val_accuracy: 0.5707\n",
      "Epoch 417/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4199 - accuracy: 0.8220 - val_loss: 1.8922 - val_accuracy: 0.5643\n",
      "Epoch 418/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4248 - accuracy: 0.8201 - val_loss: 1.8871 - val_accuracy: 0.5724\n",
      "Epoch 419/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4247 - accuracy: 0.8221 - val_loss: 1.9204 - val_accuracy: 0.5699\n",
      "Epoch 420/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4321 - accuracy: 0.8214 - val_loss: 1.9059 - val_accuracy: 0.5567\n",
      "Epoch 421/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4332 - accuracy: 0.8176 - val_loss: 1.8910 - val_accuracy: 0.5686\n",
      "Epoch 422/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4274 - accuracy: 0.8167 - val_loss: 1.8912 - val_accuracy: 0.5779\n",
      "Epoch 423/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4362 - accuracy: 0.8185 - val_loss: 1.9336 - val_accuracy: 0.5711\n",
      "Epoch 424/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4322 - accuracy: 0.8225 - val_loss: 1.9149 - val_accuracy: 0.5618\n",
      "Epoch 425/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4296 - accuracy: 0.8192 - val_loss: 1.9030 - val_accuracy: 0.5660\n",
      "Epoch 426/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4249 - accuracy: 0.8183 - val_loss: 1.8799 - val_accuracy: 0.5673\n",
      "Epoch 427/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4151 - accuracy: 0.8260 - val_loss: 1.9513 - val_accuracy: 0.5486\n",
      "Epoch 428/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4223 - accuracy: 0.8249 - val_loss: 1.9202 - val_accuracy: 0.5648\n",
      "Epoch 429/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4219 - accuracy: 0.8223 - val_loss: 1.9154 - val_accuracy: 0.5830\n",
      "Epoch 430/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4231 - accuracy: 0.8241 - val_loss: 1.9311 - val_accuracy: 0.5783\n",
      "Epoch 431/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4180 - accuracy: 0.8198 - val_loss: 1.9260 - val_accuracy: 0.5673\n",
      "Epoch 432/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4305 - accuracy: 0.8190 - val_loss: 1.9180 - val_accuracy: 0.5741\n",
      "Epoch 433/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4217 - accuracy: 0.8229 - val_loss: 1.9288 - val_accuracy: 0.5720\n",
      "Epoch 434/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4273 - accuracy: 0.8174 - val_loss: 1.9062 - val_accuracy: 0.5660\n",
      "Epoch 435/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4180 - accuracy: 0.8229 - val_loss: 1.9643 - val_accuracy: 0.5737\n",
      "Epoch 436/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4321 - accuracy: 0.8150 - val_loss: 1.9448 - val_accuracy: 0.5626\n",
      "Epoch 437/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4142 - accuracy: 0.8232 - val_loss: 1.9309 - val_accuracy: 0.5694\n",
      "Epoch 438/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4274 - accuracy: 0.8200 - val_loss: 1.9393 - val_accuracy: 0.5839\n",
      "Epoch 439/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4283 - accuracy: 0.8169 - val_loss: 1.9548 - val_accuracy: 0.5554\n",
      "Epoch 440/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4252 - accuracy: 0.8247 - val_loss: 2.0123 - val_accuracy: 0.5817\n",
      "Epoch 441/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4250 - accuracy: 0.8243 - val_loss: 1.9143 - val_accuracy: 0.5669\n",
      "Epoch 442/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4326 - accuracy: 0.8185 - val_loss: 1.9068 - val_accuracy: 0.5754\n",
      "Epoch 443/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4190 - accuracy: 0.8220 - val_loss: 1.9133 - val_accuracy: 0.5686\n",
      "Epoch 444/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4104 - accuracy: 0.8274 - val_loss: 1.9440 - val_accuracy: 0.5741\n",
      "Epoch 445/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4165 - accuracy: 0.8218 - val_loss: 1.9545 - val_accuracy: 0.5745\n",
      "Epoch 446/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4190 - accuracy: 0.8276 - val_loss: 1.9391 - val_accuracy: 0.5720\n",
      "Epoch 447/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4148 - accuracy: 0.8260 - val_loss: 1.9246 - val_accuracy: 0.5707\n",
      "Epoch 448/500\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.4212 - accuracy: 0.8245 - val_loss: 1.9531 - val_accuracy: 0.5724\n",
      "Epoch 449/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4116 - accuracy: 0.8241 - val_loss: 1.9533 - val_accuracy: 0.5571\n",
      "Epoch 450/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4204 - accuracy: 0.8240 - val_loss: 1.9907 - val_accuracy: 0.5771\n",
      "Epoch 451/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4204 - accuracy: 0.8207 - val_loss: 1.9596 - val_accuracy: 0.5622\n",
      "Epoch 452/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4360 - accuracy: 0.8109 - val_loss: 1.9698 - val_accuracy: 0.5762\n",
      "Epoch 453/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4179 - accuracy: 0.8272 - val_loss: 1.9319 - val_accuracy: 0.5567\n",
      "Epoch 454/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4222 - accuracy: 0.8138 - val_loss: 1.9891 - val_accuracy: 0.5775\n",
      "Epoch 455/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4271 - accuracy: 0.8187 - val_loss: 1.9467 - val_accuracy: 0.5682\n",
      "Epoch 456/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4134 - accuracy: 0.8274 - val_loss: 1.9707 - val_accuracy: 0.5427\n",
      "Epoch 457/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4174 - accuracy: 0.8232 - val_loss: 1.9809 - val_accuracy: 0.5665\n",
      "Epoch 458/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4162 - accuracy: 0.8249 - val_loss: 1.9335 - val_accuracy: 0.5741\n",
      "Epoch 459/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4159 - accuracy: 0.8220 - val_loss: 1.9616 - val_accuracy: 0.5614\n",
      "Epoch 460/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4152 - accuracy: 0.8260 - val_loss: 1.9339 - val_accuracy: 0.5690\n",
      "Epoch 461/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4138 - accuracy: 0.8238 - val_loss: 1.9904 - val_accuracy: 0.5779\n",
      "Epoch 462/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4135 - accuracy: 0.8230 - val_loss: 2.0028 - val_accuracy: 0.5597\n",
      "Epoch 463/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4204 - accuracy: 0.8212 - val_loss: 1.9424 - val_accuracy: 0.5715\n",
      "Epoch 464/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4103 - accuracy: 0.8276 - val_loss: 1.9413 - val_accuracy: 0.5665\n",
      "Epoch 465/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4165 - accuracy: 0.8165 - val_loss: 2.0007 - val_accuracy: 0.5660\n",
      "Epoch 466/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4139 - accuracy: 0.8240 - val_loss: 1.9990 - val_accuracy: 0.5699\n",
      "Epoch 467/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4083 - accuracy: 0.8254 - val_loss: 2.0081 - val_accuracy: 0.5694\n",
      "Epoch 468/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4123 - accuracy: 0.8296 - val_loss: 1.9701 - val_accuracy: 0.5660\n",
      "Epoch 469/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4123 - accuracy: 0.8289 - val_loss: 2.0155 - val_accuracy: 0.5686\n",
      "Epoch 470/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4206 - accuracy: 0.8245 - val_loss: 1.9402 - val_accuracy: 0.5669\n",
      "Epoch 471/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4182 - accuracy: 0.8209 - val_loss: 1.9948 - val_accuracy: 0.5775\n",
      "Epoch 472/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4170 - accuracy: 0.8274 - val_loss: 1.9974 - val_accuracy: 0.5516\n",
      "Epoch 473/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4130 - accuracy: 0.8276 - val_loss: 1.9742 - val_accuracy: 0.5529\n",
      "Epoch 474/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4228 - accuracy: 0.8232 - val_loss: 2.0366 - val_accuracy: 0.5703\n",
      "Epoch 475/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4098 - accuracy: 0.8292 - val_loss: 2.0365 - val_accuracy: 0.5618\n",
      "Epoch 476/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4127 - accuracy: 0.8252 - val_loss: 2.0032 - val_accuracy: 0.5665\n",
      "Epoch 477/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4092 - accuracy: 0.8274 - val_loss: 1.9706 - val_accuracy: 0.5673\n",
      "Epoch 478/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4117 - accuracy: 0.8245 - val_loss: 2.0130 - val_accuracy: 0.5592\n",
      "Epoch 479/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4184 - accuracy: 0.8189 - val_loss: 2.0348 - val_accuracy: 0.5554\n",
      "Epoch 480/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4173 - accuracy: 0.8218 - val_loss: 1.9754 - val_accuracy: 0.5499\n",
      "Epoch 481/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4103 - accuracy: 0.8325 - val_loss: 1.9897 - val_accuracy: 0.5584\n",
      "Epoch 482/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4174 - accuracy: 0.8247 - val_loss: 1.9820 - val_accuracy: 0.5592\n",
      "Epoch 483/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4151 - accuracy: 0.8285 - val_loss: 2.0064 - val_accuracy: 0.5546\n",
      "Epoch 484/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4121 - accuracy: 0.8249 - val_loss: 2.0060 - val_accuracy: 0.5805\n",
      "Epoch 485/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4110 - accuracy: 0.8260 - val_loss: 2.0361 - val_accuracy: 0.5597\n",
      "Epoch 486/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4150 - accuracy: 0.8243 - val_loss: 1.9961 - val_accuracy: 0.5762\n",
      "Epoch 487/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4207 - accuracy: 0.8256 - val_loss: 2.0294 - val_accuracy: 0.5732\n",
      "Epoch 488/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4102 - accuracy: 0.8241 - val_loss: 2.0385 - val_accuracy: 0.5779\n",
      "Epoch 489/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4104 - accuracy: 0.8269 - val_loss: 2.0387 - val_accuracy: 0.5575\n",
      "Epoch 490/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4184 - accuracy: 0.8258 - val_loss: 2.0050 - val_accuracy: 0.5563\n",
      "Epoch 491/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4116 - accuracy: 0.8232 - val_loss: 2.0432 - val_accuracy: 0.5635\n",
      "Epoch 492/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4086 - accuracy: 0.8305 - val_loss: 1.9856 - val_accuracy: 0.5618\n",
      "Epoch 493/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4120 - accuracy: 0.8254 - val_loss: 2.0414 - val_accuracy: 0.5707\n",
      "Epoch 494/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4094 - accuracy: 0.8260 - val_loss: 2.0061 - val_accuracy: 0.5601\n",
      "Epoch 495/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4140 - accuracy: 0.8258 - val_loss: 2.0462 - val_accuracy: 0.5737\n",
      "Epoch 496/500\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4126 - accuracy: 0.8241 - val_loss: 2.0643 - val_accuracy: 0.5648\n",
      "Epoch 497/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4192 - accuracy: 0.8214 - val_loss: 2.0528 - val_accuracy: 0.5652\n",
      "Epoch 498/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4160 - accuracy: 0.8227 - val_loss: 1.9935 - val_accuracy: 0.5648\n",
      "Epoch 499/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4029 - accuracy: 0.8283 - val_loss: 2.0542 - val_accuracy: 0.5699\n",
      "Epoch 500/500\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4073 - accuracy: 0.8301 - val_loss: 2.0331 - val_accuracy: 0.5745\n",
      "74/74 [==============================] - 0s 1ms/step\n",
      "74/74 [==============================] - 0s 1ms/step\n",
      "Training lstm_model\n",
      "Epoch 1/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0542 - accuracy: 0.9867 - val_loss: 3.4238 - val_accuracy: 0.5524\n",
      "Epoch 2/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0420 - accuracy: 0.9907 - val_loss: 3.4262 - val_accuracy: 0.5567\n",
      "Epoch 3/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0367 - accuracy: 0.9929 - val_loss: 3.4448 - val_accuracy: 0.5524\n",
      "Epoch 4/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0338 - accuracy: 0.9947 - val_loss: 3.4435 - val_accuracy: 0.5575\n",
      "Epoch 5/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0328 - accuracy: 0.9938 - val_loss: 3.4822 - val_accuracy: 0.5601\n",
      "Epoch 6/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.0322 - accuracy: 0.9938 - val_loss: 3.5477 - val_accuracy: 0.5618\n",
      "Epoch 7/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0354 - accuracy: 0.9924 - val_loss: 3.5409 - val_accuracy: 0.5571\n",
      "Epoch 8/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0357 - accuracy: 0.9918 - val_loss: 3.5735 - val_accuracy: 0.5592\n",
      "Epoch 9/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0387 - accuracy: 0.9902 - val_loss: 3.5856 - val_accuracy: 0.5597\n",
      "Epoch 10/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.0399 - accuracy: 0.9916 - val_loss: 3.6020 - val_accuracy: 0.5614\n",
      "Epoch 11/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0820 - accuracy: 0.9745 - val_loss: 3.6001 - val_accuracy: 0.5631\n",
      "Epoch 12/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.2419 - accuracy: 0.9172 - val_loss: 3.4189 - val_accuracy: 0.5287\n",
      "Epoch 13/500\n",
      "86/86 [==============================] - 3s 30ms/step - loss: 0.1849 - accuracy: 0.9397 - val_loss: 3.4382 - val_accuracy: 0.5554\n",
      "Epoch 14/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.2113 - accuracy: 0.9299 - val_loss: 3.2053 - val_accuracy: 0.5516\n",
      "Epoch 15/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.1157 - accuracy: 0.9605 - val_loss: 3.2894 - val_accuracy: 0.5495\n",
      "Epoch 16/500\n",
      "86/86 [==============================] - 2s 29ms/step - loss: 0.0491 - accuracy: 0.9902 - val_loss: 3.3498 - val_accuracy: 0.5592\n",
      "Epoch 17/500\n",
      "86/86 [==============================] - 3s 30ms/step - loss: 0.0342 - accuracy: 0.9944 - val_loss: 3.4133 - val_accuracy: 0.5580\n",
      "Epoch 18/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.0301 - accuracy: 0.9956 - val_loss: 3.4785 - val_accuracy: 0.5660\n",
      "Epoch 19/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0292 - accuracy: 0.9958 - val_loss: 3.5164 - val_accuracy: 0.5690\n",
      "Epoch 20/500\n",
      "86/86 [==============================] - 2s 29ms/step - loss: 0.0330 - accuracy: 0.9938 - val_loss: 3.5178 - val_accuracy: 0.5584\n",
      "Epoch 21/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0281 - accuracy: 0.9949 - val_loss: 3.5336 - val_accuracy: 0.5618\n",
      "Epoch 22/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0274 - accuracy: 0.9958 - val_loss: 3.5764 - val_accuracy: 0.5635\n",
      "Epoch 23/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0309 - accuracy: 0.9931 - val_loss: 3.5990 - val_accuracy: 0.5622\n",
      "Epoch 24/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.0344 - accuracy: 0.9916 - val_loss: 3.6605 - val_accuracy: 0.5567\n",
      "Epoch 25/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.0405 - accuracy: 0.9893 - val_loss: 3.6585 - val_accuracy: 0.5499\n",
      "Epoch 26/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.0819 - accuracy: 0.9736 - val_loss: 3.5697 - val_accuracy: 0.5584\n",
      "Epoch 27/500\n",
      "86/86 [==============================] - 3s 29ms/step - loss: 0.1107 - accuracy: 0.9663 - val_loss: 3.5970 - val_accuracy: 0.5626\n",
      "Epoch 28/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.1766 - accuracy: 0.9370 - val_loss: 3.5081 - val_accuracy: 0.5541\n",
      "Epoch 29/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.1491 - accuracy: 0.9494 - val_loss: 3.3884 - val_accuracy: 0.5703\n",
      "Epoch 30/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0867 - accuracy: 0.9731 - val_loss: 3.4460 - val_accuracy: 0.5614\n",
      "Epoch 31/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0849 - accuracy: 0.9747 - val_loss: 3.4613 - val_accuracy: 0.5597\n",
      "Epoch 32/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.0653 - accuracy: 0.9794 - val_loss: 3.5908 - val_accuracy: 0.5622\n",
      "Epoch 33/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.0357 - accuracy: 0.9934 - val_loss: 3.6322 - val_accuracy: 0.5580\n",
      "Epoch 34/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0272 - accuracy: 0.9954 - val_loss: 3.6271 - val_accuracy: 0.5554\n",
      "Epoch 35/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.0291 - accuracy: 0.9938 - val_loss: 3.6649 - val_accuracy: 0.5618\n",
      "Epoch 36/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0341 - accuracy: 0.9927 - val_loss: 3.7295 - val_accuracy: 0.5601\n",
      "Epoch 37/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.0297 - accuracy: 0.9947 - val_loss: 3.6898 - val_accuracy: 0.5597\n",
      "Epoch 38/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0278 - accuracy: 0.9947 - val_loss: 3.7436 - val_accuracy: 0.5584\n",
      "Epoch 39/500\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0309 - accuracy: 0.9929 - val_loss: 3.7533 - val_accuracy: 0.5478\n",
      "Epoch 40/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0317 - accuracy: 0.9920 - val_loss: 3.7874 - val_accuracy: 0.5609\n",
      "Epoch 41/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.0223 - accuracy: 0.9958 - val_loss: 3.8182 - val_accuracy: 0.5605\n",
      "Epoch 42/500\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0216 - accuracy: 0.9951 - val_loss: 3.8378 - val_accuracy: 0.5592\n",
      "Epoch 43/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0228 - accuracy: 0.9953 - val_loss: 3.8673 - val_accuracy: 0.5537\n",
      "Epoch 44/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0311 - accuracy: 0.9931 - val_loss: 3.8692 - val_accuracy: 0.5592\n",
      "Epoch 45/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0292 - accuracy: 0.9929 - val_loss: 3.8987 - val_accuracy: 0.5554\n",
      "Epoch 46/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.1410 - accuracy: 0.9558 - val_loss: 3.8265 - val_accuracy: 0.5363\n",
      "Epoch 47/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.4269 - accuracy: 0.8646 - val_loss: 3.4620 - val_accuracy: 0.5448\n",
      "Epoch 48/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.2522 - accuracy: 0.9150 - val_loss: 3.4575 - val_accuracy: 0.5720\n",
      "Epoch 49/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0927 - accuracy: 0.9687 - val_loss: 3.4696 - val_accuracy: 0.5665\n",
      "Epoch 50/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0493 - accuracy: 0.9880 - val_loss: 3.4771 - val_accuracy: 0.5597\n",
      "Epoch 51/500\n",
      "86/86 [==============================] - 3s 30ms/step - loss: 0.0280 - accuracy: 0.9953 - val_loss: 3.5806 - val_accuracy: 0.5631\n",
      "Epoch 52/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0234 - accuracy: 0.9969 - val_loss: 3.6174 - val_accuracy: 0.5580\n",
      "Epoch 53/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0225 - accuracy: 0.9962 - val_loss: 3.6742 - val_accuracy: 0.5584\n",
      "Epoch 54/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0211 - accuracy: 0.9976 - val_loss: 3.7060 - val_accuracy: 0.5618\n",
      "Epoch 55/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0190 - accuracy: 0.9973 - val_loss: 3.7050 - val_accuracy: 0.5609\n",
      "Epoch 56/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0183 - accuracy: 0.9976 - val_loss: 3.7507 - val_accuracy: 0.5660\n",
      "Epoch 57/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0194 - accuracy: 0.9984 - val_loss: 3.7370 - val_accuracy: 0.5601\n",
      "Epoch 58/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0193 - accuracy: 0.9978 - val_loss: 3.8234 - val_accuracy: 0.5588\n",
      "Epoch 59/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0182 - accuracy: 0.9976 - val_loss: 3.8068 - val_accuracy: 0.5605\n",
      "Epoch 60/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0210 - accuracy: 0.9967 - val_loss: 3.8491 - val_accuracy: 0.5575\n",
      "Epoch 61/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0213 - accuracy: 0.9953 - val_loss: 3.9026 - val_accuracy: 0.5631\n",
      "Epoch 62/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0378 - accuracy: 0.9916 - val_loss: 3.8763 - val_accuracy: 0.5673\n",
      "Epoch 63/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.0385 - accuracy: 0.9904 - val_loss: 3.9354 - val_accuracy: 0.5575\n",
      "Epoch 64/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.1231 - accuracy: 0.9592 - val_loss: 3.6901 - val_accuracy: 0.5567\n",
      "Epoch 65/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.2373 - accuracy: 0.9177 - val_loss: 3.8675 - val_accuracy: 0.5546\n",
      "Epoch 66/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.1911 - accuracy: 0.9314 - val_loss: 3.6709 - val_accuracy: 0.5546\n",
      "Epoch 67/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0723 - accuracy: 0.9756 - val_loss: 3.7005 - val_accuracy: 0.5567\n",
      "Epoch 68/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0429 - accuracy: 0.9905 - val_loss: 3.7703 - val_accuracy: 0.5618\n",
      "Epoch 69/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0263 - accuracy: 0.9962 - val_loss: 3.7266 - val_accuracy: 0.5588\n",
      "Epoch 70/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0194 - accuracy: 0.9978 - val_loss: 3.8346 - val_accuracy: 0.5601\n",
      "Epoch 71/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0177 - accuracy: 0.9976 - val_loss: 3.8449 - val_accuracy: 0.5597\n",
      "Epoch 72/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0158 - accuracy: 0.9985 - val_loss: 3.8966 - val_accuracy: 0.5614\n",
      "Epoch 73/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0144 - accuracy: 0.9991 - val_loss: 3.9109 - val_accuracy: 0.5571\n",
      "Epoch 74/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0150 - accuracy: 0.9985 - val_loss: 3.9057 - val_accuracy: 0.5643\n",
      "Epoch 75/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0134 - accuracy: 0.9989 - val_loss: 3.9274 - val_accuracy: 0.5635\n",
      "Epoch 76/500\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 0.0138 - accuracy: 0.9982 - val_loss: 3.9504 - val_accuracy: 0.5609\n",
      "Epoch 77/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0161 - accuracy: 0.9971 - val_loss: 3.9783 - val_accuracy: 0.5588\n",
      "Epoch 78/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0173 - accuracy: 0.9975 - val_loss: 3.9783 - val_accuracy: 0.5635\n",
      "Epoch 79/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0162 - accuracy: 0.9971 - val_loss: 4.0380 - val_accuracy: 0.5635\n",
      "Epoch 80/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0168 - accuracy: 0.9973 - val_loss: 4.0392 - val_accuracy: 0.5643\n",
      "Epoch 81/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0741 - accuracy: 0.9809 - val_loss: 4.0633 - val_accuracy: 0.5495\n",
      "Epoch 82/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.3622 - accuracy: 0.8895 - val_loss: 3.5814 - val_accuracy: 0.5461\n",
      "Epoch 83/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.2673 - accuracy: 0.9106 - val_loss: 3.5394 - val_accuracy: 0.5452\n",
      "Epoch 84/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.1520 - accuracy: 0.9432 - val_loss: 3.6717 - val_accuracy: 0.5495\n",
      "Epoch 85/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0575 - accuracy: 0.9836 - val_loss: 3.6315 - val_accuracy: 0.5614\n",
      "Epoch 86/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.0316 - accuracy: 0.9954 - val_loss: 3.7517 - val_accuracy: 0.5550\n",
      "Epoch 87/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0263 - accuracy: 0.9964 - val_loss: 3.7657 - val_accuracy: 0.5597\n",
      "Epoch 88/500\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0190 - accuracy: 0.9984 - val_loss: 3.8121 - val_accuracy: 0.5609\n",
      "Epoch 89/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0160 - accuracy: 0.9985 - val_loss: 3.8493 - val_accuracy: 0.5580\n",
      "Epoch 90/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0142 - accuracy: 0.9985 - val_loss: 3.8754 - val_accuracy: 0.5614\n",
      "Epoch 91/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0126 - accuracy: 0.9995 - val_loss: 3.9015 - val_accuracy: 0.5618\n",
      "Epoch 92/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0130 - accuracy: 0.9989 - val_loss: 3.9324 - val_accuracy: 0.5601\n",
      "Epoch 93/500\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0164 - accuracy: 0.9989 - val_loss: 3.9977 - val_accuracy: 0.5614\n",
      "Epoch 94/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0146 - accuracy: 0.9984 - val_loss: 3.9558 - val_accuracy: 0.5643\n",
      "Epoch 95/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0159 - accuracy: 0.9976 - val_loss: 4.0650 - val_accuracy: 0.5507\n",
      "Epoch 96/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0213 - accuracy: 0.9962 - val_loss: 3.9508 - val_accuracy: 0.5601\n",
      "Epoch 97/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.0235 - accuracy: 0.9938 - val_loss: 3.9905 - val_accuracy: 0.5609\n",
      "Epoch 98/500\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 0.0302 - accuracy: 0.9913 - val_loss: 4.0110 - val_accuracy: 0.5546\n",
      "Epoch 99/500\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.1945 - accuracy: 0.9343 - val_loss: 3.9087 - val_accuracy: 0.5580\n",
      "Epoch 100/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.2676 - accuracy: 0.9139 - val_loss: 3.6021 - val_accuracy: 0.5473\n",
      "Epoch 101/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.2254 - accuracy: 0.9285 - val_loss: 3.7560 - val_accuracy: 0.5592\n",
      "Epoch 102/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0763 - accuracy: 0.9772 - val_loss: 3.7229 - val_accuracy: 0.5537\n",
      "Epoch 103/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0292 - accuracy: 0.9953 - val_loss: 3.7587 - val_accuracy: 0.5597\n",
      "Epoch 104/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0202 - accuracy: 0.9975 - val_loss: 3.8146 - val_accuracy: 0.5584\n",
      "Epoch 105/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0147 - accuracy: 0.9989 - val_loss: 3.8408 - val_accuracy: 0.5601\n",
      "Epoch 106/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0141 - accuracy: 0.9982 - val_loss: 3.8708 - val_accuracy: 0.5601\n",
      "Epoch 107/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0130 - accuracy: 0.9984 - val_loss: 3.9138 - val_accuracy: 0.5584\n",
      "Epoch 108/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.0135 - accuracy: 0.9982 - val_loss: 3.9318 - val_accuracy: 0.5601\n",
      "Epoch 109/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.0121 - accuracy: 0.9987 - val_loss: 3.9719 - val_accuracy: 0.5622\n",
      "Epoch 110/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0127 - accuracy: 0.9989 - val_loss: 3.9944 - val_accuracy: 0.5601\n",
      "Epoch 111/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0112 - accuracy: 0.9989 - val_loss: 3.9927 - val_accuracy: 0.5652\n",
      "Epoch 112/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0125 - accuracy: 0.9985 - val_loss: 3.9986 - val_accuracy: 0.5609\n",
      "Epoch 113/500\n",
      "86/86 [==============================] - 4s 48ms/step - loss: 0.0132 - accuracy: 0.9971 - val_loss: 4.0572 - val_accuracy: 0.5622\n",
      "Epoch 114/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0168 - accuracy: 0.9975 - val_loss: 4.0833 - val_accuracy: 0.5597\n",
      "Epoch 115/500\n",
      "86/86 [==============================] - 3s 30ms/step - loss: 0.1926 - accuracy: 0.9381 - val_loss: 3.9580 - val_accuracy: 0.5554\n",
      "Epoch 116/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.2053 - accuracy: 0.9328 - val_loss: 3.8079 - val_accuracy: 0.5550\n",
      "Epoch 117/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0798 - accuracy: 0.9738 - val_loss: 3.8461 - val_accuracy: 0.5592\n",
      "Epoch 118/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0475 - accuracy: 0.9853 - val_loss: 3.8587 - val_accuracy: 0.5554\n",
      "Epoch 119/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0274 - accuracy: 0.9949 - val_loss: 3.9309 - val_accuracy: 0.5618\n",
      "Epoch 120/500\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0181 - accuracy: 0.9976 - val_loss: 3.9197 - val_accuracy: 0.5584\n",
      "Epoch 121/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0136 - accuracy: 0.9982 - val_loss: 3.9793 - val_accuracy: 0.5588\n",
      "Epoch 122/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0126 - accuracy: 0.9985 - val_loss: 3.9902 - val_accuracy: 0.5524\n",
      "Epoch 123/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0132 - accuracy: 0.9989 - val_loss: 3.9960 - val_accuracy: 0.5652\n",
      "Epoch 124/500\n",
      "86/86 [==============================] - 4s 51ms/step - loss: 0.0146 - accuracy: 0.9969 - val_loss: 4.0377 - val_accuracy: 0.5584\n",
      "Epoch 125/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0172 - accuracy: 0.9965 - val_loss: 4.0680 - val_accuracy: 0.5580\n",
      "Epoch 126/500\n",
      "86/86 [==============================] - 2s 29ms/step - loss: 0.0229 - accuracy: 0.9947 - val_loss: 4.0477 - val_accuracy: 0.5550\n",
      "Epoch 127/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0171 - accuracy: 0.9969 - val_loss: 4.0850 - val_accuracy: 0.5605\n",
      "Epoch 128/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.0182 - accuracy: 0.9960 - val_loss: 4.1096 - val_accuracy: 0.5694\n",
      "Epoch 129/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0208 - accuracy: 0.9949 - val_loss: 4.1183 - val_accuracy: 0.5652\n",
      "Epoch 130/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0222 - accuracy: 0.9944 - val_loss: 4.0703 - val_accuracy: 0.5584\n",
      "Epoch 131/500\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 0.1003 - accuracy: 0.9714 - val_loss: 4.1675 - val_accuracy: 0.5699\n",
      "Epoch 132/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.4304 - accuracy: 0.8744 - val_loss: 3.7102 - val_accuracy: 0.5461\n",
      "Epoch 133/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.1590 - accuracy: 0.9445 - val_loss: 3.7272 - val_accuracy: 0.5537\n",
      "Epoch 134/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0627 - accuracy: 0.9818 - val_loss: 3.7296 - val_accuracy: 0.5597\n",
      "Epoch 135/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0262 - accuracy: 0.9954 - val_loss: 3.7886 - val_accuracy: 0.5605\n",
      "Epoch 136/500\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0194 - accuracy: 0.9973 - val_loss: 3.8067 - val_accuracy: 0.5677\n",
      "Epoch 137/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0143 - accuracy: 0.9993 - val_loss: 3.8434 - val_accuracy: 0.5605\n",
      "Epoch 138/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0129 - accuracy: 0.9991 - val_loss: 3.8814 - val_accuracy: 0.5597\n",
      "Epoch 139/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0135 - accuracy: 0.9989 - val_loss: 3.8937 - val_accuracy: 0.5639\n",
      "Epoch 140/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0100 - accuracy: 0.9996 - val_loss: 3.9357 - val_accuracy: 0.5575\n",
      "Epoch 141/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0104 - accuracy: 0.9993 - val_loss: 3.9395 - val_accuracy: 0.5592\n",
      "Epoch 142/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0110 - accuracy: 0.9987 - val_loss: 3.9723 - val_accuracy: 0.5571\n",
      "Epoch 143/500\n",
      "86/86 [==============================] - 6s 76ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 4.0052 - val_accuracy: 0.5580\n",
      "Epoch 144/500\n",
      "86/86 [==============================] - 5s 57ms/step - loss: 0.0098 - accuracy: 0.9991 - val_loss: 4.0369 - val_accuracy: 0.5546\n",
      "Epoch 145/500\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0107 - accuracy: 0.9987 - val_loss: 4.0294 - val_accuracy: 0.5592\n",
      "Epoch 146/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0085 - accuracy: 0.9995 - val_loss: 4.0532 - val_accuracy: 0.5584\n",
      "Epoch 147/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0086 - accuracy: 0.9989 - val_loss: 4.0872 - val_accuracy: 0.5541\n",
      "Epoch 148/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0097 - accuracy: 0.9982 - val_loss: 4.0732 - val_accuracy: 0.5584\n",
      "Epoch 149/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0135 - accuracy: 0.9978 - val_loss: 4.1496 - val_accuracy: 0.5605\n",
      "Epoch 150/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0164 - accuracy: 0.9958 - val_loss: 4.1466 - val_accuracy: 0.5571\n",
      "Epoch 151/500\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.1716 - accuracy: 0.9525 - val_loss: 3.9636 - val_accuracy: 0.5584\n",
      "Epoch 152/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.4294 - accuracy: 0.8737 - val_loss: 3.5945 - val_accuracy: 0.5473\n",
      "Epoch 153/500\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.1791 - accuracy: 0.9379 - val_loss: 3.6790 - val_accuracy: 0.5465\n",
      "Epoch 154/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0759 - accuracy: 0.9749 - val_loss: 3.8285 - val_accuracy: 0.5546\n",
      "Epoch 155/500\n",
      "86/86 [==============================] - 3s 32ms/step - loss: 0.0285 - accuracy: 0.9940 - val_loss: 3.8329 - val_accuracy: 0.5524\n",
      "Epoch 156/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0169 - accuracy: 0.9985 - val_loss: 3.8915 - val_accuracy: 0.5546\n",
      "Epoch 157/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0117 - accuracy: 0.9993 - val_loss: 3.9239 - val_accuracy: 0.5537\n",
      "Epoch 158/500\n",
      "86/86 [==============================] - 4s 47ms/step - loss: 0.0098 - accuracy: 0.9998 - val_loss: 3.9681 - val_accuracy: 0.5550\n",
      "Epoch 159/500\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 3.9775 - val_accuracy: 0.5533\n",
      "Epoch 160/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0085 - accuracy: 0.9998 - val_loss: 4.0001 - val_accuracy: 0.5529\n",
      "Epoch 161/500\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 4.0294 - val_accuracy: 0.5546\n",
      "Epoch 162/500\n",
      "86/86 [==============================] - 4s 47ms/step - loss: 0.0080 - accuracy: 0.9998 - val_loss: 4.0518 - val_accuracy: 0.5554\n",
      "Epoch 163/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0081 - accuracy: 0.9993 - val_loss: 4.0533 - val_accuracy: 0.5567\n",
      "Epoch 164/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0092 - accuracy: 0.9991 - val_loss: 4.0943 - val_accuracy: 0.5567\n",
      "Epoch 165/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0085 - accuracy: 0.9995 - val_loss: 4.0928 - val_accuracy: 0.5567\n",
      "Epoch 166/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0096 - accuracy: 0.9989 - val_loss: 4.1172 - val_accuracy: 0.5580\n",
      "Epoch 167/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0136 - accuracy: 0.9982 - val_loss: 4.1267 - val_accuracy: 0.5533\n",
      "Epoch 168/500\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 0.0106 - accuracy: 0.9980 - val_loss: 4.2198 - val_accuracy: 0.5571\n",
      "Epoch 169/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0093 - accuracy: 0.9989 - val_loss: 4.1559 - val_accuracy: 0.5546\n",
      "Epoch 170/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0177 - accuracy: 0.9956 - val_loss: 4.2309 - val_accuracy: 0.5575\n",
      "Epoch 171/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.1206 - accuracy: 0.9629 - val_loss: 4.0640 - val_accuracy: 0.5380\n",
      "Epoch 172/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.4154 - accuracy: 0.8839 - val_loss: 3.6973 - val_accuracy: 0.5520\n",
      "Epoch 173/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.1863 - accuracy: 0.9381 - val_loss: 3.8108 - val_accuracy: 0.5448\n",
      "Epoch 174/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0519 - accuracy: 0.9827 - val_loss: 3.7857 - val_accuracy: 0.5524\n",
      "Epoch 175/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.0296 - accuracy: 0.9947 - val_loss: 3.8127 - val_accuracy: 0.5516\n",
      "Epoch 176/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.0300 - accuracy: 0.9931 - val_loss: 3.8837 - val_accuracy: 0.5588\n",
      "Epoch 177/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0184 - accuracy: 0.9985 - val_loss: 3.9154 - val_accuracy: 0.5520\n",
      "Epoch 178/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 3.9714 - val_accuracy: 0.5554\n",
      "Epoch 179/500\n",
      "86/86 [==============================] - 3s 41ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 3.9948 - val_accuracy: 0.5550\n",
      "Epoch 180/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 4.0203 - val_accuracy: 0.5537\n",
      "Epoch 181/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 4.0389 - val_accuracy: 0.5541\n",
      "Epoch 182/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 4.0648 - val_accuracy: 0.5550\n",
      "Epoch 183/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 4.0773 - val_accuracy: 0.5558\n",
      "Epoch 184/500\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 0.0079 - accuracy: 0.9995 - val_loss: 4.1094 - val_accuracy: 0.5614\n",
      "Epoch 185/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0084 - accuracy: 0.9989 - val_loss: 4.1185 - val_accuracy: 0.5546\n",
      "Epoch 186/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0075 - accuracy: 0.9993 - val_loss: 4.1420 - val_accuracy: 0.5592\n",
      "Epoch 187/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0075 - accuracy: 0.9993 - val_loss: 4.1668 - val_accuracy: 0.5558\n",
      "Epoch 188/500\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 0.0073 - accuracy: 0.9991 - val_loss: 4.1483 - val_accuracy: 0.5571\n",
      "Epoch 189/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0081 - accuracy: 0.9991 - val_loss: 4.1963 - val_accuracy: 0.5575\n",
      "Epoch 190/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0081 - accuracy: 0.9987 - val_loss: 4.2266 - val_accuracy: 0.5550\n",
      "Epoch 191/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.0171 - accuracy: 0.9958 - val_loss: 4.2578 - val_accuracy: 0.5486\n",
      "Epoch 192/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.1252 - accuracy: 0.9629 - val_loss: 4.2119 - val_accuracy: 0.5660\n",
      "Epoch 193/500\n",
      "86/86 [==============================] - 5s 62ms/step - loss: 0.3751 - accuracy: 0.8848 - val_loss: 3.8797 - val_accuracy: 0.5554\n",
      "Epoch 194/500\n",
      "86/86 [==============================] - 5s 56ms/step - loss: 0.1547 - accuracy: 0.9474 - val_loss: 3.7862 - val_accuracy: 0.5499\n",
      "Epoch 195/500\n",
      "86/86 [==============================] - 4s 48ms/step - loss: 0.0530 - accuracy: 0.9851 - val_loss: 3.8503 - val_accuracy: 0.5465\n",
      "Epoch 196/500\n",
      "86/86 [==============================] - 5s 58ms/step - loss: 0.0220 - accuracy: 0.9962 - val_loss: 3.8957 - val_accuracy: 0.5461\n",
      "Epoch 197/500\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 0.0123 - accuracy: 0.9993 - val_loss: 3.9437 - val_accuracy: 0.5516\n",
      "Epoch 198/500\n",
      "86/86 [==============================] - 5s 58ms/step - loss: 0.0101 - accuracy: 0.9996 - val_loss: 3.9652 - val_accuracy: 0.5499\n",
      "Epoch 199/500\n",
      "86/86 [==============================] - 7s 77ms/step - loss: 0.0108 - accuracy: 0.9989 - val_loss: 4.0169 - val_accuracy: 0.5473\n",
      "Epoch 200/500\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 0.0092 - accuracy: 0.9993 - val_loss: 4.0376 - val_accuracy: 0.5537\n",
      "Epoch 201/500\n",
      "86/86 [==============================] - 6s 67ms/step - loss: 0.0076 - accuracy: 0.9996 - val_loss: 4.0549 - val_accuracy: 0.5520\n",
      "Epoch 202/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0084 - accuracy: 0.9993 - val_loss: 4.0857 - val_accuracy: 0.5533\n",
      "Epoch 203/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.0069 - accuracy: 0.9998 - val_loss: 4.1103 - val_accuracy: 0.5533\n",
      "Epoch 204/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0060 - accuracy: 0.9998 - val_loss: 4.1107 - val_accuracy: 0.5507\n",
      "Epoch 205/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0068 - accuracy: 0.9995 - val_loss: 4.1413 - val_accuracy: 0.5550\n",
      "Epoch 206/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0059 - accuracy: 0.9996 - val_loss: 4.1531 - val_accuracy: 0.5554\n",
      "Epoch 207/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0083 - accuracy: 0.9991 - val_loss: 4.1506 - val_accuracy: 0.5541\n",
      "Epoch 208/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0081 - accuracy: 0.9989 - val_loss: 4.1748 - val_accuracy: 0.5512\n",
      "Epoch 209/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0089 - accuracy: 0.9991 - val_loss: 4.2322 - val_accuracy: 0.5439\n",
      "Epoch 210/500\n",
      "86/86 [==============================] - 6s 74ms/step - loss: 0.0156 - accuracy: 0.9973 - val_loss: 4.2387 - val_accuracy: 0.5444\n",
      "Epoch 211/500\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 0.0833 - accuracy: 0.9732 - val_loss: 4.2262 - val_accuracy: 0.5486\n",
      "Epoch 212/500\n",
      "86/86 [==============================] - 5s 58ms/step - loss: 0.4669 - accuracy: 0.8709 - val_loss: 3.8470 - val_accuracy: 0.5461\n",
      "Epoch 213/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.2059 - accuracy: 0.9315 - val_loss: 3.8074 - val_accuracy: 0.5414\n",
      "Epoch 214/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0535 - accuracy: 0.9840 - val_loss: 3.8315 - val_accuracy: 0.5520\n",
      "Epoch 215/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0236 - accuracy: 0.9954 - val_loss: 3.8957 - val_accuracy: 0.5533\n",
      "Epoch 216/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0134 - accuracy: 0.9993 - val_loss: 3.9127 - val_accuracy: 0.5575\n",
      "Epoch 217/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0102 - accuracy: 0.9993 - val_loss: 3.9314 - val_accuracy: 0.5584\n",
      "Epoch 218/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0092 - accuracy: 0.9993 - val_loss: 3.9800 - val_accuracy: 0.5533\n",
      "Epoch 219/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0080 - accuracy: 0.9996 - val_loss: 3.9993 - val_accuracy: 0.5550\n",
      "Epoch 220/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 4.0113 - val_accuracy: 0.5575\n",
      "Epoch 221/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 4.0447 - val_accuracy: 0.5588\n",
      "Epoch 222/500\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 4.0639 - val_accuracy: 0.5592\n",
      "Epoch 223/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 4.0904 - val_accuracy: 0.5584\n",
      "Epoch 224/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0056 - accuracy: 0.9998 - val_loss: 4.1136 - val_accuracy: 0.5614\n",
      "Epoch 225/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0054 - accuracy: 0.9998 - val_loss: 4.1114 - val_accuracy: 0.5597\n",
      "Epoch 226/500\n",
      "86/86 [==============================] - 3s 41ms/step - loss: 0.0069 - accuracy: 0.9993 - val_loss: 4.1380 - val_accuracy: 0.5648\n",
      "Epoch 227/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0060 - accuracy: 0.9995 - val_loss: 4.1683 - val_accuracy: 0.5558\n",
      "Epoch 228/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0053 - accuracy: 0.9996 - val_loss: 4.1909 - val_accuracy: 0.5588\n",
      "Epoch 229/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0061 - accuracy: 0.9993 - val_loss: 4.2051 - val_accuracy: 0.5575\n",
      "Epoch 230/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0059 - accuracy: 0.9998 - val_loss: 4.2165 - val_accuracy: 0.5597\n",
      "Epoch 231/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0058 - accuracy: 0.9995 - val_loss: 4.2723 - val_accuracy: 0.5592\n",
      "Epoch 232/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0132 - accuracy: 0.9973 - val_loss: 4.2545 - val_accuracy: 0.5584\n",
      "Epoch 233/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0205 - accuracy: 0.9936 - val_loss: 4.2060 - val_accuracy: 0.5469\n",
      "Epoch 234/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.1696 - accuracy: 0.9499 - val_loss: 4.2055 - val_accuracy: 0.5482\n",
      "Epoch 235/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.3864 - accuracy: 0.8833 - val_loss: 3.7820 - val_accuracy: 0.5435\n",
      "Epoch 236/500\n",
      "86/86 [==============================] - 5s 54ms/step - loss: 0.0972 - accuracy: 0.9620 - val_loss: 3.8118 - val_accuracy: 0.5512\n",
      "Epoch 237/500\n",
      "86/86 [==============================] - 4s 48ms/step - loss: 0.0404 - accuracy: 0.9874 - val_loss: 3.9130 - val_accuracy: 0.5516\n",
      "Epoch 238/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.0227 - accuracy: 0.9976 - val_loss: 3.9190 - val_accuracy: 0.5588\n",
      "Epoch 239/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.0105 - accuracy: 0.9996 - val_loss: 3.9951 - val_accuracy: 0.5614\n",
      "Epoch 240/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 4.0289 - val_accuracy: 0.5592\n",
      "Epoch 241/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0069 - accuracy: 0.9996 - val_loss: 4.0533 - val_accuracy: 0.5584\n",
      "Epoch 242/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0068 - accuracy: 0.9996 - val_loss: 4.0696 - val_accuracy: 0.5588\n",
      "Epoch 243/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 4.0702 - val_accuracy: 0.5580\n",
      "Epoch 244/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0061 - accuracy: 0.9993 - val_loss: 4.1208 - val_accuracy: 0.5580\n",
      "Epoch 245/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0073 - accuracy: 0.9993 - val_loss: 4.0969 - val_accuracy: 0.5601\n",
      "Epoch 246/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0059 - accuracy: 0.9993 - val_loss: 4.1470 - val_accuracy: 0.5554\n",
      "Epoch 247/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0231 - accuracy: 0.9954 - val_loss: 4.1663 - val_accuracy: 0.5614\n",
      "Epoch 248/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0780 - accuracy: 0.9749 - val_loss: 4.1926 - val_accuracy: 0.5699\n",
      "Epoch 249/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.1951 - accuracy: 0.9336 - val_loss: 3.9964 - val_accuracy: 0.5448\n",
      "Epoch 250/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.1543 - accuracy: 0.9527 - val_loss: 3.8896 - val_accuracy: 0.5584\n",
      "Epoch 251/500\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 0.0637 - accuracy: 0.9798 - val_loss: 3.9900 - val_accuracy: 0.5537\n",
      "Epoch 252/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0290 - accuracy: 0.9924 - val_loss: 3.9974 - val_accuracy: 0.5537\n",
      "Epoch 253/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0190 - accuracy: 0.9975 - val_loss: 4.0731 - val_accuracy: 0.5554\n",
      "Epoch 254/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0095 - accuracy: 0.9987 - val_loss: 4.1112 - val_accuracy: 0.5563\n",
      "Epoch 255/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0066 - accuracy: 0.9996 - val_loss: 4.1300 - val_accuracy: 0.5533\n",
      "Epoch 256/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 4.1662 - val_accuracy: 0.5563\n",
      "Epoch 257/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0056 - accuracy: 0.9996 - val_loss: 4.1954 - val_accuracy: 0.5567\n",
      "Epoch 258/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0060 - accuracy: 0.9996 - val_loss: 4.2063 - val_accuracy: 0.5558\n",
      "Epoch 259/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0059 - accuracy: 0.9996 - val_loss: 4.2139 - val_accuracy: 0.5537\n",
      "Epoch 260/500\n",
      "86/86 [==============================] - 3s 41ms/step - loss: 0.0047 - accuracy: 0.9998 - val_loss: 4.2228 - val_accuracy: 0.5567\n",
      "Epoch 261/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 4.2444 - val_accuracy: 0.5550\n",
      "Epoch 262/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 4.2578 - val_accuracy: 0.5537\n",
      "Epoch 263/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 4.2695 - val_accuracy: 0.5533\n",
      "Epoch 264/500\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 0.0039 - accuracy: 0.9998 - val_loss: 4.2767 - val_accuracy: 0.5567\n",
      "Epoch 265/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0045 - accuracy: 0.9996 - val_loss: 4.2807 - val_accuracy: 0.5516\n",
      "Epoch 266/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 4.3153 - val_accuracy: 0.5529\n",
      "Epoch 267/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0045 - accuracy: 0.9996 - val_loss: 4.3793 - val_accuracy: 0.5546\n",
      "Epoch 268/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0096 - accuracy: 0.9978 - val_loss: 4.3648 - val_accuracy: 0.5597\n",
      "Epoch 269/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0423 - accuracy: 0.9896 - val_loss: 4.3852 - val_accuracy: 0.5520\n",
      "Epoch 270/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.2407 - accuracy: 0.9257 - val_loss: 4.4007 - val_accuracy: 0.5597\n",
      "Epoch 271/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.2396 - accuracy: 0.9254 - val_loss: 3.9323 - val_accuracy: 0.5372\n",
      "Epoch 272/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.1065 - accuracy: 0.9621 - val_loss: 4.0420 - val_accuracy: 0.5618\n",
      "Epoch 273/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.0319 - accuracy: 0.9922 - val_loss: 4.0498 - val_accuracy: 0.5490\n",
      "Epoch 274/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0196 - accuracy: 0.9971 - val_loss: 4.0406 - val_accuracy: 0.5520\n",
      "Epoch 275/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0165 - accuracy: 0.9967 - val_loss: 4.1459 - val_accuracy: 0.5529\n",
      "Epoch 276/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0087 - accuracy: 0.9991 - val_loss: 4.1964 - val_accuracy: 0.5537\n",
      "Epoch 277/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0077 - accuracy: 0.9989 - val_loss: 4.2155 - val_accuracy: 0.5567\n",
      "Epoch 278/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.0081 - accuracy: 0.9989 - val_loss: 4.2556 - val_accuracy: 0.5571\n",
      "Epoch 279/500\n",
      "86/86 [==============================] - 3s 41ms/step - loss: 0.0066 - accuracy: 0.9995 - val_loss: 4.2518 - val_accuracy: 0.5537\n",
      "Epoch 280/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0055 - accuracy: 0.9998 - val_loss: 4.2840 - val_accuracy: 0.5524\n",
      "Epoch 281/500\n",
      "86/86 [==============================] - 3s 41ms/step - loss: 0.0045 - accuracy: 0.9998 - val_loss: 4.2905 - val_accuracy: 0.5575\n",
      "Epoch 282/500\n",
      "86/86 [==============================] - 3s 41ms/step - loss: 0.0054 - accuracy: 0.9996 - val_loss: 4.2991 - val_accuracy: 0.5541\n",
      "Epoch 283/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0045 - accuracy: 0.9996 - val_loss: 4.3182 - val_accuracy: 0.5541\n",
      "Epoch 284/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.0045 - accuracy: 0.9996 - val_loss: 4.3377 - val_accuracy: 0.5524\n",
      "Epoch 285/500\n",
      "86/86 [==============================] - 3s 41ms/step - loss: 0.0039 - accuracy: 0.9998 - val_loss: 4.3772 - val_accuracy: 0.5541\n",
      "Epoch 286/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0049 - accuracy: 0.9996 - val_loss: 4.3768 - val_accuracy: 0.5592\n",
      "Epoch 287/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0045 - accuracy: 0.9996 - val_loss: 4.3775 - val_accuracy: 0.5563\n",
      "Epoch 288/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0043 - accuracy: 0.9996 - val_loss: 4.4005 - val_accuracy: 0.5571\n",
      "Epoch 289/500\n",
      "86/86 [==============================] - 3s 41ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 4.3756 - val_accuracy: 0.5571\n",
      "Epoch 290/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0257 - accuracy: 0.9933 - val_loss: 4.3905 - val_accuracy: 0.5486\n",
      "Epoch 291/500\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 0.2647 - accuracy: 0.9283 - val_loss: 4.0518 - val_accuracy: 0.5465\n",
      "Epoch 292/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.2940 - accuracy: 0.9093 - val_loss: 3.9197 - val_accuracy: 0.5397\n",
      "Epoch 293/500\n",
      "86/86 [==============================] - 3s 41ms/step - loss: 0.0886 - accuracy: 0.9694 - val_loss: 4.0509 - val_accuracy: 0.5473\n",
      "Epoch 294/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0305 - accuracy: 0.9913 - val_loss: 4.0695 - val_accuracy: 0.5461\n",
      "Epoch 295/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0143 - accuracy: 0.9984 - val_loss: 4.1046 - val_accuracy: 0.5529\n",
      "Epoch 296/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0116 - accuracy: 0.9980 - val_loss: 4.1255 - val_accuracy: 0.5495\n",
      "Epoch 297/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0109 - accuracy: 0.9982 - val_loss: 4.1533 - val_accuracy: 0.5516\n",
      "Epoch 298/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0081 - accuracy: 0.9987 - val_loss: 4.1459 - val_accuracy: 0.5541\n",
      "Epoch 299/500\n",
      "86/86 [==============================] - 3s 41ms/step - loss: 0.0064 - accuracy: 0.9996 - val_loss: 4.1861 - val_accuracy: 0.5529\n",
      "Epoch 300/500\n",
      "86/86 [==============================] - 4s 47ms/step - loss: 0.0054 - accuracy: 0.9996 - val_loss: 4.1983 - val_accuracy: 0.5541\n",
      "Epoch 301/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0057 - accuracy: 0.9996 - val_loss: 4.2137 - val_accuracy: 0.5541\n",
      "Epoch 302/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0046 - accuracy: 0.9998 - val_loss: 4.2233 - val_accuracy: 0.5554\n",
      "Epoch 303/500\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 0.0046 - accuracy: 0.9996 - val_loss: 4.2468 - val_accuracy: 0.5554\n",
      "Epoch 304/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0041 - accuracy: 0.9998 - val_loss: 4.2403 - val_accuracy: 0.5558\n",
      "Epoch 305/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0039 - accuracy: 0.9998 - val_loss: 4.2883 - val_accuracy: 0.5571\n",
      "Epoch 306/500\n",
      "86/86 [==============================] - 3s 41ms/step - loss: 0.0046 - accuracy: 0.9996 - val_loss: 4.2924 - val_accuracy: 0.5554\n",
      "Epoch 307/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0043 - accuracy: 0.9998 - val_loss: 4.3024 - val_accuracy: 0.5550\n",
      "Epoch 308/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0044 - accuracy: 0.9996 - val_loss: 4.3038 - val_accuracy: 0.5529\n",
      "Epoch 309/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0059 - accuracy: 0.9991 - val_loss: 4.3162 - val_accuracy: 0.5558\n",
      "Epoch 310/500\n",
      "86/86 [==============================] - 3s 41ms/step - loss: 0.0051 - accuracy: 0.9995 - val_loss: 4.3016 - val_accuracy: 0.5558\n",
      "Epoch 311/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0074 - accuracy: 0.9984 - val_loss: 4.3358 - val_accuracy: 0.5550\n",
      "Epoch 312/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0425 - accuracy: 0.9862 - val_loss: 4.3808 - val_accuracy: 0.5469\n",
      "Epoch 313/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.3290 - accuracy: 0.9093 - val_loss: 4.0395 - val_accuracy: 0.5376\n",
      "Epoch 314/500\n",
      "86/86 [==============================] - 3s 41ms/step - loss: 0.2095 - accuracy: 0.9288 - val_loss: 3.9536 - val_accuracy: 0.5541\n",
      "Epoch 315/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0866 - accuracy: 0.9711 - val_loss: 3.9324 - val_accuracy: 0.5554\n",
      "Epoch 316/500\n",
      "86/86 [==============================] - 3s 41ms/step - loss: 0.0465 - accuracy: 0.9873 - val_loss: 3.9570 - val_accuracy: 0.5410\n",
      "Epoch 317/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0247 - accuracy: 0.9944 - val_loss: 4.0593 - val_accuracy: 0.5546\n",
      "Epoch 318/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0118 - accuracy: 0.9984 - val_loss: 4.1209 - val_accuracy: 0.5490\n",
      "Epoch 319/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0074 - accuracy: 0.9993 - val_loss: 4.1054 - val_accuracy: 0.5482\n",
      "Epoch 320/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0061 - accuracy: 0.9995 - val_loss: 4.1566 - val_accuracy: 0.5469\n",
      "Epoch 321/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.0050 - accuracy: 0.9998 - val_loss: 4.1840 - val_accuracy: 0.5499\n",
      "Epoch 322/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0049 - accuracy: 0.9996 - val_loss: 4.2134 - val_accuracy: 0.5520\n",
      "Epoch 323/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.0052 - accuracy: 0.9995 - val_loss: 4.2272 - val_accuracy: 0.5507\n",
      "Epoch 324/500\n",
      "86/86 [==============================] - 4s 52ms/step - loss: 0.0044 - accuracy: 0.9996 - val_loss: 4.2430 - val_accuracy: 0.5520\n",
      "Epoch 325/500\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 0.0044 - accuracy: 0.9996 - val_loss: 4.2505 - val_accuracy: 0.5541\n",
      "Epoch 326/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0038 - accuracy: 0.9998 - val_loss: 4.2804 - val_accuracy: 0.5507\n",
      "Epoch 327/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0041 - accuracy: 0.9996 - val_loss: 4.2841 - val_accuracy: 0.5533\n",
      "Epoch 328/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0040 - accuracy: 0.9996 - val_loss: 4.3079 - val_accuracy: 0.5516\n",
      "Epoch 329/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0033 - accuracy: 0.9998 - val_loss: 4.2955 - val_accuracy: 0.5478\n",
      "Epoch 330/500\n",
      "86/86 [==============================] - 4s 47ms/step - loss: 0.0039 - accuracy: 0.9996 - val_loss: 4.3229 - val_accuracy: 0.5503\n",
      "Epoch 331/500\n",
      "86/86 [==============================] - 4s 43ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 4.3611 - val_accuracy: 0.5571\n",
      "Epoch 332/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0029 - accuracy: 0.9998 - val_loss: 4.3546 - val_accuracy: 0.5516\n",
      "Epoch 333/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0049 - accuracy: 0.9993 - val_loss: 4.3949 - val_accuracy: 0.5507\n",
      "Epoch 334/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0039 - accuracy: 0.9998 - val_loss: 4.3908 - val_accuracy: 0.5558\n",
      "Epoch 335/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0046 - accuracy: 0.9995 - val_loss: 4.4365 - val_accuracy: 0.5550\n",
      "Epoch 336/500\n",
      "86/86 [==============================] - 3s 41ms/step - loss: 0.0775 - accuracy: 0.9798 - val_loss: 4.3665 - val_accuracy: 0.5490\n",
      "Epoch 337/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.3620 - accuracy: 0.8997 - val_loss: 4.0107 - val_accuracy: 0.5499\n",
      "Epoch 338/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.2021 - accuracy: 0.9306 - val_loss: 3.8261 - val_accuracy: 0.5503\n",
      "Epoch 339/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0725 - accuracy: 0.9756 - val_loss: 3.9884 - val_accuracy: 0.5512\n",
      "Epoch 340/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0341 - accuracy: 0.9907 - val_loss: 4.0744 - val_accuracy: 0.5567\n",
      "Epoch 341/500\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 0.0138 - accuracy: 0.9982 - val_loss: 4.0916 - val_accuracy: 0.5563\n",
      "Epoch 342/500\n",
      "86/86 [==============================] - 4s 52ms/step - loss: 0.0095 - accuracy: 0.9993 - val_loss: 4.1243 - val_accuracy: 0.5541\n",
      "Epoch 343/500\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 4.1404 - val_accuracy: 0.5524\n",
      "Epoch 344/500\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 0.0050 - accuracy: 0.9998 - val_loss: 4.1776 - val_accuracy: 0.5533\n",
      "Epoch 345/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0048 - accuracy: 0.9996 - val_loss: 4.2099 - val_accuracy: 0.5546\n",
      "Epoch 346/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0043 - accuracy: 0.9998 - val_loss: 4.2141 - val_accuracy: 0.5533\n",
      "Epoch 347/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0041 - accuracy: 0.9998 - val_loss: 4.2467 - val_accuracy: 0.5567\n",
      "Epoch 348/500\n",
      "86/86 [==============================] - 3s 41ms/step - loss: 0.0038 - accuracy: 0.9998 - val_loss: 4.2599 - val_accuracy: 0.5554\n",
      "Epoch 349/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0036 - accuracy: 0.9998 - val_loss: 4.2919 - val_accuracy: 0.5567\n",
      "Epoch 350/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0041 - accuracy: 0.9996 - val_loss: 4.3130 - val_accuracy: 0.5575\n",
      "Epoch 351/500\n",
      "86/86 [==============================] - 4s 41ms/step - loss: 0.0045 - accuracy: 0.9995 - val_loss: 4.3296 - val_accuracy: 0.5507\n",
      "Epoch 352/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0041 - accuracy: 0.9996 - val_loss: 4.3238 - val_accuracy: 0.5524\n",
      "Epoch 353/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0047 - accuracy: 0.9993 - val_loss: 4.3441 - val_accuracy: 0.5563\n",
      "Epoch 354/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0063 - accuracy: 0.9987 - val_loss: 4.3659 - val_accuracy: 0.5580\n",
      "Epoch 355/500\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0085 - accuracy: 0.9982 - val_loss: 4.3621 - val_accuracy: 0.5537\n",
      "Epoch 356/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0101 - accuracy: 0.9982 - val_loss: 4.4286 - val_accuracy: 0.5537\n",
      "Epoch 357/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.1113 - accuracy: 0.9689 - val_loss: 4.3458 - val_accuracy: 0.5372\n",
      "Epoch 358/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.2777 - accuracy: 0.9143 - val_loss: 4.1922 - val_accuracy: 0.5444\n",
      "Epoch 359/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.1787 - accuracy: 0.9459 - val_loss: 3.8950 - val_accuracy: 0.5448\n",
      "Epoch 360/500\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0666 - accuracy: 0.9782 - val_loss: 4.0722 - val_accuracy: 0.5414\n",
      "Epoch 361/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0187 - accuracy: 0.9967 - val_loss: 4.0468 - val_accuracy: 0.5397\n",
      "Epoch 362/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0090 - accuracy: 0.9995 - val_loss: 4.0921 - val_accuracy: 0.5439\n",
      "Epoch 363/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0069 - accuracy: 0.9995 - val_loss: 4.1465 - val_accuracy: 0.5503\n",
      "Epoch 364/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0061 - accuracy: 0.9998 - val_loss: 4.1497 - val_accuracy: 0.5431\n",
      "Epoch 365/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0051 - accuracy: 0.9998 - val_loss: 4.1736 - val_accuracy: 0.5452\n",
      "Epoch 366/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0054 - accuracy: 0.9996 - val_loss: 4.1820 - val_accuracy: 0.5452\n",
      "Epoch 367/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0047 - accuracy: 0.9998 - val_loss: 4.1953 - val_accuracy: 0.5448\n",
      "Epoch 368/500\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0065 - accuracy: 0.9993 - val_loss: 4.2227 - val_accuracy: 0.5495\n",
      "Epoch 369/500\n",
      "86/86 [==============================] - 3s 40ms/step - loss: 0.0047 - accuracy: 0.9996 - val_loss: 4.2369 - val_accuracy: 0.5427\n",
      "Epoch 370/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0040 - accuracy: 0.9996 - val_loss: 4.2781 - val_accuracy: 0.5444\n",
      "Epoch 371/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0040 - accuracy: 0.9998 - val_loss: 4.2895 - val_accuracy: 0.5448\n",
      "Epoch 372/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0034 - accuracy: 0.9998 - val_loss: 4.3081 - val_accuracy: 0.5414\n",
      "Epoch 373/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0032 - accuracy: 0.9998 - val_loss: 4.3102 - val_accuracy: 0.5448\n",
      "Epoch 374/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0031 - accuracy: 0.9998 - val_loss: 4.3449 - val_accuracy: 0.5478\n",
      "Epoch 375/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0041 - accuracy: 0.9996 - val_loss: 4.3570 - val_accuracy: 0.5452\n",
      "Epoch 376/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0038 - accuracy: 0.9996 - val_loss: 4.3637 - val_accuracy: 0.5473\n",
      "Epoch 377/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0039 - accuracy: 0.9996 - val_loss: 4.3900 - val_accuracy: 0.5452\n",
      "Epoch 378/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 4.3968 - val_accuracy: 0.5435\n",
      "Epoch 379/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0171 - accuracy: 0.9960 - val_loss: 4.4677 - val_accuracy: 0.5503\n",
      "Epoch 380/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.2117 - accuracy: 0.9386 - val_loss: 4.1528 - val_accuracy: 0.5516\n",
      "Epoch 381/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.4174 - accuracy: 0.8797 - val_loss: 3.9045 - val_accuracy: 0.5427\n",
      "Epoch 382/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.1312 - accuracy: 0.9539 - val_loss: 3.8036 - val_accuracy: 0.5495\n",
      "Epoch 383/500\n",
      "86/86 [==============================] - 4s 42ms/step - loss: 0.0421 - accuracy: 0.9867 - val_loss: 4.0321 - val_accuracy: 0.5524\n",
      "Epoch 384/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0135 - accuracy: 0.9982 - val_loss: 4.0610 - val_accuracy: 0.5507\n",
      "Epoch 385/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0085 - accuracy: 0.9996 - val_loss: 4.0596 - val_accuracy: 0.5473\n",
      "Epoch 386/500\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 4.1018 - val_accuracy: 0.5469\n",
      "Epoch 387/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 4.1142 - val_accuracy: 0.5482\n",
      "Epoch 388/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 4.1328 - val_accuracy: 0.5541\n",
      "Epoch 389/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 4.1591 - val_accuracy: 0.5486\n",
      "Epoch 390/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0042 - accuracy: 0.9998 - val_loss: 4.1710 - val_accuracy: 0.5512\n",
      "Epoch 391/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 4.1971 - val_accuracy: 0.5482\n",
      "Epoch 392/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 4.2000 - val_accuracy: 0.5507\n",
      "Epoch 393/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 4.2267 - val_accuracy: 0.5456\n",
      "Epoch 394/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0034 - accuracy: 0.9998 - val_loss: 4.2286 - val_accuracy: 0.5486\n",
      "Epoch 395/500\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0033 - accuracy: 0.9998 - val_loss: 4.2726 - val_accuracy: 0.5512\n",
      "Epoch 396/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0033 - accuracy: 0.9998 - val_loss: 4.2434 - val_accuracy: 0.5461\n",
      "Epoch 397/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0033 - accuracy: 0.9998 - val_loss: 4.2754 - val_accuracy: 0.5516\n",
      "Epoch 398/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0038 - accuracy: 0.9996 - val_loss: 4.2826 - val_accuracy: 0.5533\n",
      "Epoch 399/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0077 - accuracy: 0.9982 - val_loss: 4.3767 - val_accuracy: 0.5524\n",
      "Epoch 400/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0125 - accuracy: 0.9973 - val_loss: 4.2954 - val_accuracy: 0.5486\n",
      "Epoch 401/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0204 - accuracy: 0.9956 - val_loss: 4.2979 - val_accuracy: 0.5584\n",
      "Epoch 402/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0512 - accuracy: 0.9851 - val_loss: 4.2059 - val_accuracy: 0.5486\n",
      "Epoch 403/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.1244 - accuracy: 0.9612 - val_loss: 4.2240 - val_accuracy: 0.5512\n",
      "Epoch 404/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.2007 - accuracy: 0.9341 - val_loss: 4.0893 - val_accuracy: 0.5473\n",
      "Epoch 405/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.1111 - accuracy: 0.9638 - val_loss: 4.1665 - val_accuracy: 0.5550\n",
      "Epoch 406/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0478 - accuracy: 0.9853 - val_loss: 4.1165 - val_accuracy: 0.5486\n",
      "Epoch 407/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0176 - accuracy: 0.9969 - val_loss: 4.1828 - val_accuracy: 0.5558\n",
      "Epoch 408/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0071 - accuracy: 0.9996 - val_loss: 4.2053 - val_accuracy: 0.5469\n",
      "Epoch 409/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0051 - accuracy: 0.9996 - val_loss: 4.2295 - val_accuracy: 0.5499\n",
      "Epoch 410/500\n",
      "86/86 [==============================] - 3s 41ms/step - loss: 0.0045 - accuracy: 0.9996 - val_loss: 4.2407 - val_accuracy: 0.5520\n",
      "Epoch 411/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0066 - accuracy: 0.9993 - val_loss: 4.2719 - val_accuracy: 0.5486\n",
      "Epoch 412/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0046 - accuracy: 0.9995 - val_loss: 4.3030 - val_accuracy: 0.5507\n",
      "Epoch 413/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0058 - accuracy: 0.9989 - val_loss: 4.3122 - val_accuracy: 0.5533\n",
      "Epoch 414/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0048 - accuracy: 0.9996 - val_loss: 4.3011 - val_accuracy: 0.5512\n",
      "Epoch 415/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0042 - accuracy: 0.9996 - val_loss: 4.3318 - val_accuracy: 0.5495\n",
      "Epoch 416/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0065 - accuracy: 0.9991 - val_loss: 4.2995 - val_accuracy: 0.5533\n",
      "Epoch 417/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0046 - accuracy: 0.9993 - val_loss: 4.3517 - val_accuracy: 0.5533\n",
      "Epoch 418/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 4.3691 - val_accuracy: 0.5490\n",
      "Epoch 419/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0075 - accuracy: 0.9978 - val_loss: 4.3422 - val_accuracy: 0.5597\n",
      "Epoch 420/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0186 - accuracy: 0.9953 - val_loss: 4.4304 - val_accuracy: 0.5546\n",
      "Epoch 421/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.1354 - accuracy: 0.9556 - val_loss: 4.2504 - val_accuracy: 0.5533\n",
      "Epoch 422/500\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.1520 - accuracy: 0.9499 - val_loss: 4.1961 - val_accuracy: 0.5541\n",
      "Epoch 423/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.1304 - accuracy: 0.9583 - val_loss: 4.1206 - val_accuracy: 0.5660\n",
      "Epoch 424/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0420 - accuracy: 0.9858 - val_loss: 4.1381 - val_accuracy: 0.5554\n",
      "Epoch 425/500\n",
      "86/86 [==============================] - 3s 38ms/step - loss: 0.0219 - accuracy: 0.9954 - val_loss: 4.1519 - val_accuracy: 0.5495\n",
      "Epoch 426/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0144 - accuracy: 0.9980 - val_loss: 4.2028 - val_accuracy: 0.5507\n",
      "Epoch 427/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0109 - accuracy: 0.9982 - val_loss: 4.1817 - val_accuracy: 0.5520\n",
      "Epoch 428/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0078 - accuracy: 0.9987 - val_loss: 4.2320 - val_accuracy: 0.5533\n",
      "Epoch 429/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0072 - accuracy: 0.9989 - val_loss: 4.2609 - val_accuracy: 0.5554\n",
      "Epoch 430/500\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0087 - accuracy: 0.9984 - val_loss: 4.3033 - val_accuracy: 0.5567\n",
      "Epoch 431/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0416 - accuracy: 0.9878 - val_loss: 4.2967 - val_accuracy: 0.5503\n",
      "Epoch 432/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0592 - accuracy: 0.9829 - val_loss: 4.3476 - val_accuracy: 0.5359\n",
      "Epoch 433/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0477 - accuracy: 0.9858 - val_loss: 4.3183 - val_accuracy: 0.5575\n",
      "Epoch 434/500\n",
      "86/86 [==============================] - 3s 36ms/step - loss: 0.0633 - accuracy: 0.9818 - val_loss: 4.2490 - val_accuracy: 0.5410\n",
      "Epoch 435/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0685 - accuracy: 0.9789 - val_loss: 4.2041 - val_accuracy: 0.5380\n",
      "Epoch 436/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0457 - accuracy: 0.9860 - val_loss: 4.2544 - val_accuracy: 0.5495\n",
      "Epoch 437/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0266 - accuracy: 0.9944 - val_loss: 4.3165 - val_accuracy: 0.5537\n",
      "Epoch 438/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0125 - accuracy: 0.9980 - val_loss: 4.3707 - val_accuracy: 0.5563\n",
      "Epoch 439/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0067 - accuracy: 0.9989 - val_loss: 4.3742 - val_accuracy: 0.5571\n",
      "Epoch 440/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0064 - accuracy: 0.9991 - val_loss: 4.3725 - val_accuracy: 0.5601\n",
      "Epoch 441/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0050 - accuracy: 0.9993 - val_loss: 4.3393 - val_accuracy: 0.5512\n",
      "Epoch 442/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0048 - accuracy: 0.9991 - val_loss: 4.3983 - val_accuracy: 0.5558\n",
      "Epoch 443/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0039 - accuracy: 0.9995 - val_loss: 4.4061 - val_accuracy: 0.5529\n",
      "Epoch 444/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 4.4206 - val_accuracy: 0.5541\n",
      "Epoch 445/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0044 - accuracy: 0.9995 - val_loss: 4.4498 - val_accuracy: 0.5546\n",
      "Epoch 446/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0040 - accuracy: 0.9993 - val_loss: 4.4742 - val_accuracy: 0.5575\n",
      "Epoch 447/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0160 - accuracy: 0.9973 - val_loss: 4.4693 - val_accuracy: 0.5478\n",
      "Epoch 448/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0318 - accuracy: 0.9914 - val_loss: 4.5340 - val_accuracy: 0.5435\n",
      "Epoch 449/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0317 - accuracy: 0.9891 - val_loss: 4.5077 - val_accuracy: 0.5507\n",
      "Epoch 450/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.1108 - accuracy: 0.9643 - val_loss: 4.2615 - val_accuracy: 0.5524\n",
      "Epoch 451/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0948 - accuracy: 0.9718 - val_loss: 4.1974 - val_accuracy: 0.5473\n",
      "Epoch 452/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0590 - accuracy: 0.9825 - val_loss: 4.2516 - val_accuracy: 0.5490\n",
      "Epoch 453/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0196 - accuracy: 0.9945 - val_loss: 4.2806 - val_accuracy: 0.5439\n",
      "Epoch 454/500\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 0.0128 - accuracy: 0.9975 - val_loss: 4.3639 - val_accuracy: 0.5444\n",
      "Epoch 455/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0085 - accuracy: 0.9987 - val_loss: 4.3790 - val_accuracy: 0.5452\n",
      "Epoch 456/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 4.3946 - val_accuracy: 0.5490\n",
      "Epoch 457/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.4100 - val_accuracy: 0.5490\n",
      "Epoch 458/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.4199 - val_accuracy: 0.5473\n",
      "Epoch 459/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0034 - accuracy: 0.9996 - val_loss: 4.4399 - val_accuracy: 0.5495\n",
      "Epoch 460/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0026 - accuracy: 0.9998 - val_loss: 4.4556 - val_accuracy: 0.5512\n",
      "Epoch 461/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 4.4763 - val_accuracy: 0.5456\n",
      "Epoch 462/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0040 - accuracy: 0.9996 - val_loss: 4.4710 - val_accuracy: 0.5486\n",
      "Epoch 463/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.5048 - val_accuracy: 0.5541\n",
      "Epoch 464/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 4.5183 - val_accuracy: 0.5516\n",
      "Epoch 465/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 4.5230 - val_accuracy: 0.5486\n",
      "Epoch 466/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 4.5551 - val_accuracy: 0.5524\n",
      "Epoch 467/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0377 - accuracy: 0.9898 - val_loss: 4.6656 - val_accuracy: 0.5499\n",
      "Epoch 468/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.1395 - accuracy: 0.9565 - val_loss: 4.5051 - val_accuracy: 0.5350\n",
      "Epoch 469/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.2117 - accuracy: 0.9383 - val_loss: 4.2439 - val_accuracy: 0.5389\n",
      "Epoch 470/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.1457 - accuracy: 0.9550 - val_loss: 4.1846 - val_accuracy: 0.5490\n",
      "Epoch 471/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0407 - accuracy: 0.9853 - val_loss: 4.2641 - val_accuracy: 0.5448\n",
      "Epoch 472/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0144 - accuracy: 0.9973 - val_loss: 4.2756 - val_accuracy: 0.5490\n",
      "Epoch 473/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0063 - accuracy: 0.9995 - val_loss: 4.3031 - val_accuracy: 0.5469\n",
      "Epoch 474/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0048 - accuracy: 0.9996 - val_loss: 4.3154 - val_accuracy: 0.5456\n",
      "Epoch 475/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0045 - accuracy: 0.9995 - val_loss: 4.3635 - val_accuracy: 0.5469\n",
      "Epoch 476/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 4.3878 - val_accuracy: 0.5512\n",
      "Epoch 477/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 4.4071 - val_accuracy: 0.5499\n",
      "Epoch 478/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.4197 - val_accuracy: 0.5486\n",
      "Epoch 479/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.4331 - val_accuracy: 0.5478\n",
      "Epoch 480/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0028 - accuracy: 0.9998 - val_loss: 4.4570 - val_accuracy: 0.5512\n",
      "Epoch 481/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 4.4794 - val_accuracy: 0.5520\n",
      "Epoch 482/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 4.4889 - val_accuracy: 0.5516\n",
      "Epoch 483/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0035 - accuracy: 0.9996 - val_loss: 4.5135 - val_accuracy: 0.5512\n",
      "Epoch 484/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 4.4997 - val_accuracy: 0.5499\n",
      "Epoch 485/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 4.5445 - val_accuracy: 0.5507\n",
      "Epoch 486/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0027 - accuracy: 0.9998 - val_loss: 4.5335 - val_accuracy: 0.5533\n",
      "Epoch 487/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 4.5474 - val_accuracy: 0.5520\n",
      "Epoch 488/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 4.5600 - val_accuracy: 0.5537\n",
      "Epoch 489/500\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 4.5763 - val_accuracy: 0.5546\n",
      "Epoch 490/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 4.6526 - val_accuracy: 0.5486\n",
      "Epoch 491/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0100 - accuracy: 0.9973 - val_loss: 4.6782 - val_accuracy: 0.5490\n",
      "Epoch 492/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.2658 - accuracy: 0.9306 - val_loss: 4.1916 - val_accuracy: 0.5380\n",
      "Epoch 493/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.3471 - accuracy: 0.8957 - val_loss: 4.0495 - val_accuracy: 0.5452\n",
      "Epoch 494/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.1054 - accuracy: 0.9661 - val_loss: 4.1234 - val_accuracy: 0.5414\n",
      "Epoch 495/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0224 - accuracy: 0.9951 - val_loss: 4.1528 - val_accuracy: 0.5439\n",
      "Epoch 496/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0083 - accuracy: 0.9995 - val_loss: 4.2176 - val_accuracy: 0.5452\n",
      "Epoch 497/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0062 - accuracy: 0.9998 - val_loss: 4.2522 - val_accuracy: 0.5439\n",
      "Epoch 498/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0047 - accuracy: 0.9998 - val_loss: 4.2702 - val_accuracy: 0.5473\n",
      "Epoch 499/500\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 0.0041 - accuracy: 0.9998 - val_loss: 4.3064 - val_accuracy: 0.5461\n",
      "Epoch 500/500\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 0.0039 - accuracy: 0.9998 - val_loss: 4.3119 - val_accuracy: 0.5482\n",
      "74/74 [==============================] - 1s 9ms/step\n",
      "74/74 [==============================] - 1s 9ms/step\n",
      "RandomForestClassifier Score is 0.61\n",
      "GradientBoostingClassifier Score is 0.63\n",
      "LogisticRegression Score is 0.62\n",
      "SVC Score is 0.56\n",
      "LinearSVC Score is 0.61\n",
      "NuSVC Score is 0.60\n",
      "KNeighborsClassifier Score is 0.53\n",
      "cnn_model Score is 0.52\n",
      "dense_model Score is 0.57\n",
      "lstm_model Score is 0.54\n",
      "GaussianNB Score is 0.50\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1500x800 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 2111.0916640758514 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time()\n",
    "match_data = pd.read_csv(\"data/all_stats.csv\")\n",
    "X_train, X_test, y_train, y_test = split_match_data(match_data, list_of_features, target_variable)\n",
    "metrics_df = get_model_metrics(models, X_train, X_test, y_train, y_test,500)\n",
    "get_scores(metrics_df)\n",
    "plot_scores(metrics_df)\n",
    "end = time()\n",
    "print(f\"Time taken: {end - start} seconds\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is 6080-5FF0\n",
      "\n",
      " Directory of C:\\Users\\Niklas\\PycharmProjects\\fb-predictions-pl\n",
      "\n",
      "04/03/2024  15:45    <DIR>          .\n",
      "20/02/2024  15:53    <DIR>          ..\n",
      "04/03/2024  15:43    <DIR>          .idea\n",
      "22/02/2024  14:57             1,265 createLeagueTable.csv.py\n",
      "22/02/2024  20:00    <DIR>          data\n",
      "04/03/2024  11:58            14,644 dataManipulation.py\n",
      "04/03/2024  15:44    <DIR>          logs\n",
      "04/03/2024  14:15             2,385 main.py\n",
      "04/03/2024  11:41             2,006 models.py\n",
      "04/03/2024  11:57             4,931 model_evaluation.py\n",
      "22/02/2024  14:57               862 replacenames.py\n",
      "27/02/2024  10:51               143 requirements.txt\n",
      "04/03/2024  14:44    <DIR>          stat\n",
      "04/03/2024  15:45           224,392 stats.ipynb\n",
      "04/03/2024  14:12    <DIR>          venv\n",
      "04/03/2024  11:42    <DIR>          __pycache__\n",
      "               8 File(s)        250,628 bytes\n",
      "               8 Dir(s)  41,206,796,288 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
